{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Глубокое обучение\n",
    "\n",
    "# Лекция 2: Обучение искуственных нейронных сетей в PyTorch\n",
    "\n",
    "__Автор: Сергей Вячеславович Макрушин__ e-mail: s-makrushin@yandex.ru \n",
    "\n",
    "Финансовый универсиет, 2023 г. \n",
    "\n",
    "V 0.8 20.09.2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При подготовке лекции использованы материалы:\n",
    "* Николенко, Кадурин, Архангельская \"Глубокое обучение. Погружение в мир нейронных сетей\"\n",
    "* Документация PyTorch: https://pytorch.org/docs/stable/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разделы: <a class=\"anchor\" id=\"разделы\"></a>\n",
    "* [Современные методы обучения нейронной сети и обратное распространение ошибки](#современные-методы)\n",
    "* [Обучение модели нейронной сети, алгоритм обратного распространения ошибки](#обратное-распространение)\n",
    "    * [Задача обучения модели нейронной сети](#проблема-обучения)\n",
    "    * [Проблема поиска градиента](#проблема-поиска)\n",
    "* [Дифференцируемое программирование и реализация обратного распространения ошибки](#дифференцируемое)\n",
    "    * [Автоматическое дифференциирование в PyTorch](#автоматическое-PyTorch)\n",
    "\n",
    "-\n",
    "\n",
    "* [к оглавлению](#разделы)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "﻿<style>\r\n",
       "\r\n",
       "\r\n",
       "b.n {\r\n",
       "    font-weight: normal;        \r\n",
       "}\r\n",
       "\r\n",
       "b.grbg {\r\n",
       "    background-color: #a0a0a0;      \r\n",
       "}\r\n",
       "\r\n",
       "b.r {\r\n",
       "    color: #ff0000;    \r\n",
       "}\r\n",
       "\r\n",
       "\r\n",
       "b.b {    \r\n",
       "    color: #0000ff;    \r\n",
       "}\r\n",
       "\r\n",
       "b.g {\r\n",
       "    color: #00ff00;    \r\n",
       "}\r\n",
       "\r\n",
       "\r\n",
       "// add your CSS styling here\r\n",
       "\r\n",
       "list-style: none;\r\n",
       "\r\n",
       "ul.s {\r\n",
       "//    list-style-type: none;\r\n",
       "    list-style: none;\r\n",
       "//    background-color: #ff0000;  \r\n",
       "//    color: #ffff00;\r\n",
       "//  padding-left: 1.2em;\r\n",
       "//  text-indent: -1.2em;\r\n",
       "}\r\n",
       "\r\n",
       "li.t {\r\n",
       "    list-style: none;\r\n",
       "//  padding-left: 1.2em;\r\n",
       "//  text-indent: -1.2em;    \r\n",
       "}\r\n",
       "\r\n",
       "\r\n",
       "*.r {\r\n",
       "    color: #ff0000;    \r\n",
       "}\r\n",
       "\r\n",
       "li.t:before {\r\n",
       "    content: \"\\21D2\";    \r\n",
       "//    content: \"►\";\r\n",
       "//    padding-left: -1.2em;    \r\n",
       "    text-indent: -1.2em;    \r\n",
       "    display: block;\r\n",
       "    float: left;\r\n",
       "    \r\n",
       "    \r\n",
       "//    width: 1.2em;\r\n",
       "//    color: #ff0000;\r\n",
       "}\r\n",
       "\r\n",
       "i.m:before {\r\n",
       "    font-style: normal;    \r\n",
       "    content: \"\\21D2\";  \r\n",
       "}\r\n",
       "i.m {\r\n",
       "    font-style: normal; \r\n",
       "}    \r\n",
       "\r\n",
       "/*--------------------*/\r\n",
       "/* em {\r\n",
       "    font-style: normal; \r\n",
       "} */\r\n",
       "\r\n",
       "\r\n",
       "em.bl {\r\n",
       "    font-style: normal;     \r\n",
       "    font-weight: bold;        \r\n",
       "}\r\n",
       "\r\n",
       "/* em.grbg {\r\n",
       "    font-style: normal;         \r\n",
       "    background-color: #a0a0a0;      \r\n",
       "} */\r\n",
       "\r\n",
       "em.cr {\r\n",
       "    font-style: normal;         \r\n",
       "    color: #ff0000;    \r\n",
       "}\r\n",
       "\r\n",
       "em.cb {    \r\n",
       "    font-style: normal;         \r\n",
       "    color: #0000ff;    \r\n",
       "}\r\n",
       "\r\n",
       "em.cg {\r\n",
       "    font-style: normal;         \r\n",
       "    color: #00ff00;    \r\n",
       "}\r\n",
       "\r\n",
       "/*--------------------*/\r\n",
       "\r\n",
       "em.qs {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.qs::before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #ff0000;    \r\n",
       "    content: \"Q:\";  \r\n",
       "}\r\n",
       "\r\n",
       "em.an {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.an:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"A:\";  \r\n",
       "}\r\n",
       "    \r\n",
       "em.nt {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.nt:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"Note:\";  \r\n",
       "}    \r\n",
       "    \r\n",
       "em.ex {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.ex:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #00ff00;    \r\n",
       "    content: \"Ex:\";  \r\n",
       "} \r\n",
       "    \r\n",
       "em.df {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.df:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"Def:\";  \r\n",
       "}    \r\n",
       "\r\n",
       "em.pl {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.pl:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"+\";  \r\n",
       "}    \r\n",
       "\r\n",
       "em.mn {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.mn:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"-\";  \r\n",
       "}        \r\n",
       "\r\n",
       "em.plmn {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.plmn:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"\\00B1\";\\\\\"&plusmn;\";  \r\n",
       "}\r\n",
       "    \r\n",
       "em.hn {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.hn:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"\\21D2\";\\\\\"&rArr;\";  \r\n",
       "}     \r\n",
       "    \r\n",
       "\r\n",
       "#cssTableCenter td, th \r\n",
       "{\r\n",
       "    text-align: center; \r\n",
       "    vertical-align: middle;\r\n",
       "}\r\n",
       "\r\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# загружаем стиль для оформления презентации\n",
    "from IPython.display import HTML\n",
    "from urllib.request import urlopen\n",
    "html = urlopen(\"file:./lec_v2.css\")\n",
    "HTML(html.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Современные методы обучения нейронной сети и обратное распространение ошибки <a class=\"anchor\" id=\"современные-методы\"></a>\n",
    "* [к оглавлению](#разделы)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Применение тензоров:  прямое распространение сигналов и оценка ошибки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Постановка задачи__ \n",
    "\n",
    "* У нас есть набор данных $D$, состоящий из пар $(\\pmb{x}, \\pmb{y})$, где $\\pmb{x}$ - признаки, а $\\pmb{y}$ - правильный ответ. \n",
    "* Модель сети $f_L$, имеющей $L$ слоев с весами $\\pmb{\\theta}$ (совокупность весов нейронов из всех слоев) на этих данных делает некоторые предсказания $\\hat{\\pmb{y}} = f_L(\\pmb{x}, \\pmb{\\theta})$\n",
    "* Задана функция ошибки $E$, которую можно подсчитать на каждом примере: $E(f_L(\\pmb{x}, \\pmb{\\theta}), \\pmb{y})$ (например, это может быть квадрат или модуль отклонения $\\hat{\\pmb{y}}$ от $\\pmb{y}$ в случае регрессии или перекрестная энтропия в случае классификации)\n",
    "* Тогда суммарная ошибка на наборе данных $D$ будет функцией от параметров модели: $E(\\pmb{\\theta})$ и определяется как $E(\\pmb{\\theta})=\\sum_{(\\pmb{x}, \\pmb{y}) \\in D} E(f_L(\\pmb{x}, \\pmb{\\theta}), \\pmb{y})$\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/main_cycle_p2_v2.png\" alt=\"Прямой проход и оценка ошибки\" style=\"width: 600px;\"/><br/>\n",
    "    <b>Прямой проход и оценка ошибки</b>    \n",
    "</center> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Прямое распространение сигналов__\n",
    "\n",
    "* Модель нейронной сети это иерархия (она может быть простой и очень сложной) связанных (последовательно применяемых) функций слоев:\n",
    "    * т.е. модель сети $f_L$ может быть представленна как суперпозиция из $L$ слоев $h^i\\text{, }i \\in \\{1, \\ldots, L\\}$, каждый из которых параметризуется своими весами $w_i$:\n",
    "$$f_L(\\pmb{x}, \\pmb{\\theta})=f_L(\\pmb{x}, \\pmb{w}_1, \\ldots, \\pmb{w}_L )=h^L(h^{L-1}(\\ldots h^1(\\pmb{x}, \\pmb{w}_1), \\ldots, \\pmb{w}_{L-1}),\\pmb{w}_L)$$\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/ann_11.png\" alt=\"многослойный перцептрон с двумя скрытыми слоями\" style=\"width: 600px;\"/><br/>\n",
    "    <b><em class=\"ex\"></em> пример модели сети: многослойный перцептрон с двумя скрытыми слоями</b>    \n",
    "</center> \n",
    "\n",
    "* Прямое распространение сигналов по модели (в частности: нейронной сети) реализуется с помощощью __прямого прохода (forward pass)__: входящая информация (вектор $\\pmb{x}$) распространяется через сеть $f_L$ с учетом весов связей $\\pmb{\\theta}$, расчитывается выходной вектор $\\hat{\\pmb{y}}=f_L(\\pmb{x}, \\pmb{\\theta})$ .\n",
    "    * Каждый слой нейронной сети - это последовательно применяемая функция слоя, которая рассчитывается при помощи операций с тензорами.\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/ann_12.png\" alt=\"пример прямого прохода\" style=\"width: 600px;\"/><br/>\n",
    "    <b><em class=\"ex\"></em> пример прямого прохода</b>    \n",
    "</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Последовательность операций с тензормаи используется для расчета результата:\n",
    "    * Прямой проход (forward pass): входящая информация (вектор $\\pmb{x}$) распространяется через сеть с учетом весов связей, расчитывается выходной вектор $\\hat{\\pmb{y}} = f_L(\\pmb{x}, \\pmb{\\theta})= h^L(h^{L-1}(\\ldots h^1(\\pmb{x}, \\pmb{w}_1), \\ldots, \\pmb{w}_{L-1}),\\pmb{w}_L)$\n",
    "    * Оценки ошибки $E(\\hat{\\pmb{y}}, \\pmb{y})$ на множестве правильных ответов: $\\pmb{y}$.\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/main_cycle_p2_v2.png\" alt=\"Прямой проход и оценка ошибки\" style=\"width: 600px;\"/><br/>\n",
    "    <b>Прямой проход и оценка ошибки</b>    \n",
    "</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Обучение модели нейронной сети, алгоритм обратного распространения ошибки <a class=\"anchor\" id=\"обратное-распространение\"></a>\n",
    "* [к оглавлению](#разделы)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача обучения модели нейронной сети <a class=\"anchor\" id=\"проблема-обучения\"></a>\n",
    "* [к оглавлению](#разделы)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Проблема обучения модели нейронной сети: общий взгляд__\n",
    "\n",
    "* <em class=\"nt\"></em> __основная проблема__ это не применение модели к входным данным $\\pmb{x}$ и оцнка ошибки на правильных ответах $\\pmb{y}$, а __обучение модели__ (опредление наилучших параметров модели $\\pmb{\\theta}$). \n",
    "     * В случае нейронной сети обучение сводится к поиску весов слоев сети $\\pmb{\\theta}=(\\pmb{w}_1, \\ldots, \\pmb{w}_L)$, которые в совокупности являются параметрами модели $\\pmb{\\theta}$.\n",
    "\n",
    "* Формально: цель обучения - найти оптимальное значение параметров $\\theta^{*}$, минимизирующих ошибку на обучающией выборке $D$: \n",
    "$$\\theta^{*} = \\arg \\underset{\\pmb{\\theta}}{\\min} \\ E(\\pmb{\\theta}) = \\arg \\underset{\\pmb{\\theta}}{\\min} \\ \\sum_{(\\pmb{x}, \\pmb{y}) \\in D} E(f_L(\\pmb{x}, \\pmb{\\theta}), \\pmb{y})$$\n",
    "* Т.е. задача обучения сводится к задаче оптимизации.\n",
    "    * <em class=\"nt\"></em> На самом деле __все сложнее__: хороший результат на $D$ может плохо обобщаться (модель может давать низкое качество на другой выборке из той же генеральной совокупности) - __проблема переобучения__.\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/main_cycle_p1_v1.png\" alt=\"Приниципиальная логика обучения нейронной сети\" style=\"width: 800px;\"/><br/>\n",
    "    <b>Приниципиальная логика обучения нейронной сети</b>    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Прямой проход и оценка ошибки__\n",
    "\n",
    "* __Прямой проход__ (forward pass): входящая информация (вектор $\\pmb{x}$) распространяется через сеть с учетом весов связей, расчитывается выходной вектор $\\hat{\\pmb{y}} = f_L(\\pmb{x}, \\pmb{\\theta})= h^L(h^{L-1}(\\ldots h^1(\\pmb{x}, \\pmb{w}_1), \\ldots, \\pmb{w}_{L-1}),\\pmb{w}_L)$\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/ann_12.png\" alt=\"пример прямого прохода\" style=\"width: 300px;\"/><br/>\n",
    "    <b><em class=\"ex\"></em> пример прямого прохода</b>    \n",
    "</center> \n",
    "\n",
    "* __Оценки ошибки__ $E(\\hat{\\pmb{y}}, \\pmb{y})$ на множестве правильных ответов: $\\pmb{y}$.\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/main_cycle_p2_v2.png\" alt=\"Прямой проход и оценка ошибки\" style=\"width: 400px;\"/><br/>\n",
    "    <b>Прямой проход и оценка ошибки в общей логике обучения нейронной сети</b>    \n",
    "</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задача оптимизации__\n",
    "\n",
    "* Задача: корректировка весов сети (параметров модели $\\pmb{\\theta}$) на основе информации об ошибке на обучающих примерах $E(\\hat{\\pmb{y}}, \\pmb{y})$.\n",
    "    * Решение: использовать методы оптимизации, основанные на __методе градиентного спуска__.\n",
    "    \n",
    "\n",
    "* __Метод градиентныого спуска__ - метод нахождения локального экстремума (минимума или максимума) функции с помощью движения вдоль градиента. В нашем случае шаг метода градиентного спуска выглядит следующим образом:\n",
    "$$\\pmb{\\theta}_t = \\pmb{\\theta}_{t-1}-\\gamma\\nabla_\\theta E(\\pmb{\\theta}_{t-1}) = \\pmb{\\theta}_{t-1}-\\gamma \\sum_{(\\pmb{x}, \\pmb{y}) \\in D} \\nabla_\\theta E(f_L(\\pmb{x}, \\pmb{\\theta}), \\pmb{y})$$\n",
    "\n",
    "* <em class=\"nt\"></em> Выполнение на каждом шаге градиентого спуска суммирование по всем $(\\pmb{x}, \\pmb{y}) \\in D$ __обычно слшиком неэффективно__\n",
    "\n",
    "\n",
    "* Для выпуклых функций __задача локальной оптимизации__ - найти локальный минимум (максимум) автоматически превращается в __задачу глобальной оптимизации__ - найти точку, в которой достигается наименьшее (наибольшее) значение функции, то есть самую низкую (высокую) точку среди всех.\n",
    "* Оптимизировать веса одного перцептрона - выпуклая задача, но __для большой нейронной сети  целевая функция не является выпуклой__.\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/ann_15.png\" alt=\"Прямой проход и оценка ошибки\" style=\"width: 500px;\"/><br/>\n",
    "    <b>Пример работы градиентного спуска для функции двух переменных</b>    \n",
    "</center>\n",
    "\n",
    "* У нейронных сетей функция ошибки может задавать __очень сложный ландшафт__ с огромным числом локальных максимумов и минимумов. Это свойство необходимо для обеспечения __выразительности нейронных сетей__, позволяющей им решать так много разных задач.\n",
    "\n",
    "\n",
    "* <em class=\"nt\"></em> для использования методов, основанных на методе градиентного спуска __необходимо знать градиент функции потерь по параметрам модели__: $\\nabla_\\theta E(f_L(\\pmb{x}, \\pmb{\\theta}), \\pmb{y})$. Этот градиент определяет вектор (\"направление\") изменения параметров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Пример: обучение модели линейной регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Исходные данные:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Матрица X: \n",
      "tensor([[ 1., 40.],\n",
      "        [ 2., 30.],\n",
      "        [ 3., 20.],\n",
      "        [ 4., 10.]])\n",
      "X.size = torch.Size([4, 2])\n",
      "w true value = tensor([2., 1.]), Y = tensor([42., 34., 26., 18.])\n"
     ]
    }
   ],
   "source": [
    "# Модель линейной регрессии (с несколькими параметрами)\n",
    "# f = X * w \n",
    "\n",
    "# Данные для обучения: \n",
    "# принзаки X: рассматривается 4 наблюдения (ось 0) и 2 признака (ось 1):\n",
    "\n",
    "# вариант исходных данны #1 (2й признак всегда равен 0):\n",
    "# X = torch.tensor([[1., 0.],\n",
    "#                   [2., 0.],\n",
    "#                   [3., 0.],\n",
    "#                   [4., 0.]], dtype=torch.float32) # Size([4, 2])\n",
    "\n",
    "# вариант исходных данны #2 (2й признак используется и существенно больше 1го):\n",
    "X = torch.tensor([[1., 40.],\n",
    "                  [2., 30.],\n",
    "                  [3., 20.],\n",
    "                  [4., 10.]], dtype=torch.float32) # Size([4, 2])\n",
    "\n",
    "print(f'Матрица X: \\n{X}')\n",
    "print(f'X.size = {X.size()}') \n",
    "\n",
    "# истинное значение весов (используется только для получения обучающих правильных ответов):\n",
    "\n",
    "# вариант #1:\n",
    "# w_ans = torch.tensor([2., 0.], dtype=torch.float32)\n",
    "\n",
    "# вариант #2:\n",
    "w_ans = torch.tensor([2., 1.], dtype=torch.float32)\n",
    "\n",
    "# Y - приавильные ответы: \n",
    "Y = X @ w_ans\n",
    "\n",
    "torch.set_printoptions(precision=5) # точность вывода на печать значений тензоров\n",
    "print(f'w true value = {w_ans}, Y = {Y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Модель (прямой проход):__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:\n",
      "tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# model (модель, в нашем случае: линейная регрессия)\n",
    "\n",
    "# изначальное значение весов w\n",
    "w = torch.tensor([0.0, 0.0], dtype=torch.float32, requires_grad=False)\n",
    "\n",
    "print(f'w:\\n{w}')\n",
    "\n",
    "# прямое распространение:\n",
    "def forward(X):\n",
    "    return X @ w # Size([4])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "<img src=\"./img/main_cycle_p2_v2.png\" alt=\"Прямой проход и оценка ошибки\" style=\"width: 400px;\"/><br/>\n",
    "    <b>Прямой проход и оценка ошибки в общей логике обучения нейронной сети</b>    \n",
    "</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Функция ошибки и рассчет градиента (аналитический):__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = MSE (функция потерь, в нашем слаучае: средняя квадратичная ошибка)\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred - y)**2).mean() # Size([])\n",
    "\n",
    "# градиент: \n",
    "# рассчитан аналитически по модели и функции потерь:\n",
    "# J = MSE = 1/N * (w*x - y)**2\n",
    "# dJ/dw = 1/N * 2 * (w*x - y) * x\n",
    "def gradient(x, y, y_pred):   \n",
    "#     print(f'''y = {y},\n",
    "#     y_pred = {y_pred},\n",
    "#     (2* (y_pred - y)).unsqueeze(1) = {(2* (y_pred - y)).unsqueeze(1)},\n",
    "#     x = {x},\n",
    "#     ((2* (y_pred - y)).unsqueeze(1) * x) = {((2* (y_pred - y)).unsqueeze(1) * x)},\n",
    "#     ((2* (y_pred - y)).unsqueeze(1) * x).mean(dim=0) = {((2* (y_pred - y)).unsqueeze(1) * x).mean(dim=0)}''')\n",
    "    return ((2* (y_pred - y)).unsqueeze(1) * x).mean(dim=0)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Один проход обучения:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch BEFORE LEARNING: w = tensor([0., 0.]), y_pred = tensor([0., 0., 0., 0.]), loss = 980.00000000\n",
      "gradient = tensor([ -130., -1700.])\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0013\n",
    "\n",
    "# predict = forward pass\n",
    "y_pred = forward(X)\n",
    "\n",
    "# loss\n",
    "l = loss(Y, y_pred)\n",
    "\n",
    "# calculate gradients\n",
    "dw = gradient(X, Y, y_pred)\n",
    "\n",
    "print(f'epoch BEFORE LEARNING: w = {w}, y_pred = {y_pred}, loss = {l:.8f}\\ngradient = {dw}')\n",
    "\n",
    "# update weights\n",
    "w -= learning_rate * dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch AFTER FIRST STEP: w = tensor([0.16900, 2.21000]), y_pred = tensor([88.56900, 66.63800, 44.70700, 22.77600]), loss = 901.66833496\n",
      "gradient = tensor([ -130., -1700.])\n"
     ]
    }
   ],
   "source": [
    "# predict = forward pass\n",
    "y_pred = forward(X)\n",
    "\n",
    "# loss\n",
    "l = loss(Y, y_pred)\n",
    "\n",
    "print(f'epoch AFTER FIRST STEP: w = {w}, y_pred = {y_pred}, loss = {l:.8f}\\ngradient = {dw}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Основной цикл обучения:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# изначальное значение весов w\n",
    "w = torch.tensor([0.0, 0.0], dtype=torch.float32, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 0: w = tensor([0.16900, 2.21000]), y_p = tensor([0., 0., 0., 0.]), loss = 980.00000000\n",
      "ep 5: w = tensor([0.13813, 0.24422]), y_p = tensor([81.70274, 61.57523, 41.44772, 21.32021]), loss = 646.58898926\n",
      "ep 10: w = tensor([0.33966, 1.82453]), y_p = tensor([15.22918, 11.70162,  8.17406,  4.64650]), loss = 427.49359131\n",
      "ep 15: w = tensor([0.34364, 0.53338]), y_p = tensor([68.78387, 52.09386, 35.40386, 18.71385]), loss = 283.42648315\n",
      "ep 20: w = tensor([0.49880, 1.56850]), y_p = tensor([25.13912, 19.37721, 13.61530,  7.85340]), loss = 188.61242676\n",
      "ep 25: w = tensor([0.52316, 0.72007]), y_p = tensor([60.23328, 45.87371, 31.51414, 17.15457]), loss = 126.13953400\n",
      "ep 30: w = tensor([0.64554, 1.39771]), y_p = tensor([31.56791, 24.41181, 17.25570, 10.09960]), loss = 84.91027832\n",
      "ep 35: w = tensor([0.68103, 0.83984]), y_p = tensor([54.55604, 41.79293, 29.02982, 16.26671]), loss = 57.64197159\n",
      "ep 40: w = tensor([0.77979, 1.28313]), y_p = tensor([35.72054, 27.71401, 19.70748, 11.70095]), loss = 39.55477524\n",
      "ep 45: w = tensor([0.82057, 0.91600]), y_p = tensor([50.77066, 39.11558, 27.46050, 15.80541]), loss = 27.51074600\n",
      "ep 50: w = tensor([0.90194, 1.20568]), y_p = tensor([38.38666, 29.87982, 21.37299, 12.86615]), loss = 19.44941521\n",
      "ep 55: w = tensor([0.94440, 0.96380]), y_p = tensor([48.23264, 37.35892, 26.48521, 15.61149]), loss = 14.01713943\n",
      "ep 60: w = tensor([1.01265, 1.15282]), y_p = tensor([40.08362, 31.30023, 22.51685, 13.73346]), loss = 10.32425213\n",
      "ep 65: w = tensor([1.05460, 0.99321]), y_p = tensor([46.51847, 36.20624, 25.89402, 15.58179]), loss = 7.78575230\n",
      "ep 70: w = tensor([1.11272, 1.11631]), y_p = tensor([41.15016, 32.23169, 23.31323, 14.39476]), loss = 6.01640034\n",
      "ep 75: w = tensor([1.15289, 1.01076]), y_p = tensor([45.34982, 35.44982, 25.54981, 15.64980]), loss = 4.76234531\n",
      "ep 80: w = tensor([1.20299, 1.09071]), y_p = tensor([41.80791, 32.84242, 23.87693, 14.91144]), loss = 3.85587311\n",
      "ep 85: w = tensor([1.24068, 1.02071]), y_p = tensor([44.54358, 34.95336, 25.36314, 15.77292]), loss = 3.18603969\n",
      "ep 90: w = tensor([1.28429, 1.07244]), y_p = tensor([42.20184, 33.24281, 24.28377, 15.32473]), loss = 2.67915201\n",
      "ep 95: w = tensor([1.31920, 1.02583]), y_p = tensor([43.97911, 34.62745, 25.27578, 15.92412]), loss = 2.28608990\n",
      "ep 100: w = tensor([1.35745, 1.05912]), y_p = tensor([42.42657, 33.50523, 24.58390, 15.66256]), loss = 1.97393024\n",
      "ep 105: w = tensor([1.38948, 1.02793]), y_p = tensor([43.57692, 34.41344, 25.24997, 16.08649]), loss = 1.72041523\n",
      "ep 110: w = tensor([1.42322, 1.04919]), y_p = tensor([42.54377, 33.67719, 24.81061, 15.94404]), loss = 1.51038074\n",
      "ep 115: w = tensor([1.45243, 1.02818]), y_p = tensor([43.28445, 34.27287, 25.26129, 16.24971]), loss = 1.33336711\n",
      "ep 120: w = tensor([1.48233, 1.04161]), y_p = tensor([42.59350, 33.78979, 24.98609, 16.18238]), loss = 1.18204284\n",
      "ep 125: w = tensor([1.50884, 1.02733]), y_p = tensor([43.06691, 34.18049, 25.29408, 16.40766]), loss = 1.05118954\n",
      "ep 130: w = tensor([1.53541, 1.03567]), y_p = tensor([42.60181, 33.86351, 25.12522, 16.38693]), loss = 0.93700927\n",
      "ep 135: w = tensor([1.55940, 1.02587]), y_p = tensor([42.90114, 34.11973, 25.33832, 16.55691]), loss = 0.83667958\n",
      "ep 140: w = tensor([1.58308, 1.03092]), y_p = tensor([42.58543, 33.91173, 25.23804, 16.56435]), loss = 0.74805033\n",
      "ep 145: w = tensor([1.60474, 1.02409]), y_p = tensor([42.77172, 34.07974, 25.38776, 16.69577]), loss = 0.66943973\n",
      "ep 150: w = tensor([1.62588, 1.02703]), y_p = tensor([42.55507, 33.94322, 25.33137, 16.71952]), loss = 0.59950614\n",
      "ep 155: w = tensor([1.64540, 1.02219]), y_p = tensor([42.66827, 34.05339, 25.43850, 16.82361]), loss = 0.53715116\n",
      "ep 160: w = tensor([1.66429, 1.02378]), y_p = tensor([42.51758, 33.96376, 25.40994, 16.85612]), loss = 0.48146009\n",
      "ep 165: w = tensor([1.68186, 1.02029]), y_p = tensor([42.58377, 34.03600, 25.48822, 16.94045]), loss = 0.43166173\n",
      "ep 170: w = tensor([1.69877, 1.02103]), y_p = tensor([42.47721, 33.97713, 25.47704, 16.97695]), loss = 0.38709140\n",
      "ep 175: w = tensor([1.71457, 1.01845]), y_p = tensor([42.51338, 34.02448, 25.53557, 17.04667]), loss = 0.34717295\n",
      "ep 180: w = tensor([1.72971, 1.01867]), y_p = tensor([42.43659, 33.98581, 25.53503, 17.08425]), loss = 0.31140509\n",
      "ep 185: w = tensor([1.74391, 1.01672]), y_p = tensor([42.45380, 34.01683, 25.57986, 17.14289]), loss = 0.27934361\n",
      "ep 190: w = tensor([1.75748, 1.01662]), y_p = tensor([42.39725, 33.99142, 25.58560, 17.17977]), loss = 0.25059801\n",
      "ep 195: w = tensor([1.77024, 1.01511]), y_p = tensor([42.40269, 34.01173, 25.62078, 17.22982]), loss = 0.22481927\n",
      "ep 200: w = tensor([1.78240, 1.01482]), y_p = tensor([42.36006, 33.99504, 25.63001, 17.26499]), loss = 0.20169993\n",
      "ep 205: w = tensor([1.79385, 1.01363]), y_p = tensor([42.35838, 34.00832, 25.65827, 17.30822]), loss = 0.18096074\n",
      "ep 210: w = tensor([1.80476, 1.01324]), y_p = tensor([42.32542, 33.99732, 25.66922, 17.34113]), loss = 0.16235729\n",
      "ep 215: w = tensor([1.81504, 1.01227]), y_p = tensor([42.31963, 34.00603, 25.69242, 17.37882]), loss = 0.14566770\n",
      "ep 220: w = tensor([1.82482, 1.01185]), y_p = tensor([42.29353, 33.99876, 25.70400, 17.40924]), loss = 0.13069558\n",
      "ep 225: w = tensor([1.83405, 1.01104]), y_p = tensor([42.28552, 34.00446, 25.72339, 17.44233]), loss = 0.11726224\n",
      "ep 230: w = tensor([1.84282, 1.01060]), y_p = tensor([42.26440, 33.99967, 25.73494, 17.47021]), loss = 0.10521053\n",
      "ep 235: w = tensor([1.85111, 1.00993]), y_p = tensor([42.25534, 34.00337, 25.75140, 17.49942]), loss = 0.09439759\n",
      "ep 240: w = tensor([1.85897, 1.00950]), y_p = tensor([42.23791, 34.00021, 25.76252, 17.52483]), loss = 0.08469677\n",
      "ep 245: w = tensor([1.86641, 1.00892]), y_p = tensor([42.22856, 34.00261, 25.77667, 17.55073]), loss = 0.07599217\n",
      "ep 250: w = tensor([1.87346, 1.00851]), y_p = tensor([42.21390, 34.00052, 25.78715, 17.57377]), loss = 0.06818256\n",
      "ep 255: w = tensor([1.88014, 1.00801]), y_p = tensor([42.20472, 34.00208, 25.79945, 17.59681]), loss = 0.06117591\n",
      "ep 260: w = tensor([1.88647, 1.00763]), y_p = tensor([42.19220, 34.00068, 25.80916, 17.61764]), loss = 0.05488885\n",
      "ep 265: w = tensor([1.89246, 1.00719]), y_p = tensor([42.18345, 34.00169, 25.81994, 17.63818]), loss = 0.04924808\n",
      "ep 270: w = tensor([1.89813, 1.00684]), y_p = tensor([42.17264, 34.00076, 25.82887, 17.65699]), loss = 0.04418735\n",
      "ep 275: w = tensor([1.90351, 1.00646]), y_p = tensor([42.16444, 34.00140, 25.83836, 17.67532]), loss = 0.03964623\n",
      "ep 280: w = tensor([1.90860, 1.00614]), y_p = tensor([42.15502, 34.00077, 25.84652, 17.69227]), loss = 0.03557184\n",
      "ep 285: w = tensor([1.91342, 1.00580]), y_p = tensor([42.14744, 34.00118, 25.85492, 17.70866]), loss = 0.03191639\n",
      "ep 290: w = tensor([1.91799, 1.00550]), y_p = tensor([42.13918, 34.00076, 25.86233, 17.72391]), loss = 0.02863660\n",
      "ep 295: w = tensor([1.92232, 1.00520]), y_p = tensor([42.13222, 34.00101, 25.86980, 17.73858]), loss = 0.02569385\n",
      "ep 300: w = tensor([1.92642, 1.00494]), y_p = tensor([42.12493, 34.00072, 25.87651, 17.75230]), loss = 0.02305340\n",
      "ep 305: w = tensor([1.93030, 1.00467]), y_p = tensor([42.11858, 34.00087, 25.88315, 17.76544]), loss = 0.02068412\n",
      "ep 310: w = tensor([1.93398, 1.00443]), y_p = tensor([42.11213, 34.00067, 25.88922, 17.77776]), loss = 0.01855873\n",
      "ep 315: w = tensor([1.93747, 1.00419]), y_p = tensor([42.10637, 34.00076, 25.89515, 17.78953]), loss = 0.01665138\n",
      "ep 320: w = tensor([1.94077, 1.00397]), y_p = tensor([42.10064, 34.00063, 25.90062, 17.80061]), loss = 0.01494033\n",
      "ep 325: w = tensor([1.94389, 1.00376]), y_p = tensor([42.09541, 34.00066, 25.90591, 17.81116]), loss = 0.01340491\n",
      "ep 330: w = tensor([1.94685, 1.00356]), y_p = tensor([42.09031, 34.00057, 25.91084, 17.82110]), loss = 0.01202754\n",
      "ep 335: w = tensor([1.94966, 1.00337]), y_p = tensor([42.08558, 34.00058, 25.91557, 17.83055]), loss = 0.01079139\n",
      "ep 340: w = tensor([1.95231, 1.00320]), y_p = tensor([42.08104, 34.00053, 25.92001, 17.83949]), loss = 0.00968240\n",
      "ep 345: w = tensor([1.95483, 1.00303]), y_p = tensor([42.07679, 34.00052, 25.92424, 17.84797]), loss = 0.00868748\n",
      "ep 350: w = tensor([1.95722, 1.00287]), y_p = tensor([42.07271, 34.00047, 25.92823, 17.85599]), loss = 0.00779471\n",
      "ep 355: w = tensor([1.95947, 1.00272]), y_p = tensor([42.06890, 34.00047, 25.93203, 17.86359]), loss = 0.00699359\n",
      "ep 360: w = tensor([1.96161, 1.00257]), y_p = tensor([42.06524, 34.00042, 25.93560, 17.87078]), loss = 0.00627505\n",
      "ep 365: w = tensor([1.96364, 1.00244]), y_p = tensor([42.06182, 34.00042, 25.93901, 17.87761]), loss = 0.00563018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 370: w = tensor([1.96556, 1.00231]), y_p = tensor([42.05854, 34.00038, 25.94222, 17.88406]), loss = 0.00505164\n",
      "ep 375: w = tensor([1.96737, 1.00219]), y_p = tensor([42.05547, 34.00037, 25.94528, 17.89019]), loss = 0.00453248\n",
      "ep 380: w = tensor([1.96910, 1.00207]), y_p = tensor([42.05252, 34.00034, 25.94816, 17.89598]), loss = 0.00406663\n",
      "ep 385: w = tensor([1.97073, 1.00196]), y_p = tensor([42.04977, 34.00034, 25.95090, 17.90147]), loss = 0.00364873\n",
      "ep 390: w = tensor([1.97227, 1.00186]), y_p = tensor([42.04712, 34.00031, 25.95349, 17.90667]), loss = 0.00327375\n",
      "ep 395: w = tensor([1.97374, 1.00176]), y_p = tensor([42.04465, 34.00030, 25.95595, 17.91160]), loss = 0.00293732\n",
      "ep 400: w = tensor([1.97512, 1.00167]), y_p = tensor([42.04229, 34.00028, 25.95827, 17.91626]), loss = 0.00263548\n",
      "ep 405: w = tensor([1.97643, 1.00158]), y_p = tensor([42.04005, 34.00026, 25.96047, 17.92068]), loss = 0.00236459\n",
      "ep 410: w = tensor([1.97768, 1.00150]), y_p = tensor([42.03794, 34.00025, 25.96256, 17.92487]), loss = 0.00212172\n",
      "ep 415: w = tensor([1.97886, 1.00142]), y_p = tensor([42.03594, 34.00024, 25.96453, 17.92883]), loss = 0.00190360\n",
      "ep 420: w = tensor([1.97997, 1.00134]), y_p = tensor([42.03405, 34.00023, 25.96641, 17.93259]), loss = 0.00170798\n",
      "ep 425: w = tensor([1.98103, 1.00127]), y_p = tensor([42.03225, 34.00022, 25.96818, 17.93615]), loss = 0.00153252\n",
      "ep 430: w = tensor([1.98203, 1.00120]), y_p = tensor([42.03054, 34.00020, 25.96986, 17.93951]), loss = 0.00137501\n",
      "ep 435: w = tensor([1.98298, 1.00114]), y_p = tensor([42.02893, 34.00019, 25.97145, 17.94271]), loss = 0.00123367\n",
      "ep 440: w = tensor([1.98388, 1.00108]), y_p = tensor([42.02741, 34.00019, 25.97296, 17.94573]), loss = 0.00110684\n",
      "ep 445: w = tensor([1.98473, 1.00102]), y_p = tensor([42.02594, 34.00016, 25.97437, 17.94859]), loss = 0.00099315\n",
      "ep 450: w = tensor([1.98553, 1.00097]), y_p = tensor([42.02460, 34.00017, 25.97574, 17.95131]), loss = 0.00089105\n",
      "ep 455: w = tensor([1.98630, 1.00092]), y_p = tensor([42.02329, 34.00015, 25.97701, 17.95388]), loss = 0.00079952\n",
      "ep 460: w = tensor([1.98702, 1.00087]), y_p = tensor([42.02205, 34.00014, 25.97823, 17.95631]), loss = 0.00071731\n",
      "ep 465: w = tensor([1.98771, 1.00082]), y_p = tensor([42.02092, 34.00015, 25.97939, 17.95862]), loss = 0.00064363\n",
      "ep 470: w = tensor([1.98835, 1.00078]), y_p = tensor([42.01978, 34.00011, 25.98046, 17.96080]), loss = 0.00057748\n",
      "ep 475: w = tensor([1.98897, 1.00074]), y_p = tensor([42.01876, 34.00014, 25.98150, 17.96288]), loss = 0.00051811\n",
      "ep 480: w = tensor([1.98955, 1.00070]), y_p = tensor([42.01775, 34.00011, 25.98247, 17.96483]), loss = 0.00046487\n",
      "ep 485: w = tensor([1.99010, 1.00066]), y_p = tensor([42.01683, 34.00012, 25.98340, 17.96669]), loss = 0.00041709\n",
      "ep 490: w = tensor([1.99063, 1.00063]), y_p = tensor([42.01592, 34.00009, 25.98427, 17.96844]), loss = 0.00037425\n",
      "ep 495: w = tensor([1.99112, 1.00060]), y_p = tensor([42.01510, 34.00010, 25.98511, 17.97011]), loss = 0.00033581\n",
      "ep 500: w = tensor([1.99159, 1.00056]), y_p = tensor([42.01431, 34.00010, 25.98589, 17.97169]), loss = 0.00030129\n",
      "ep 505: w = tensor([1.99203, 1.00053]), y_p = tensor([42.01354, 34.00008, 25.98663, 17.97318]), loss = 0.00027033\n",
      "ep 510: w = tensor([1.99245, 1.00051]), y_p = tensor([42.01283, 34.00009, 25.98734, 17.97460]), loss = 0.00024254\n",
      "ep 515: w = tensor([1.99285, 1.00048]), y_p = tensor([42.01215, 34.00007, 25.98800, 17.97593]), loss = 0.00021764\n",
      "ep 520: w = tensor([1.99323, 1.00045]), y_p = tensor([42.01151, 34.00008, 25.98864, 17.97721]), loss = 0.00019526\n",
      "ep 525: w = tensor([1.99359, 1.00043]), y_p = tensor([42.01090, 34.00007, 25.98924, 17.97841]), loss = 0.00017518\n",
      "ep 530: w = tensor([1.99392, 1.00041]), y_p = tensor([42.01033, 34.00007, 25.98981, 17.97955]), loss = 0.00015719\n",
      "ep 535: w = tensor([1.99425, 1.00039]), y_p = tensor([42.00978, 34.00006, 25.99035, 17.98063]), loss = 0.00014103\n",
      "ep 540: w = tensor([1.99455, 1.00037]), y_p = tensor([42.00927, 34.00006, 25.99086, 17.98165]), loss = 0.00012652\n",
      "ep 545: w = tensor([1.99484, 1.00035]), y_p = tensor([42.00878, 34.00006, 25.99134, 17.98262]), loss = 0.00011353\n",
      "ep 550: w = tensor([1.99511, 1.00033]), y_p = tensor([42.00831, 34.00005, 25.99179, 17.98354]), loss = 0.00010187\n",
      "ep 555: w = tensor([1.99537, 1.00031]), y_p = tensor([42.00787, 34.00005, 25.99223, 17.98441]), loss = 0.00009140\n",
      "ep 560: w = tensor([1.99561, 1.00029]), y_p = tensor([42.00747, 34.00006, 25.99264, 17.98523]), loss = 0.00008202\n",
      "ep 565: w = tensor([1.99584, 1.00028]), y_p = tensor([42.00705, 34.00004, 25.99302, 17.98600]), loss = 0.00007359\n",
      "ep 570: w = tensor([1.99606, 1.00026]), y_p = tensor([42.00670, 34.00005, 25.99340, 17.98675]), loss = 0.00006602\n",
      "ep 575: w = tensor([1.99627, 1.00025]), y_p = tensor([42.00633, 34.00003, 25.99374, 17.98744]), loss = 0.00005925\n",
      "ep 580: w = tensor([1.99647, 1.00024]), y_p = tensor([42.00601, 34.00005, 25.99408, 17.98811]), loss = 0.00005316\n",
      "ep 585: w = tensor([1.99665, 1.00022]), y_p = tensor([42.00569, 34.00003, 25.99439, 17.98874]), loss = 0.00004769\n",
      "ep 590: w = tensor([1.99683, 1.00021]), y_p = tensor([42.00538, 34.00003, 25.99468, 17.98933]), loss = 0.00004279\n",
      "ep 595: w = tensor([1.99700, 1.00020]), y_p = tensor([42.00511, 34.00004, 25.99497, 17.98989]), loss = 0.00003839\n",
      "ep 600: w = tensor([1.99716, 1.00019]), y_p = tensor([42.00483, 34.00003, 25.99523, 17.99043]), loss = 0.00003444\n",
      "ep 605: w = tensor([1.99731, 1.00018]), y_p = tensor([42.00458, 34.00003, 25.99548, 17.99093]), loss = 0.00003091\n",
      "ep 610: w = tensor([1.99745, 1.00017]), y_p = tensor([42.00435, 34.00003, 25.99572, 17.99141]), loss = 0.00002773\n",
      "ep 615: w = tensor([1.99758, 1.00016]), y_p = tensor([42.00410, 34.00002, 25.99594, 17.99186]), loss = 0.00002489\n",
      "ep 620: w = tensor([1.99771, 1.00015]), y_p = tensor([42.00390, 34.00003, 25.99616, 17.99229]), loss = 0.00002233\n",
      "ep 625: w = tensor([1.99783, 1.00015]), y_p = tensor([42.00368, 34.00002, 25.99636, 17.99270]), loss = 0.00002004\n",
      "ep 630: w = tensor([1.99795, 1.00014]), y_p = tensor([42.00349, 34.00002, 25.99655, 17.99308]), loss = 0.00001798\n",
      "ep 635: w = tensor([1.99805, 1.00013]), y_p = tensor([42.00330, 34.00002, 25.99673, 17.99345]), loss = 0.00001613\n",
      "ep 640: w = tensor([1.99816, 1.00012]), y_p = tensor([42.00313, 34.00002, 25.99691, 17.99379]), loss = 0.00001448\n",
      "ep 645: w = tensor([1.99825, 1.00012]), y_p = tensor([42.00298, 34.00003, 25.99708, 17.99413]), loss = 0.00001298\n",
      "ep 650: w = tensor([1.99835, 1.00011]), y_p = tensor([42.00280, 34.00001, 25.99722, 17.99443]), loss = 0.00001165\n",
      "ep 655: w = tensor([1.99843, 1.00010]), y_p = tensor([42.00267, 34.00002, 25.99738, 17.99473]), loss = 0.00001045\n",
      "ep 660: w = tensor([1.99852, 1.00010]), y_p = tensor([42.00251, 34.00001, 25.99751, 17.99500]), loss = 0.00000938\n",
      "ep 665: w = tensor([1.99859, 1.00009]), y_p = tensor([42.00239, 34.00002, 25.99764, 17.99527]), loss = 0.00000841\n",
      "ep 670: w = tensor([1.99867, 1.00009]), y_p = tensor([42.00225, 34.00001, 25.99776, 17.99552]), loss = 0.00000755\n",
      "ep 675: w = tensor([1.99874, 1.00008]), y_p = tensor([42.00216, 34.00003, 25.99789, 17.99576]), loss = 0.00000678\n",
      "ep 680: w = tensor([1.99881, 1.00008]), y_p = tensor([42.00201, 34.00000, 25.99799, 17.99597]), loss = 0.00000608\n",
      "ep 685: w = tensor([1.99887, 1.00008]), y_p = tensor([42.00194, 34.00002, 25.99811, 17.99619]), loss = 0.00000546\n",
      "ep 690: w = tensor([1.99893, 1.00007]), y_p = tensor([42.00181, 34.00000, 25.99820, 17.99639]), loss = 0.00000489\n",
      "ep 695: w = tensor([1.99898, 1.00007]), y_p = tensor([42.00173, 34.00002, 25.99830, 17.99658]), loss = 0.00000439\n",
      "ep 700: w = tensor([1.99904, 1.00006]), y_p = tensor([42.00164, 34.00002, 25.99839, 17.99677]), loss = 0.00000394\n",
      "ep 705: w = tensor([1.99909, 1.00006]), y_p = tensor([42.00154, 34.00001, 25.99847, 17.99693]), loss = 0.00000353\n",
      "ep 710: w = tensor([1.99914, 1.00006]), y_p = tensor([42.00146, 34.00000, 25.99855, 17.99709]), loss = 0.00000317\n",
      "ep 715: w = tensor([1.99918, 1.00005]), y_p = tensor([42.00139, 34.00002, 25.99863, 17.99725]), loss = 0.00000284\n",
      "ep 720: w = tensor([1.99923, 1.00005]), y_p = tensor([42.00131, 34.00000, 25.99870, 17.99739]), loss = 0.00000255\n",
      "ep 725: w = tensor([1.99927, 1.00005]), y_p = tensor([42.00126, 34.00002, 25.99878, 17.99753]), loss = 0.00000229\n",
      "ep 730: w = tensor([1.99931, 1.00005]), y_p = tensor([42.00117, 34.00000, 25.99883, 17.99766]), loss = 0.00000206\n",
      "ep 735: w = tensor([1.99934, 1.00004]), y_p = tensor([42.00113, 34.00002, 25.99890, 17.99779]), loss = 0.00000184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 740: w = tensor([1.99938, 1.00004]), y_p = tensor([42.00105, 34.00000, 25.99895, 17.99790]), loss = 0.00000166\n",
      "ep 745: w = tensor([1.99941, 1.00004]), y_p = tensor([42.00101, 34.00001, 25.99901, 17.99801]), loss = 0.00000149\n",
      "ep 750: w = tensor([1.99944, 1.00004]), y_p = tensor([42.00096, 34.00001, 25.99907, 17.99812]), loss = 0.00000133\n",
      "ep 755: w = tensor([1.99947, 1.00004]), y_p = tensor([42.00090, 34.00000, 25.99911, 17.99822]), loss = 0.00000119\n",
      "ep 760: w = tensor([1.99950, 1.00003]), y_p = tensor([42.00086, 34.00001, 25.99916, 17.99831]), loss = 0.00000107\n",
      "ep 765: w = tensor([1.99952, 1.00003]), y_p = tensor([42.00081, 34.00000, 25.99920, 17.99840]), loss = 0.00000096\n",
      "ep 770: w = tensor([1.99955, 1.00003]), y_p = tensor([42.00076, 34.00000, 25.99924, 17.99848]), loss = 0.00000086\n",
      "ep 775: w = tensor([1.99957, 1.00003]), y_p = tensor([42.00073, 34.00001, 25.99929, 17.99857]), loss = 0.00000077\n",
      "ep 780: w = tensor([1.99960, 1.00003]), y_p = tensor([42.00068, 34.00000, 25.99932, 17.99864]), loss = 0.00000069\n",
      "ep 785: w = tensor([1.99962, 1.00003]), y_p = tensor([42.00065, 34.00001, 25.99936, 17.99871]), loss = 0.00000062\n",
      "ep 790: w = tensor([1.99964, 1.00002]), y_p = tensor([42.00062, 34.00001, 25.99940, 17.99878]), loss = 0.00000056\n",
      "ep 795: w = tensor([1.99966, 1.00002]), y_p = tensor([42.00059, 34.00001, 25.99943, 17.99885]), loss = 0.00000050\n",
      "ep 800: w = tensor([1.99967, 1.00002]), y_p = tensor([42.00055, 34.00000, 25.99945, 17.99891]), loss = 0.00000045\n",
      "ep 805: w = tensor([1.99969, 1.00002]), y_p = tensor([42.00053, 34.00001, 25.99949, 17.99896]), loss = 0.00000040\n",
      "ep 810: w = tensor([1.99971, 1.00002]), y_p = tensor([42.00049, 34.00000, 25.99951, 17.99902]), loss = 0.00000036\n",
      "ep 815: w = tensor([1.99972, 1.00002]), y_p = tensor([42.00047, 34.00000, 25.99954, 17.99907]), loss = 0.00000033\n",
      "ep 820: w = tensor([1.99974, 1.00002]), y_p = tensor([42.00045, 34.00000, 25.99956, 17.99912]), loss = 0.00000029\n",
      "ep 825: w = tensor([1.99975, 1.00002]), y_p = tensor([42.00042, 34.00000, 25.99958, 17.99916]), loss = 0.00000026\n",
      "ep 830: w = tensor([1.99977, 1.00002]), y_p = tensor([42.00040, 34.00000, 25.99960, 17.99921]), loss = 0.00000024\n",
      "ep 835: w = tensor([1.99978, 1.00001]), y_p = tensor([42.00038, 34.00000, 25.99963, 17.99925]), loss = 0.00000021\n",
      "ep 840: w = tensor([1.99979, 1.00001]), y_p = tensor([42.00035, 34.00000, 25.99965, 17.99929]), loss = 0.00000019\n",
      "ep 845: w = tensor([1.99980, 1.00001]), y_p = tensor([42.00034, 34.00000, 25.99967, 17.99933]), loss = 0.00000017\n",
      "ep 850: w = tensor([1.99981, 1.00001]), y_p = tensor([42.00032, 34.00000, 25.99968, 17.99936]), loss = 0.00000015\n",
      "ep 855: w = tensor([1.99982, 1.00001]), y_p = tensor([42.00030, 34.00000, 25.99970, 17.99940]), loss = 0.00000014\n",
      "ep 860: w = tensor([1.99983, 1.00001]), y_p = tensor([42.00028, 33.99999, 25.99971, 17.99943]), loss = 0.00000012\n",
      "ep 865: w = tensor([1.99984, 1.00001]), y_p = tensor([42.00027, 34.00000, 25.99973, 17.99946]), loss = 0.00000011\n",
      "ep 870: w = tensor([1.99985, 1.00001]), y_p = tensor([42.00026, 34.00000, 25.99975, 17.99949]), loss = 0.00000010\n",
      "ep 875: w = tensor([1.99986, 1.00001]), y_p = tensor([42.00023, 33.99999, 25.99975, 17.99951]), loss = 0.00000009\n",
      "ep 880: w = tensor([1.99986, 1.00001]), y_p = tensor([42.00024, 34.00001, 25.99978, 17.99954]), loss = 0.00000008\n",
      "ep 885: w = tensor([1.99987, 1.00001]), y_p = tensor([42.00021, 33.99999, 25.99978, 17.99956]), loss = 0.00000007\n",
      "ep 890: w = tensor([1.99988, 1.00001]), y_p = tensor([42.00024, 34.00002, 25.99981, 17.99960]), loss = 0.00000006\n",
      "ep 895: w = tensor([1.99988, 1.00001]), y_p = tensor([42.00018, 33.99998, 25.99980, 17.99961]), loss = 0.00000006\n",
      "ep 900: w = tensor([1.99989, 1.00001]), y_p = tensor([42.00020, 34.00001, 25.99982, 17.99963]), loss = 0.00000005\n",
      "ep 905: w = tensor([1.99990, 1.00001]), y_p = tensor([42.00016, 33.99999, 25.99982, 17.99965]), loss = 0.00000005\n",
      "ep 910: w = tensor([1.99990, 1.00001]), y_p = tensor([42.00018, 34.00001, 25.99984, 17.99967]), loss = 0.00000004\n",
      "ep 915: w = tensor([1.99991, 1.00001]), y_p = tensor([42.00016, 34.00000, 25.99985, 17.99969]), loss = 0.00000004\n",
      "ep 920: w = tensor([1.99991, 1.00001]), y_p = tensor([42.00015, 34.00000, 25.99985, 17.99970]), loss = 0.00000003\n",
      "ep 925: w = tensor([1.99992, 1.00001]), y_p = tensor([42.00014, 34.00000, 25.99986, 17.99972]), loss = 0.00000003\n",
      "ep 930: w = tensor([1.99992, 1.00001]), y_p = tensor([42.00013, 33.99999, 25.99986, 17.99973]), loss = 0.00000003\n",
      "ep 935: w = tensor([1.99992, 1.00000]), y_p = tensor([42.00013, 34.00000, 25.99988, 17.99975]), loss = 0.00000002\n",
      "ep 940: w = tensor([1.99993, 1.00001]), y_p = tensor([42.00011, 33.99999, 25.99988, 17.99976]), loss = 0.00000002\n",
      "ep 945: w = tensor([1.99993, 1.00000]), y_p = tensor([42.00012, 34.00000, 25.99989, 17.99977]), loss = 0.00000002\n",
      "ep 950: w = tensor([1.99994, 1.00000]), y_p = tensor([42.00010, 34.00000, 25.99989, 17.99978]), loss = 0.00000002\n",
      "ep 955: w = tensor([1.99994, 1.00000]), y_p = tensor([42.00011, 34.00000, 25.99990, 17.99980]), loss = 0.00000002\n",
      "ep 960: w = tensor([1.99994, 1.00000]), y_p = tensor([42.00010, 34.00000, 25.99990, 17.99981]), loss = 0.00000001\n",
      "ep 965: w = tensor([1.99995, 1.00000]), y_p = tensor([42.00010, 34.00000, 25.99991, 17.99982]), loss = 0.00000001\n",
      "ep 970: w = tensor([1.99995, 1.00000]), y_p = tensor([42.00008, 33.99999, 25.99991, 17.99982]), loss = 0.00000001\n",
      "ep 975: w = tensor([1.99995, 1.00000]), y_p = tensor([42.00010, 34.00001, 25.99993, 17.99984]), loss = 0.00000001\n",
      "ep 980: w = tensor([1.99995, 1.00000]), y_p = tensor([42.00007, 34.00000, 25.99992, 17.99984]), loss = 0.00000001\n",
      "ep 985: w = tensor([1.99996, 1.00000]), y_p = tensor([42.00008, 34.00001, 25.99993, 17.99986]), loss = 0.00000001\n",
      "ep 990: w = tensor([1.99996, 1.00000]), y_p = tensor([42.00007, 34.00000, 25.99993, 17.99986]), loss = 0.00000001\n",
      "ep 995: w = tensor([1.99996, 1.00000]), y_p = tensor([42.00006, 34.00000, 25.99993, 17.99987]), loss = 0.00000001\n",
      "ep 1000: w = tensor([1.99996, 1.00000]), y_p = tensor([42.00007, 34.00001, 25.99994, 17.99988]), loss = 0.00000001\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "# вариант #1:\n",
    "# learning_rate = 0.05\n",
    "# n_iters = 20 + 1\n",
    "\n",
    "# вариант #2:\n",
    "learning_rate = 0.0013\n",
    "n_iters = 1000 + 1\n",
    "\n",
    "# основной цикл:\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # calculate gradients\n",
    "    dw = gradient(X, Y, y_pred)\n",
    "\n",
    "    # update weights\n",
    "    w -= learning_rate * dw\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f'ep {epoch}: w = {w}, y_p = {y_pred}, loss = {l:.8f}')                \n",
    "#         print(f'ep {epoch}: w = {w}, y_p = {y_pred}, loss = {l:.8f}\\ngrad = {dw}')        \n",
    "#         print(f'ep {epoch:3}: y_p = {y_pred}, loss = {l:.8f} grd = {dw}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проблема поиска градиента <a class=\"anchor\" id=\"проблема-поиска\"></a>\n",
    "* [к оглавлению](#разделы)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <em class=\"qs\"></em> Проблема: как найти градиент для нейронной сети: $\\nabla_\\theta E(f_L(\\pmb{x}, \\pmb{\\theta}), \\pmb{y})$?\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/main_cycle_p3_v1.png\" alt=\"Прямой проход и оценка ошибки\" style=\"width: 400px;\"/><br/>\n",
    "    <b>Проблема поиска градиента в общей логике обучения нейронной сети</b>    \n",
    "</center> \n",
    "\n",
    "* Для решения этой задачи и используется __алгоритм обратного распространения ошибки__ (backpropagation). Суть алгоритма:\n",
    "    * рассчитывается ошибка между выходным вектором сети $\\hat{\\pmb{y}}$ и правильным ответом обучающего примера $\\pmb{y}$\n",
    "    * ошибка распростаняется от результата к источнику (в обратную сторону) для корректировки весов\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/ann_13.png\" alt=\"Пример обратного распространения ошибки\" style=\"width: 400px;\"/><br/>\n",
    "    <b><em class=\"ex\"></em>Пример обратного распространения ошибки</b>    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Рассчет градиента суперпозиции двух функций нескольких переменных__\n",
    "\n",
    "* Сначала рассмотрим подзадачу: как рассчитать градиент для $f_L(\\mathbf{x}, \\mathbf{w}_1, \\ldots, \\mathbf{w}_L )=h^L(h^{L-1}(\\ldots h^1(\\mathbf{x}, \\mathbf{w}_1), \\ldots, \\mathbf{w}_{L-1}),\\mathbf{w}_L)$? \n",
    "\n",
    "* Для этого нам нужно будет __рассчитывать градиент суперпозиции (сложной функции)__ состоящей из последовательного применения функций слоев $h^i$.\n",
    "\n",
    "* Вспомним, как рассчитать производную (градиент) суперпозиции нескольких функций.\n",
    "    * Пусть $z=f(y)$, $y=g(x)$\n",
    "    * Тогда производная суперпозиции функций (правило дифференцирования сложной функции (chain rule)): $\\frac{\\mathrm{d} z}{\\mathrm{d} x}=\\frac{\\mathrm{d} z}{\\mathrm{d} y}\\frac{\\mathrm{d} y}{\\mathrm{d} x}$\n",
    "    * Если $\\mathbf{x} \\in \\mathbb{R}^n$, $\\mathbf{y} \\in \\mathbb{R}^m$, а $\\mathbf{z} \\in \\mathbb{R}$, то: $\\frac{\\partial z }{\\partial x_i} = \\sum_j \\frac{\\partial z}{\\partial y_j} \\frac{\\partial y_j}{\\partial x_i}$\n",
    "\n",
    "<center> \n",
    "    \n",
    "__Примеры рассчета градиента суперпозиции двух функций нескольких переменных:__\n",
    "\n",
    "<img src=\"./img/ann_18.png\" alt=\"Примеры иерархий в нейронных сетях\" style=\"width: 500px;\"/>\n",
    "</center>\n",
    "Т.е. нам нужны градиенты по всем возможным путям (рассмотренным в обработном порядке) завимиостей переменных.\n",
    "\n",
    "Запись этой же задачи в векторной нотации: \n",
    "* $\\frac{\\mathrm{d} z}{\\mathrm{d} \\mathbf{x}} = \\nabla_x (z)= \\begin{pmatrix}\n",
    "    \\dfrac{\\partial z}{\\partial x_1} \\\\ \\cdots \\\\ \\dfrac{\\partial z}{\\partial x_n} \\end{pmatrix}=\\left ( \\frac{\\mathrm{d} \\mathbf{y}}{\\mathrm{d} \\mathbf{x}} \\right )^T \\cdot \\nabla_y (z) = J(\\mathbf{y}(\\mathbf{x}))^T \\cdot \\nabla_y (z)= J(\\mathbf{y}(\\mathbf{x}))^T \\cdot \\begin{pmatrix}\n",
    "    \\dfrac{\\partial z}{\\partial y_1} \\\\ \\cdots \\\\ \\dfrac{\\partial z}{\\partial y_m} \\end{pmatrix}$    \n",
    "* Где $J$ это Якобиан: $$J(\\mathbf{y}(\\mathbf{x})) = \\begin{pmatrix}\n",
    "    \\dfrac{\\partial y_1}{\\partial x_1} & \\cdots & \\dfrac{\\partial y_1}{\\partial x_n}\\\\\n",
    "    \\vdots & \\ddots & \\vdots\\\\\n",
    "    \\dfrac{\\partial y_m}{\\partial x_1} & \\cdots & \\dfrac{\\partial y_m}{\\partial x_n} \\end{pmatrix} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задача поиска градиента: $\\nabla_\\theta E(f_L(\\mathbf{x}, \\mathbf{\\theta}), \\mathbf{y})$__\n",
    "\n",
    "* Перейдем от $f_L$ к последовательному рассчету функций слоев $h^i$:\n",
    "$$\\nabla_\\theta E(f_L(\\mathbf{x}, \\mathbf{\\theta}), \\mathbf{y})=\\nabla_{\\mathbf{w}_i} E(f_L(\\mathbf{x}, \\mathbf{w}_1, \\ldots, \\mathbf{w}_L ), \\mathbf{y})=\\nabla_{\\mathbf{w}_i} E(h^L(h^{L-1}(\\ldots h^1(\\mathbf{x}, \\mathbf{w}_1), \\ldots, \\mathbf{w}_{L-1}),\\mathbf{w}_L), \\mathbf{y})$$\n",
    "\n",
    "* Обозначим через $\\mathbf{a}^l$ результат рассчета функции активации на слое $l$: $\\mathbf{a}^l=h^l(\\mathbf{x}_l,\\mathbf{w}_l)$. Тогда: $\\mathbf{x}_{l+1}=\\mathbf{a}_l$ (вход следущего слоя является результатом рассчета функции активации предыдущего слоя)\n",
    "\n",
    "* Тогда можно записать: $\\nabla_\\theta E(f_L(\\mathbf{x}, \\mathbf{\\theta}), \\mathbf{y})=\\nabla_\\theta E(\\mathbf{a}^L, \\mathbf{y})$. Функция потерь $E(\\mathbf{a}^L, \\mathbf{y})$ зависит от $\\mathbf{a}^L$, $\\mathbf{a}^L$ от $\\mathbf{a}^{L-1}$, ..., $\\mathbf{a}^{l+1}$ от $\\mathbf{a}^{l}$\n",
    "\n",
    "* Исходя из этого представления можно градиенты весов $l$-го слоя можно записать как: \n",
    "$$\\dfrac{\\partial E}{\\partial \\mathbf{w}_l}=\\color{blue}{ \\dfrac{\\partial E}{\\partial \\mathbf{a}_L} \\cdot \\dfrac{\\partial \\mathbf{a}_L}{\\partial \\mathbf{a}_{L-1}} \\cdot \\cdots \\cdot \\dfrac{\\partial \\mathbf{a}_{l+1}}{\\partial \\mathbf{a}_{l}}} \\cdot \\color{red}{ \\dfrac{\\partial \\mathbf{a}_{l}}{\\partial \\mathbf{w}_{l}}}$$\n",
    "\n",
    "* Произведение всех сомножетелей кроме последнего является градиентом функции потерь по результатам рассчета функции активации слоя $l$:\n",
    "$$\\color{blue} {\\dfrac{\\partial E}{\\partial \\mathbf{a}_L} \\cdot \\dfrac{\\partial \\mathbf{a}_L}{\\partial \\mathbf{a}_{L-1}} \\cdot \\cdots \\cdot \\dfrac{\\partial \\mathbf{a}_{l+1}}{\\partial \\mathbf{a}_{l}}} = \\color{blue} {\\dfrac{\\partial E}{\\partial \\mathbf{a}_l}}$$\n",
    "\n",
    "* Тогда:\n",
    "$$\\dfrac{\\partial E}{\\partial \\mathbf{w}_l}=\\left ( \\color{red}{ \\dfrac{\\partial \\mathbf{a}_{l}}{\\partial \\mathbf{w}_{l}}} \\right )^T \\cdot \\color{blue}{\\dfrac{\\partial E}{\\partial \\mathbf{a}_l}}$$ для рассчета $\\color{red}{\\dfrac{\\partial \\mathbf{a}_{l}}{\\partial \\mathbf{w}_{l}}}$ нам нужен только якобиан функции активации $l$-го слоя по параметрам слоя $\\mathbf{w}_{l}$. \n",
    "\n",
    "* Градиент функции потерь по результатам рассчета функции активации слоя  $l$ может быть рассчитан рекурсивно по результатам слоя $l$, собственно тут и происходит __обратное распространение__:\n",
    "$\\color{blue}{\\dfrac{\\partial E}{\\partial \\mathbf{a}_l}}=\\left ( \\dfrac{\\partial \\mathbf{a}_{l+1}}{\\partial \\mathbf{a}_{l}} \\right )^T \\cdot \\dfrac{\\partial E}{\\partial \\mathbf{a}_{l+1}}=\\left ( \\color{magenta}{\\dfrac{\\partial \\mathbf{a}_{l+1}}{\\partial \\mathbf{x}_{l+1}}} \\right )^T \\cdot \\color{blue}{\\dfrac{\\partial E}{\\partial \\mathbf{a}_{l+1}}}$ для рассчета $\\color{magenta}{\\dfrac{\\partial \\mathbf{a}_{l+1}}{\\partial \\mathbf{x}_{l+1}}}$ нам нужен только якобиан функции активации $l+1$-го слоя по входным значениям слоя $\\mathbf{x}_{l+1}$\n",
    "\n",
    "* Т.е. чтобы проводить обратное распространение ошибки, нам на каждом слое (например $l$-м) нужно рассчитывать два якобиана:\n",
    "    * якобиан функции активации $l$-го слоя по параметрам слоя $\\color{red}{\\dfrac{\\partial \\mathbf{a}_{l}}{\\partial \\mathbf{w}_{l}}}$ - он позволит рассчитать градиент $\\dfrac{\\partial E}{\\partial \\mathbf{w}_l}$ и сделать очередной шаг градиентного спуска для параметров этого слоя: $\\mathbf{w}_l^{t+1} = \\mathbf{w}_l^{t}-\\gamma\\nabla_{w_l} E(\\mathbf{w}^{t})=\\mathbf{w}_l^{t}-\\gamma \\dfrac{\\partial E(\\mathbf{w}^{t})}{\\partial \\mathbf{w}_l}$\n",
    "    * якобиан функции активации $l$-го слоя по входным значениям слоя: $\\color{magenta}{\\dfrac{\\partial \\mathbf{a}_{l}}{\\partial \\mathbf{x}_{l}}}$ - он позволит распространить ошибку на низлежащие слои.\n",
    "\n",
    "Такми образом при обучении нам нужна только __очень локальная информация, содержащаяся в самом слое__. Т.е.:\n",
    "* __нет нобходимости знать как устроены сосоедние слои__: между слоями __очень простой интерфейс__ \n",
    "* т.е. __можно создать модульную архитектуру для слоев нейронной сети__: каждый модуль рассчитывает значение функции активации на основе выходов на прямом проходе и распространяет ошибку пришедшую на выходы на обратном проходе; все модули станадртным образом стыкуются друг с другом\n",
    "* при модульной архитектуре граф нейронной сети может быть очень сложным, но его __рассчет выполняется по одной простой и универсальной схеме__\n",
    "* внутри __модули могут быть сложно устроены, это никак не меняет логику остальных модулей и всего процесса обучения__, главное чтобы модуль корректно выполнял прямой и обратный проход."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "\n",
    "##  Дифференцируемое программирование и реализация обратного распространения ошибки <a class=\"anchor\" id=\"дифференцируемое\"></a>\n",
    "* [к оглавлению](#разделы)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Почему Tensor *Flow*?__\n",
    "\n",
    "<em class=\"qs\"></em> Как реализовать __алгоритм обратного распространения ошибки__ удобно для использования в задачах моделирования ИНС?\n",
    "\n",
    "Основная абстракция TensorFlow, PyTorch и других аналогичных библиотеках - __граф потока вычислений__.\n",
    "\n",
    "* Рассматриваемые библиотеки обычно:\n",
    "    1. задают __граф потока вычислений__ (формирует объект отложенных вычислений)\n",
    "    2. запускают __процедуру выполненния отложенных вычислений__ и получает __результаты__ вычислений (в т.ч. ошибку модели).\n",
    "* Возможность в явном виде работать с графом потока вычислений дает большое приемущество для __автоматического решения задачи обратного распространения ошибки__, являющейся составляющей адачей обучения модели ИНС.\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/ker_6.png\" alt=\"Принцип устройства графа потока вычислений в TensorFlow\" style=\"width: 400px;\"/><br/>\n",
    "    <b>Принцип устройства графа потока вычислений в TensorFlow</b>    \n",
    "</center>\n",
    "\n",
    "* Нейронная сеть это иерархия (она может быть простой и очень сложной) связанных (последовательно применяемых) функций слоев ИНС. Модель сети $f_L$ может быть представленна как суперпозиция из $L$ слоев $h^i\\text{, }i \\in \\{1, \\ldots, L\\}$, каждый из которых параметризуется своими весами $w_i$:\n",
    "$$f_L(\\pmb{x}, \\pmb{\\theta})=f_L(\\pmb{x}, \\pmb{w}_1, \\ldots, \\pmb{w}_L )=h^L(h^{L-1}(\\ldots h^1(\\pmb{x}, \\pmb{w}_1), \\ldots, \\pmb{w}_{L-1}),\\pmb{w}_L)$$\n",
    "\n",
    "* Вычисление функций слоев и взаимосвязи между слоями формируют граф потока вычислений в библиотеке моделирования ИНС.\n",
    "\n",
    "<center> \n",
    "<img src=\"./img/ann_16.png\" alt=\"Примеры иерархий в нейронных сетях\" style=\"width: 400px;\"/><br/>\n",
    "    <b>Примеры иерархий в нейронных сетях</b>    \n",
    "</center>\n",
    "\n",
    "\n",
    "* По сути, ИНС это композиция модулей, представляющих собой слои нейронной сети:\n",
    "    * если сеть прямого распространения (feedforward), то все просто\n",
    "    * если сеть является направленным ациклическим графом, то существует правильный порядок применения функций\n",
    "    * в случае, если есть циклы, образующие рекуррентные связи, то существуют специальные подходы (будут рассмотрены позднее)\n",
    "\n",
    "\n",
    "* На обратном проходе (при обратном распространении ошибки) нам необходимо __дифференциировать сложную функцию__ многослойной ИНС\n",
    "$$\\nabla_\\theta E(f_L(\\mathbf{x}, \\mathbf{\\theta}), \\mathbf{y})=\\nabla_{\\mathbf{w}_i} E(f_L(\\mathbf{x}, \\mathbf{w}_1, \\ldots, \\mathbf{w}_L ), \\mathbf{y})=\\nabla_{\\mathbf{w}_i} E(h^L(h^{L-1}(\\ldots h^1(\\mathbf{x}, \\mathbf{w}_1), \\ldots, \\mathbf{w}_{L-1}),\\mathbf{w}_L), \\mathbf{y})$$\n",
    "* алгоритм обратного распространения ошибки позволяет свести эту задачу к дифференциированию составляющих функций, но для этого необходимо __храниить информацию о виде и взаимосвязях функций задействованных в расчете модели ИНС__, именно эта информация и хранится в графе потока вычислений. Это позволяет организовать __автоматическое дифференциирование__ сложной функци многослойной ИНС."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Дифференциируемое программирование__\n",
    "\n",
    "<em class=\"df\"></em> __Дифференциируемое программирование__ (differentiable programming) - парадигма программирования при которой программа (функция рассчета значения) может быть продифференциирована в любой точке, обычно с помощью __автоматического диффиренциирования__. \n",
    "\n",
    "Это свойство позволяет использовать к программе __методы оптимизации основанные на рассчете градиента__, обычно - __методы градиентного спуска__.\n",
    "\n",
    "Дифференциируемое программирование используется в:\n",
    "* глубоком обучении\n",
    "* глубоком обучении комбинированном с физическими моделями в робототехнике\n",
    "* специализированных методах трассировки лучей\n",
    "* обработке изображений\n",
    "\n",
    "Большинство фреймоврков для дифференциируемого программирования использует граф потока вычислений определяющий выполнение программы и ее структуры данных.\n",
    "\n",
    "Основные классы фреймворков для дифференциируемого программирования:\n",
    "* __статические__ - они компилируют граф потока вычислений. Типичные представители: TensorFlow, Theano и др. Плюсы и минусы\n",
    "    * <em class=\"pl\"></em> могут использовать оптимизацию при компиляции\n",
    "    * <em class=\"pl\"></em> легче масштабирются на большие системы\n",
    "    * <em class=\"mn\"></em> статичность ограничевает интерактивность\n",
    "    * <em class=\"mn\"></em> многие программы не могут реализовываться легко (в частности: циклы, рекурсия)\n",
    "* __динамические__ - динамически исполняют граф потока вычислений. Используют перегрузку операторов для записи. Типичные представители: PyTorch, AutoGradrFlow. Плюсы и минусы:\n",
    "    * <em class=\"pl\"></em> более простая и понятная запись программы\n",
    "    * <em class=\"mn\"></em> накладные расходы интерпретатора\n",
    "    * <em class=\"mn\"></em> невозможно использовать оптимизацию компилятора\n",
    "    * <em class=\"mn\"></em> хуже масштабируемость\n",
    "* статическая на основе разбора промежуточного представления синтаксического разбора исходной программы. Пример фрэймоврк Zygote (язык программирования Julia)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Прямой проход__:\n",
    "* Модули из графа обходятся один за одним начиная с узла входных данных и далее по мере готовности всех необходимых входных данных для очередного модуля, который еще не был обойден\n",
    "* Рассчет функций активации для каждого модуля по входным данным: $a_l=h_l(x_l, w_l)$\n",
    "* Промежуточные значения кэшируются, чтобы не рассчитывать их повторно (в сложном графе сети и при обратном проходе)\n",
    "* Выходы одних модулей становятся входами других модулей: $x_{l+1}=a_l$\n",
    "* Последним модулем рассчитывается сумма потерь для входных данных\n",
    "<center> \n",
    "    \n",
    "__Прямой и обратный проход процедуры обучения многослойной ИНС:__\n",
    "\n",
    "<img src=\"./img/ann_19.png\" alt=\"Прямой и обратный проход \" style=\"width: 300px;\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "__Обратный проход__:\n",
    "* Сначала должен быть произведен прямой проход. На входе обратного прохода известна сумма потерь.\n",
    "* Строится обратный порядок обхода графа зависимостей модулей.\n",
    "* Модули из графа обходятся один за одним начиная с узла рассчета функции потерь и далее по мере готовности всех необходимых входных данных для очередного модуля, который еще не был обойден \n",
    "* Для каждого модуля рассчитыватся якобиан функции активации по параметрам слоя $\\color{red}{\\dfrac{\\partial \\mathbf{a}_{l}}{\\partial \\mathbf{w}_{l}}}$ и якобиан функции активации по входным значениям слоя: $\\color{magenta}{\\dfrac{\\partial \\mathbf{a}_{l}}{\\partial \\mathbf{x}_{l}}}$ \n",
    "* По пришедшму в модуль градиенту ошибки (полученному из модулей использовавших результаты данного модуля на прямом проходе) $\\color{blue}{\\dfrac{\\partial E}{\\partial \\mathbf{a}_l}}$ рассчитывается:\n",
    "    * Градиент для шага градиентного спуска по параметрам модуля $w_l$: $\\dfrac{\\partial E}{\\partial \\mathbf{w}_l}=\\left ( \\color{red}{ \\dfrac{\\partial \\mathbf{a}_{l}}{\\partial \\mathbf{w}_{l}}} \\right )^T \\cdot \\color{blue}{\\dfrac{\\partial E}{\\partial \\mathbf{a}_l}}$\n",
    "    * Градиент ошбки, который передается в модули, поставившие данные в этот модуль во время прямого прохода: $\\color{blue}{\\dfrac{\\partial E}{\\partial \\mathbf{a}_{l-1}}}=\\left ( \\color{magenta}{\\dfrac{\\partial \\mathbf{a}_{l}}{\\partial \\mathbf{x}_{l}}} \\right )^T \\cdot \\color{blue}{\\dfrac{\\partial E}{\\partial \\mathbf{a}_{l}}}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "    \n",
    "__Пример: прямой проход, шаг 1__\n",
    "\n",
    "<img src=\"./img/bp_2.png\" alt=\"Пример\" style=\"width: 700px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "    \n",
    "__Пример: прямой проход, шаг 2__\n",
    "\n",
    "<img src=\"./img/bp_3.png\" alt=\"Пример\" style=\"width: 700px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "    \n",
    "__Пример: прямой проход, шаг 3__\n",
    "\n",
    "<img src=\"./img/bp_4.png\" alt=\"Пример\" style=\"width: 700px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "    \n",
    "__Пример: обратный проход, шаг 1__\n",
    "\n",
    "<img src=\"./img/bp_5.png\" alt=\"Пример\" style=\"width: 700px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "    \n",
    "__Пример: обратный проход, шаг 2__\n",
    "\n",
    "<img src=\"./img/bp_6.png\" alt=\"Пример\" style=\"width: 700px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "    \n",
    "__Производные популярных функций активации__\n",
    "\n",
    "<img src=\"./img/ann_17.png\" alt=\"Пример\" style=\"width: 500px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center> \n",
    "    \n",
    "__Пример: обратный проход, шаг 3__\n",
    "\n",
    "<img src=\"./img/bp_7.png\" alt=\"Пример\" style=\"width: 700px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":\n",
    "##  Автоматическое дифференциирование в PyTorch <a class=\"anchor\" id=\"автоматическое-PyTorch\"></a>\n",
    "* [к оглавлению](#разделы)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.02651, -0.59082,  0.87850], requires_grad=True)\n",
      "tensor([1.97349, 1.40918, 2.87850], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x0000025E9219BA30>\n"
     ]
    }
   ],
   "source": [
    "# The autograd package provides automatic differentiation \n",
    "# for all operations on Tensors\n",
    "\n",
    "# requires_grad = True -> tracks all operations on the tensor. \n",
    "x = torch.randn(3, requires_grad=True)\n",
    "y = x + 2\n",
    "\n",
    "# y was created as a result of an operation, so it has a grad_fn attribute.\n",
    "# grad_fn: references a Function that has created the Tensor\n",
    "print(x) # created by the user -> grad_fn is None\n",
    "print(y)\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11.68397,  5.95739, 24.85736], grad_fn=<MulBackward0>)\n",
      "tensor(14.16624, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Do more operations on y\n",
    "z = y * y * 3\n",
    "print(z)\n",
    "z = z.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.94698, 2.81837, 5.75701])\n"
     ]
    }
   ],
   "source": [
    "# Let's compute the gradients with backpropagation\n",
    "# When we finish our computation we can call .backward() and have all the gradients computed automatically.\n",
    "# The gradient for this tensor will be accumulated into .grad attribute.\n",
    "# It is the partial derivate of the function w.r.t. the tensor\n",
    "\n",
    "z.backward()\n",
    "print(x.grad) # dz/dx\n",
    "\n",
    "# Generally speaking, torch.autograd is an engine for computing vector-Jacobian product\n",
    "# It computes partial derivates while applying the chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "__Примеры рассчета градиента суперпозиции двух функций нескольких переменных:__\n",
    "\n",
    "<img src=\"./img/ann_18.png\" alt=\"Примеры иерархий в нейронных сетях\" style=\"width: 500px;\"/>\n",
    "</center>\n",
    "Т.е. нам нужны градиенты по всем возможным путям (рассмотренным в обработном порядке) завимиостей переменных.\n",
    "\n",
    "Запись этой же задачи в векторной нотации: \n",
    "* $\\frac{\\mathrm{d} z}{\\mathrm{d} \\mathbf{x}} = \\nabla_x (z)= \\begin{pmatrix}\n",
    "    \\dfrac{\\partial z}{\\partial x_1} \\\\ \\cdots \\\\ \\dfrac{\\partial z}{\\partial x_n} \\end{pmatrix}=\\left ( \\frac{\\mathrm{d} \\mathbf{y}}{\\mathrm{d} \\mathbf{x}} \\right )^T \\cdot \\nabla_y (z) = J(\\mathbf{y}(\\mathbf{x}))^T \\cdot \\nabla_y (z)= J(\\mathbf{y}(\\mathbf{x}))^T \\cdot \\begin{pmatrix}\n",
    "    \\dfrac{\\partial z}{\\partial y_1} \\\\ \\cdots \\\\ \\dfrac{\\partial z}{\\partial y_m} \\end{pmatrix}$    \n",
    "* Где $J$ это Якобиан: $$J(\\mathbf{y}(\\mathbf{x})) = \\begin{pmatrix}\n",
    "    \\dfrac{\\partial y_1}{\\partial x_1} & \\cdots & \\dfrac{\\partial y_1}{\\partial x_n}\\\\\n",
    "    \\vdots & \\ddots & \\vdots\\\\\n",
    "    \\dfrac{\\partial y_m}{\\partial x_1} & \\cdots & \\dfrac{\\partial y_m}{\\partial x_n} \\end{pmatrix} $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1122.91479, -2130.42871,  3587.57251], grad_fn=<MulBackward0>)\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# Model with non-scalar output:\n",
    "# If a Tensor is non-scalar (more than 1 elements), we need to specify arguments for backward() \n",
    "# specify a gradient argument that is a tensor of matching shape.\n",
    "# needed for vector-Jacobian product\n",
    "\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "for _ in range(10):\n",
    "    y = y * 2\n",
    "\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.04800e+02, 2.04800e+03, 2.04800e-01])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float32)\n",
    "y.backward(v)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "Stop a tensor from tracking history:\n",
    "For example during our training loop when we want to update our weights\n",
    "then this update operation should not be part of the gradient computation\n",
    "* `x.requires_grad_(False)`\n",
    "* `x.detach()`\n",
    "* wrap in `with torch.no_grad():`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.requires_grad = False\n",
      "b.grad_fn = None\n",
      "a.requires_grad = True\n",
      "b.grad_fn = <SumBackward0 object at 0x0000025E9219B370>\n"
     ]
    }
   ],
   "source": [
    "# .requires_grad_(...) changes an existing flag in-place.\n",
    "\n",
    "a = torch.randn(2, 2)\n",
    "print(f'a.requires_grad = {a.requires_grad}')\n",
    "\n",
    "b = ((a * 3) / (a - 1))\n",
    "print(f'b.grad_fn = {b.grad_fn}')\n",
    "      \n",
    "a.requires_grad_(True)\n",
    "print(f'a.requires_grad = {a.requires_grad}')\n",
    "\n",
    "b = (a * a).sum()\n",
    "print(f'b.grad_fn = {b.grad_fn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# .detach(): get a new Tensor with the same content but no gradient computation:\n",
    "a = torch.randn(2, 2, requires_grad=True)\n",
    "print(a.requires_grad)\n",
    "\n",
    "b = a.detach()\n",
    "print(b.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# wrap in 'with torch.no_grad():'\n",
    "a = torch.randn(2, 2, requires_grad=True)\n",
    "print(a.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Пример использования градиентного спуска с автоматическим диффиринциированием в PyTorch:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0, weights: tensor([1., 1., 1., 1.], requires_grad=True), weights.grad: tensor([3., 3., 3., 3.]), m_output: 12.000\n",
      "ep: 1, weights: tensor([0.70000, 0.70000, 0.70000, 0.70000], requires_grad=True), weights.grad: tensor([3., 3., 3., 3.]), m_output: 8.400\n",
      "ep: 2, weights: tensor([0.40000, 0.40000, 0.40000, 0.40000], requires_grad=True), weights.grad: tensor([3., 3., 3., 3.]), m_output: 4.800\n",
      "ep: 2 (FIN), weights: tensor([0.10000, 0.10000, 0.10000, 0.10000], requires_grad=True), weights.grad: tensor([0., 0., 0., 0.]), m_output: 4.800\n"
     ]
    }
   ],
   "source": [
    "# backward() accumulates the gradient for this tensor into .grad attribute.\n",
    "# !!! We need to be careful during optimization !!!\n",
    "# Use .zero_() to empty the gradients before a new optimization step!\n",
    "\n",
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    # just a dummy example\n",
    "    # 'forward pass'\n",
    "    model_output = (weights*3).sum()\n",
    "    \n",
    "    \n",
    "    model_output.backward()\n",
    "    \n",
    "    print(f'ep: {epoch}, weights: {weights}, weights.grad: {weights.grad}, m_output: {model_output:.3f}')\n",
    "\n",
    "    # optimize model, i.e. adjust weights...\n",
    "    with torch.no_grad():\n",
    "        weights -= 0.1 * weights.grad\n",
    "\n",
    "    # this is important! It affects the final weights & output\n",
    "    weights.grad.zero_()\n",
    "    \n",
    "print(f'ep: {epoch} (FIN), weights: {weights}, weights.grad: {weights.grad}, m_output: {model_output:.3f}')    \n",
    "\n",
    "# Optimizer has zero_grad() method\n",
    "# optimizer = torch.optim.SGD([weights], lr=0.1)\n",
    "# During training:\n",
    "# optimizer.step()\n",
    "# optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Автоматическое выполнение обратного прохода с помощью `l.backward()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w true value = tensor([2., 1.]), Y = tensor([42., 34., 26., 18.])\n",
      "ep 0: w = tensor([0.16900, 2.21000], requires_grad=True), loss = 980.00000000 grad = tensor([ -130., -1700.])\n",
      "ep 5: w = tensor([0.13813, 0.24422], requires_grad=True), loss = 646.58898926 grad = tensor([  77.23860, 1378.76135])\n",
      "ep 10: w = tensor([0.33966, 1.82453], requires_grad=True), loss = 427.49307251 grad = tensor([  -89.12965, -1114.91821])\n",
      "ep 15: w = tensor([0.34364, 0.53338], requires_grad=True), loss = 283.42614746 grad = tensor([ 47.01924, 904.69263])\n",
      "ep 20: w = tensor([0.49880, 1.56850], requires_grad=True), loss = 188.61228943 grad = tensor([ -61.92347, -731.13928])\n",
      "ep 25: w = tensor([0.52316, 0.72007], requires_grad=True), loss = 126.13926697 grad = tensor([ 27.57067, 593.68494])\n",
      "ep 30: w = tensor([0.64554, 1.39771], requires_grad=True), loss = 84.91012573 grad = tensor([ -43.72146, -479.40924])\n",
      "ep 35: w = tensor([0.68103, 0.83984], requires_grad=True), loss = 57.64197159 grad = tensor([ 15.14910, 389.64639])\n",
      "ep 40: w = tensor([0.77979, 1.28313], requires_grad=True), loss = 39.55477905 grad = tensor([ -31.46260, -314.29950])\n",
      "ep 45: w = tensor([0.82057, 0.91600], requires_grad=True), loss = 27.51074600 grad = tensor([  7.30249, 255.77905])\n",
      "ep 50: w = tensor([0.90194, 1.20568], requires_grad=True), loss = 19.44943237 grad = tensor([ -23.13506, -206.00888])\n",
      "ep 55: w = tensor([0.94440, 0.96380], requires_grad=True), loss = 14.01713943 grad = tensor([  2.42605, 167.94620])\n",
      "ep 60: w = tensor([1.01265, 1.15282], requires_grad=True), loss = 10.32425213 grad = tensor([ -17.41577, -134.98840])\n",
      "ep 65: w = tensor([1.05460, 0.99321], requires_grad=True), loss = 7.78575325 grad = tensor([ -0.52992, 110.31209])\n",
      "ep 70: w = tensor([1.11272, 1.11631], requires_grad=True), loss = 6.01642036 grad = tensor([-13.43388, -88.41566])\n",
      "ep 75: w = tensor([1.15289, 1.01076], requires_grad=True), loss = 4.76234579 grad = tensor([-2.25095, 72.49083])\n",
      "ep 80: w = tensor([1.20299, 1.09071], requires_grad=True), loss = 3.85587597 grad = tensor([-10.61534, -57.87885])\n",
      "ep 85: w = tensor([1.24068, 1.02071], requires_grad=True), loss = 3.18603969 grad = tensor([-3.18430, 47.66795])\n",
      "ep 90: w = tensor([1.28429, 1.07244], requires_grad=True), loss = 2.67915201 grad = tensor([ -8.58116, -37.85973])\n",
      "ep 95: w = tensor([1.31920, 1.02583], requires_grad=True), loss = 2.28610182 grad = tensor([-3.62107, 31.37278])\n",
      "ep 100: w = tensor([1.35745, 1.05912], requires_grad=True), loss = 1.97393394 grad = tensor([ -7.08053, -24.73854])\n",
      "ep 105: w = tensor([1.38948, 1.02793], requires_grad=True), loss = 1.72041774 grad = tensor([-3.75016, 20.67220])\n",
      "ep 110: w = tensor([1.42322, 1.04919], requires_grad=True), loss = 1.51038170 grad = tensor([ -5.94694, -16.14056])\n",
      "ep 115: w = tensor([1.45243, 1.02818], requires_grad=True), loss = 1.33336782 grad = tensor([-3.69356, 13.64340])\n",
      "ep 120: w = tensor([1.48233, 1.04161], requires_grad=True), loss = 1.18204451 grad = tensor([ -5.06955, -10.51023])\n",
      "ep 125: w = tensor([1.50884, 1.02733], requires_grad=True), loss = 1.05119181 grad = tensor([-3.52963,  9.02459])\n",
      "ep 130: w = tensor([1.53541, 1.03567], requires_grad=True), loss = 0.93700927 grad = tensor([-4.37389, -6.82430])\n",
      "ep 135: w = tensor([1.55940, 1.02587], requires_grad=True), loss = 0.83668095 grad = tensor([-3.30841,  5.98640])\n",
      "ep 140: w = tensor([1.58308, 1.03092], requires_grad=True), loss = 0.74805164 grad = tensor([-3.80985, -4.41404])\n",
      "ep 145: w = tensor([1.60474, 1.02409], requires_grad=True), loss = 0.66944206 grad = tensor([-3.06120,  3.98732])\n",
      "ep 150: w = tensor([1.62588, 1.02703], requires_grad=True), loss = 0.59950674 grad = tensor([-3.34317, -2.83932])\n",
      "ep 155: w = tensor([1.64540, 1.02219], requires_grad=True), loss = 0.53715217 grad = tensor([-2.80748,  2.66960])\n",
      "ep 160: w = tensor([1.66429, 1.02378], requires_grad=True), loss = 0.48146015 grad = tensor([-2.95031, -1.81211])\n",
      "ep 165: w = tensor([1.68186, 1.02029], requires_grad=True), loss = 0.43166173 grad = tensor([-2.55889,  1.79972])\n",
      "ep 170: w = tensor([1.69877, 1.02103], requires_grad=True), loss = 0.38709140 grad = tensor([-2.61480, -1.14371])\n",
      "ep 175: w = tensor([1.71457, 1.01845], requires_grad=True), loss = 0.34717295 grad = tensor([-2.32213,  1.22391])\n",
      "ep 180: w = tensor([1.72971, 1.01867], requires_grad=True), loss = 0.31140596 grad = tensor([-2.32487, -0.70953])\n",
      "ep 185: w = tensor([1.74391, 1.01672], requires_grad=True), loss = 0.27934441 grad = tensor([-2.10070,  0.84159])\n",
      "ep 190: w = tensor([1.75748, 1.01662], requires_grad=True), loss = 0.25059801 grad = tensor([-2.07201, -0.42871])\n",
      "ep 195: w = tensor([1.77024, 1.01511], requires_grad=True), loss = 0.22481927 grad = tensor([-1.89610,  0.58674])\n",
      "ep 200: w = tensor([1.78240, 1.01482], requires_grad=True), loss = 0.20169993 grad = tensor([-1.84993, -0.24814])\n",
      "ep 205: w = tensor([1.79385, 1.01363], requires_grad=True), loss = 0.18096074 grad = tensor([-1.70864,  0.41626])\n",
      "ep 210: w = tensor([1.80476, 1.01324], requires_grad=True), loss = 0.16235729 grad = tensor([-1.65388, -0.13396])\n",
      "ep 215: w = tensor([1.81504, 1.01227], requires_grad=True), loss = 0.14566770 grad = tensor([-1.53788,  0.30142])\n",
      "ep 220: w = tensor([1.82482, 1.01185], requires_grad=True), loss = 0.13069558 grad = tensor([-1.48000, -0.06169])\n",
      "ep 225: w = tensor([1.83405, 1.01104], requires_grad=True), loss = 0.11726224 grad = tensor([-1.38304,  0.22278])\n",
      "ep 230: w = tensor([1.84282, 1.01060], requires_grad=True), loss = 0.10521053 grad = tensor([-1.32529, -0.01637])\n",
      "ep 235: w = tensor([1.85111, 1.00993], requires_grad=True), loss = 0.09439759 grad = tensor([-1.24302,  0.16842])\n",
      "ep 240: w = tensor([1.85897, 1.00950], requires_grad=True), loss = 0.08469677 grad = tensor([-1.18740,  0.01068])\n",
      "ep 245: w = tensor([1.86641, 1.00892], requires_grad=True), loss = 0.07599217 grad = tensor([-1.11665,  0.13068])\n",
      "ep 250: w = tensor([1.87346, 1.00851], requires_grad=True), loss = 0.06818256 grad = tensor([-1.06427,  0.02612])\n",
      "ep 255: w = tensor([1.88014, 1.00801], requires_grad=True), loss = 0.06117591 grad = tensor([-1.00278,  0.10420])\n",
      "ep 260: w = tensor([1.88647, 1.00763], requires_grad=True), loss = 0.05488885 grad = tensor([-0.95419,  0.03394])\n",
      "ep 265: w = tensor([1.89246, 1.00719], requires_grad=True), loss = 0.04924808 grad = tensor([-0.90031,  0.08475])\n",
      "ep 270: w = tensor([1.89813, 1.00684], requires_grad=True), loss = 0.04418735 grad = tensor([-0.85564,  0.03786])\n",
      "ep 275: w = tensor([1.90351, 1.00646], requires_grad=True), loss = 0.03964623 grad = tensor([-0.80819,  0.07013])\n",
      "ep 280: w = tensor([1.90860, 1.00614], requires_grad=True), loss = 0.03557184 grad = tensor([-0.76740,  0.03853])\n",
      "ep 285: w = tensor([1.91342, 1.00580], requires_grad=True), loss = 0.03191639 grad = tensor([-0.72539,  0.05912])\n",
      "ep 290: w = tensor([1.91799, 1.00550], requires_grad=True), loss = 0.02863660 grad = tensor([-0.68833,  0.03774])\n",
      "ep 295: w = tensor([1.92232, 1.00520], requires_grad=True), loss = 0.02569385 grad = tensor([-0.65102,  0.05041])\n",
      "ep 300: w = tensor([1.92642, 1.00494], requires_grad=True), loss = 0.02305340 grad = tensor([-0.61746,  0.03589])\n",
      "ep 305: w = tensor([1.93030, 1.00467], requires_grad=True), loss = 0.02068412 grad = tensor([-0.58424,  0.04342])\n",
      "ep 310: w = tensor([1.93398, 1.00443], requires_grad=True), loss = 0.01855873 grad = tensor([-0.55391,  0.03366])\n",
      "ep 315: w = tensor([1.93747, 1.00419], requires_grad=True), loss = 0.01665138 grad = tensor([-0.52427,  0.03789])\n",
      "ep 320: w = tensor([1.94077, 1.00397], requires_grad=True), loss = 0.01494033 grad = tensor([-0.49691,  0.03132])\n",
      "ep 325: w = tensor([1.94389, 1.00376], requires_grad=True), loss = 0.01340491 grad = tensor([-0.47046,  0.03308])\n",
      "ep 330: w = tensor([1.94685, 1.00356], requires_grad=True), loss = 0.01202754 grad = tensor([-0.44581,  0.02867])\n",
      "ep 335: w = tensor([1.94966, 1.00337], requires_grad=True), loss = 0.01079139 grad = tensor([-0.42217,  0.02872])\n",
      "ep 340: w = tensor([1.95231, 1.00320], requires_grad=True), loss = 0.00968240 grad = tensor([-0.39996,  0.02622])\n",
      "ep 345: w = tensor([1.95483, 1.00303], requires_grad=True), loss = 0.00868748 grad = tensor([-0.37878,  0.02594])\n",
      "ep 350: w = tensor([1.95722, 1.00287], requires_grad=True), loss = 0.00779471 grad = tensor([-0.35886,  0.02347])\n",
      "ep 355: w = tensor([1.95947, 1.00272], requires_grad=True), loss = 0.00699359 grad = tensor([-0.33986,  0.02316])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 360: w = tensor([1.96161, 1.00257], requires_grad=True), loss = 0.00627505 grad = tensor([-0.32198,  0.02117])\n",
      "ep 365: w = tensor([1.96364, 1.00244], requires_grad=True), loss = 0.00563018 grad = tensor([-0.30494,  0.02073])\n",
      "ep 370: w = tensor([1.96556, 1.00231], requires_grad=True), loss = 0.00505164 grad = tensor([-0.28889,  0.01907])\n",
      "ep 375: w = tensor([1.96737, 1.00219], requires_grad=True), loss = 0.00453248 grad = tensor([-0.27360,  0.01865])\n",
      "ep 380: w = tensor([1.96910, 1.00207], requires_grad=True), loss = 0.00406663 grad = tensor([-0.25919,  0.01716])\n",
      "ep 385: w = tensor([1.97073, 1.00196], requires_grad=True), loss = 0.00364873 grad = tensor([-0.24547,  0.01686])\n",
      "ep 390: w = tensor([1.97227, 1.00186], requires_grad=True), loss = 0.00327375 grad = tensor([-0.23257,  0.01525])\n",
      "ep 395: w = tensor([1.97374, 1.00176], requires_grad=True), loss = 0.00293732 grad = tensor([-0.22025,  0.01501])\n",
      "ep 400: w = tensor([1.97512, 1.00167], requires_grad=True), loss = 0.00263548 grad = tensor([-0.20865,  0.01399])\n",
      "ep 405: w = tensor([1.97643, 1.00158], requires_grad=True), loss = 0.00236459 grad = tensor([-0.19764,  0.01316])\n",
      "ep 410: w = tensor([1.97768, 1.00150], requires_grad=True), loss = 0.00212172 grad = tensor([-0.18721,  0.01259])\n",
      "ep 415: w = tensor([1.97886, 1.00142], requires_grad=True), loss = 0.00190360 grad = tensor([-0.17732,  0.01196])\n",
      "ep 420: w = tensor([1.97997, 1.00134], requires_grad=True), loss = 0.00170798 grad = tensor([-0.16796,  0.01138])\n",
      "ep 425: w = tensor([1.98103, 1.00127], requires_grad=True), loss = 0.00153252 grad = tensor([-0.15909,  0.01087])\n",
      "ep 430: w = tensor([1.98203, 1.00120], requires_grad=True), loss = 0.00137501 grad = tensor([-0.15071,  0.01001])\n",
      "ep 435: w = tensor([1.98298, 1.00114], requires_grad=True), loss = 0.00123367 grad = tensor([-0.14276,  0.00950])\n",
      "ep 440: w = tensor([1.98388, 1.00108], requires_grad=True), loss = 0.00110684 grad = tensor([-0.13520,  0.00934])\n",
      "ep 445: w = tensor([1.98473, 1.00102], requires_grad=True), loss = 0.00099315 grad = tensor([-0.12813,  0.00793])\n",
      "ep 450: w = tensor([1.98553, 1.00097], requires_grad=True), loss = 0.00089105 grad = tensor([-0.12130,  0.00848])\n",
      "ep 455: w = tensor([1.98630, 1.00092], requires_grad=True), loss = 0.00079952 grad = tensor([-0.11493,  0.00759])\n",
      "ep 460: w = tensor([1.98702, 1.00087], requires_grad=True), loss = 0.00071731 grad = tensor([-0.10887,  0.00698])\n",
      "ep 465: w = tensor([1.98771, 1.00082], requires_grad=True), loss = 0.00064363 grad = tensor([-0.10306,  0.00759])\n",
      "ep 470: w = tensor([1.98835, 1.00078], requires_grad=True), loss = 0.00057748 grad = tensor([-0.09772,  0.00578])\n",
      "ep 475: w = tensor([1.98897, 1.00074], requires_grad=True), loss = 0.00051811 grad = tensor([-0.09247,  0.00677])\n",
      "ep 480: w = tensor([1.98955, 1.00070], requires_grad=True), loss = 0.00046487 grad = tensor([-0.08766,  0.00547])\n",
      "ep 485: w = tensor([1.99010, 1.00066], requires_grad=True), loss = 0.00041709 grad = tensor([-0.08298,  0.00595])\n",
      "ep 490: w = tensor([1.99063, 1.00063], requires_grad=True), loss = 0.00037425 grad = tensor([-0.07866,  0.00471])\n",
      "ep 495: w = tensor([1.99112, 1.00060], requires_grad=True), loss = 0.00033581 grad = tensor([-0.07447,  0.00512])\n",
      "ep 500: w = tensor([1.99159, 1.00056], requires_grad=True), loss = 0.00030129 grad = tensor([-0.07053,  0.00497])\n",
      "ep 505: w = tensor([1.99203, 1.00053], requires_grad=True), loss = 0.00027033 grad = tensor([-0.06684,  0.00425])\n",
      "ep 510: w = tensor([1.99245, 1.00051], requires_grad=True), loss = 0.00024254 grad = tensor([-0.06329,  0.00439])\n",
      "ep 515: w = tensor([1.99285, 1.00048], requires_grad=True), loss = 0.00021764 grad = tensor([-0.05998,  0.00373])\n",
      "ep 520: w = tensor([1.99323, 1.00045], requires_grad=True), loss = 0.00019526 grad = tensor([-0.05680,  0.00376])\n",
      "ep 525: w = tensor([1.99359, 1.00043], requires_grad=True), loss = 0.00017518 grad = tensor([-0.05380,  0.00354])\n",
      "ep 530: w = tensor([1.99392, 1.00041], requires_grad=True), loss = 0.00015719 grad = tensor([-0.05095,  0.00349])\n",
      "ep 535: w = tensor([1.99425, 1.00039], requires_grad=True), loss = 0.00014103 grad = tensor([-0.04827,  0.00313])\n",
      "ep 540: w = tensor([1.99455, 1.00037], requires_grad=True), loss = 0.00012652 grad = tensor([-0.04570,  0.00323])\n",
      "ep 545: w = tensor([1.99484, 1.00035], requires_grad=True), loss = 0.00011353 grad = tensor([-0.04331,  0.00290])\n",
      "ep 550: w = tensor([1.99511, 1.00033], requires_grad=True), loss = 0.00010187 grad = tensor([-0.04103,  0.00267])\n",
      "ep 555: w = tensor([1.99537, 1.00031], requires_grad=True), loss = 0.00009140 grad = tensor([-0.03886,  0.00257])\n",
      "ep 560: w = tensor([1.99561, 1.00029], requires_grad=True), loss = 0.00008202 grad = tensor([-0.03678,  0.00282])\n",
      "ep 565: w = tensor([1.99584, 1.00028], requires_grad=True), loss = 0.00007359 grad = tensor([-0.03490,  0.00187])\n",
      "ep 570: w = tensor([1.99606, 1.00026], requires_grad=True), loss = 0.00006602 grad = tensor([-0.03301,  0.00237])\n",
      "ep 575: w = tensor([1.99627, 1.00025], requires_grad=True), loss = 0.00005925 grad = tensor([-0.03131,  0.00177])\n",
      "ep 580: w = tensor([1.99647, 1.00024], requires_grad=True), loss = 0.00005316 grad = tensor([-0.02962,  0.00223])\n",
      "ep 585: w = tensor([1.99665, 1.00022], requires_grad=True), loss = 0.00004769 grad = tensor([-0.02807,  0.00181])\n",
      "ep 590: w = tensor([1.99683, 1.00021], requires_grad=True), loss = 0.00004279 grad = tensor([-0.02660,  0.00154])\n",
      "ep 595: w = tensor([1.99700, 1.00020], requires_grad=True), loss = 0.00003839 grad = tensor([-0.02516,  0.00196])\n",
      "ep 600: w = tensor([1.99716, 1.00019], requires_grad=True), loss = 0.00003444 grad = tensor([-0.02386,  0.00145])\n",
      "ep 605: w = tensor([1.99731, 1.00018], requires_grad=True), loss = 0.00003091 grad = tensor([-0.02259,  0.00156])\n",
      "ep 610: w = tensor([1.99745, 1.00017], requires_grad=True), loss = 0.00002773 grad = tensor([-0.02138,  0.00179])\n",
      "ep 615: w = tensor([1.99758, 1.00016], requires_grad=True), loss = 0.00002489 grad = tensor([-0.02030,  0.00092])\n",
      "ep 620: w = tensor([1.99771, 1.00015], requires_grad=True), loss = 0.00002233 grad = tensor([-0.01918,  0.00168])\n",
      "ep 625: w = tensor([1.99783, 1.00015], requires_grad=True), loss = 0.00002004 grad = tensor([-0.01821,  0.00098])\n",
      "ep 630: w = tensor([1.99795, 1.00014], requires_grad=True), loss = 0.00001798 grad = tensor([-0.01723,  0.00118])\n",
      "ep 635: w = tensor([1.99805, 1.00013], requires_grad=True), loss = 0.00001613 grad = tensor([-0.01633,  0.00100])\n",
      "ep 640: w = tensor([1.99816, 1.00012], requires_grad=True), loss = 0.00001448 grad = tensor([-0.01547,  0.00095])\n",
      "ep 645: w = tensor([1.99825, 1.00012], requires_grad=True), loss = 0.00001298 grad = tensor([-0.01461,  0.00145])\n",
      "ep 650: w = tensor([1.99835, 1.00011], requires_grad=True), loss = 0.00001165 grad = tensor([-0.01390,  0.00051])\n",
      "ep 655: w = tensor([1.99843, 1.00010], requires_grad=True), loss = 0.00001045 grad = tensor([-0.01312,  0.00114])\n",
      "ep 660: w = tensor([1.99852, 1.00010], requires_grad=True), loss = 0.00000938 grad = tensor([-0.01247,  0.00051])\n",
      "ep 665: w = tensor([1.99859, 1.00009], requires_grad=True), loss = 0.00000841 grad = tensor([-0.01178,  0.00090])\n",
      "ep 670: w = tensor([1.99867, 1.00009], requires_grad=True), loss = 0.00000755 grad = tensor([-0.01119,  0.00039])\n",
      "ep 675: w = tensor([1.99874, 1.00008], requires_grad=True), loss = 0.00000678 grad = tensor([-0.01054,  0.00137])\n",
      "ep 680: w = tensor([1.99881, 1.00008], requires_grad=True), loss = 0.00000608 grad = tensor([-1.00603e-02,  8.58307e-05])\n",
      "ep 685: w = tensor([1.99887, 1.00008], requires_grad=True), loss = 0.00000546 grad = tensor([-0.00946,  0.00114])\n",
      "ep 690: w = tensor([1.99893, 1.00007], requires_grad=True), loss = 0.00000489 grad = tensor([-9.02748e-03,  5.72205e-05])\n",
      "ep 695: w = tensor([1.99898, 1.00007], requires_grad=True), loss = 0.00000439 grad = tensor([-0.00851,  0.00069])\n",
      "ep 700: w = tensor([1.99904, 1.00006], requires_grad=True), loss = 0.00000394 grad = tensor([-0.00805,  0.00076])\n",
      "ep 705: w = tensor([1.99909, 1.00006], requires_grad=True), loss = 0.00000353 grad = tensor([-0.00765,  0.00038])\n",
      "ep 710: w = tensor([1.99914, 1.00006], requires_grad=True), loss = 0.00000317 grad = tensor([-0.00726,  0.00015])\n",
      "ep 715: w = tensor([1.99918, 1.00005], requires_grad=True), loss = 0.00000284 grad = tensor([-0.00684,  0.00063])\n",
      "ep 720: w = tensor([1.99923, 1.00005], requires_grad=True), loss = 0.00000255 grad = tensor([-6.51836e-03,  9.53674e-05])\n",
      "ep 725: w = tensor([1.99927, 1.00005], requires_grad=True), loss = 0.00000229 grad = tensor([-0.00612,  0.00083])\n",
      "ep 730: w = tensor([1.99931, 1.00005], requires_grad=True), loss = 0.00000206 grad = tensor([-0.00586,  0.00000])\n",
      "ep 735: w = tensor([1.99934, 1.00004], requires_grad=True), loss = 0.00000184 grad = tensor([-0.00549,  0.00076])\n",
      "ep 740: w = tensor([1.99938, 1.00004], requires_grad=True), loss = 0.00000166 grad = tensor([-5.25570e-03, -3.81470e-05])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 745: w = tensor([1.99941, 1.00004], requires_grad=True), loss = 0.00000149 grad = tensor([-0.00494,  0.00058])\n",
      "ep 750: w = tensor([1.99944, 1.00004], requires_grad=True), loss = 0.00000133 grad = tensor([-0.00467,  0.00057])\n",
      "ep 755: w = tensor([1.99947, 1.00004], requires_grad=True), loss = 0.00000119 grad = tensor([-0.00445,  0.00010])\n",
      "ep 760: w = tensor([1.99950, 1.00003], requires_grad=True), loss = 0.00000107 grad = tensor([-0.00419,  0.00046])\n",
      "ep 765: w = tensor([1.99952, 1.00003], requires_grad=True), loss = 0.00000096 grad = tensor([-0.00398,  0.00027])\n",
      "ep 770: w = tensor([1.99955, 1.00003], requires_grad=True), loss = 0.00000086 grad = tensor([-0.00379,  0.00010])\n",
      "ep 775: w = tensor([1.99957, 1.00003], requires_grad=True), loss = 0.00000077 grad = tensor([-0.00357,  0.00038])\n",
      "ep 780: w = tensor([1.99960, 1.00003], requires_grad=True), loss = 0.00000069 grad = tensor([-3.39699e-03,  6.67572e-05])\n",
      "ep 785: w = tensor([1.99962, 1.00003], requires_grad=True), loss = 0.00000062 grad = tensor([-0.00320,  0.00031])\n",
      "ep 790: w = tensor([1.99964, 1.00002], requires_grad=True), loss = 0.00000056 grad = tensor([-0.00303,  0.00041])\n",
      "ep 795: w = tensor([1.99966, 1.00002], requires_grad=True), loss = 0.00000050 grad = tensor([-0.00287,  0.00035])\n",
      "ep 800: w = tensor([1.99967, 1.00002], requires_grad=True), loss = 0.00000045 grad = tensor([-2.73609e-03,  3.81470e-05])\n",
      "ep 805: w = tensor([1.99969, 1.00002], requires_grad=True), loss = 0.00000040 grad = tensor([-0.00257,  0.00031])\n",
      "ep 810: w = tensor([1.99971, 1.00002], requires_grad=True), loss = 0.00000036 grad = tensor([-0.00246,  0.00000])\n",
      "ep 815: w = tensor([1.99972, 1.00002], requires_grad=True), loss = 0.00000033 grad = tensor([-0.00232,  0.00015])\n",
      "ep 820: w = tensor([1.99974, 1.00002], requires_grad=True), loss = 0.00000029 grad = tensor([-0.00219,  0.00019])\n",
      "ep 825: w = tensor([1.99975, 1.00002], requires_grad=True), loss = 0.00000026 grad = tensor([-0.00208,  0.00011])\n",
      "ep 830: w = tensor([1.99977, 1.00002], requires_grad=True), loss = 0.00000024 grad = tensor([-1.97983e-03,  9.53674e-06])\n",
      "ep 835: w = tensor([1.99978, 1.00001], requires_grad=True), loss = 0.00000021 grad = tensor([-0.00186,  0.00025])\n",
      "ep 840: w = tensor([1.99979, 1.00001], requires_grad=True), loss = 0.00000019 grad = tensor([-0.00177,  0.00000])\n",
      "ep 845: w = tensor([1.99980, 1.00001], requires_grad=True), loss = 0.00000017 grad = tensor([-0.00167,  0.00015])\n",
      "ep 850: w = tensor([1.99981, 1.00001], requires_grad=True), loss = 0.00000015 grad = tensor([-0.00158,  0.00020])\n",
      "ep 855: w = tensor([1.99982, 1.00001], requires_grad=True), loss = 0.00000014 grad = tensor([-0.00151,  0.00000])\n",
      "ep 860: w = tensor([1.99983, 1.00001], requires_grad=True), loss = 0.00000012 grad = tensor([-0.00145, -0.00031])\n",
      "ep 865: w = tensor([1.99984, 1.00001], requires_grad=True), loss = 0.00000011 grad = tensor([-0.00134,  0.00018])\n",
      "ep 870: w = tensor([1.99985, 1.00001], requires_grad=True), loss = 0.00000010 grad = tensor([-0.00127,  0.00023])\n",
      "ep 875: w = tensor([1.99986, 1.00001], requires_grad=True), loss = 0.00000009 grad = tensor([-0.00124, -0.00038])\n",
      "ep 880: w = tensor([1.99986, 1.00001], requires_grad=True), loss = 0.00000008 grad = tensor([-0.00111,  0.00054])\n",
      "ep 885: w = tensor([1.99987, 1.00001], requires_grad=True), loss = 0.00000007 grad = tensor([-0.00111, -0.00032])\n",
      "ep 890: w = tensor([1.99988, 1.00001], requires_grad=True), loss = 0.00000006 grad = tensor([-0.00095,  0.00114])\n",
      "ep 895: w = tensor([1.99988, 1.00001], requires_grad=True), loss = 0.00000006 grad = tensor([-0.00102, -0.00066])\n",
      "ep 900: w = tensor([1.99989, 1.00001], requires_grad=True), loss = 0.00000005 grad = tensor([-0.00089,  0.00051])\n",
      "ep 905: w = tensor([1.99990, 1.00001], requires_grad=True), loss = 0.00000005 grad = tensor([-0.00092, -0.00057])\n",
      "ep 910: w = tensor([1.99990, 1.00001], requires_grad=True), loss = 0.00000004 grad = tensor([-0.00081,  0.00035])\n",
      "ep 915: w = tensor([1.99991, 1.00001], requires_grad=True), loss = 0.00000004 grad = tensor([-0.00077,  0.00025])\n",
      "ep 920: w = tensor([1.99991, 1.00001], requires_grad=True), loss = 0.00000003 grad = tensor([-7.50542e-04, -2.86102e-05])\n",
      "ep 925: w = tensor([1.99992, 1.00001], requires_grad=True), loss = 0.00000003 grad = tensor([-0.00070,  0.00015])\n",
      "ep 930: w = tensor([1.99992, 1.00001], requires_grad=True), loss = 0.00000003 grad = tensor([-0.00069, -0.00031])\n",
      "ep 935: w = tensor([1.99992, 1.00000], requires_grad=True), loss = 0.00000002 grad = tensor([-0.00062,  0.00025])\n",
      "ep 940: w = tensor([1.99993, 1.00001], requires_grad=True), loss = 0.00000002 grad = tensor([-0.00062, -0.00035])\n",
      "ep 945: w = tensor([1.99993, 1.00000], requires_grad=True), loss = 0.00000002 grad = tensor([-0.00055,  0.00019])\n",
      "ep 950: w = tensor([1.99994, 1.00000], requires_grad=True), loss = 0.00000002 grad = tensor([-0.00055, -0.00019])\n",
      "ep 955: w = tensor([1.99994, 1.00000], requires_grad=True), loss = 0.00000002 grad = tensor([-0.00050,  0.00019])\n",
      "ep 960: w = tensor([1.99994, 1.00000], requires_grad=True), loss = 0.00000001 grad = tensor([-4.83513e-04, -2.86102e-05])\n",
      "ep 965: w = tensor([1.99995, 1.00000], requires_grad=True), loss = 0.00000001 grad = tensor([-4.52995e-04,  9.53674e-05])\n",
      "ep 970: w = tensor([1.99995, 1.00000], requires_grad=True), loss = 0.00000001 grad = tensor([-0.00046, -0.00038])\n",
      "ep 975: w = tensor([1.99995, 1.00000], requires_grad=True), loss = 0.00000001 grad = tensor([-0.00037,  0.00063])\n",
      "ep 980: w = tensor([1.99995, 1.00000], requires_grad=True), loss = 0.00000001 grad = tensor([-0.00040, -0.00019])\n",
      "ep 985: w = tensor([1.99996, 1.00000], requires_grad=True), loss = 0.00000001 grad = tensor([-0.00034,  0.00038])\n",
      "ep 990: w = tensor([1.99996, 1.00000], requires_grad=True), loss = 0.00000001 grad = tensor([-3.47137e-04, -9.53674e-06])\n",
      "ep 995: w = tensor([1.99996, 1.00000], requires_grad=True), loss = 0.00000001 grad = tensor([-0.00034, -0.00019])\n",
      "ep 1000: w = tensor([1.99996, 1.00000], requires_grad=True), loss = 0.00000001 grad = tensor([-0.00029,  0.00038])\n"
     ]
    }
   ],
   "source": [
    "# Модель линейной регрессии (с несколькими параметрами)\n",
    "# f = X * w \n",
    "\n",
    "# Данные для обучения: \n",
    "# принзаки X: рассматривается 4 наблюдения (ось 0) и 2 признака (ось 1):\n",
    "\n",
    "X = torch.tensor([[1., 40.],\n",
    "                  [2., 30.],\n",
    "                  [3., 20.],\n",
    "                  [4., 10.]], dtype=torch.float32) # Size([4, 2])\n",
    "\n",
    "\n",
    "# истинное значение весов (используется только для получения обучающих правильных ответов):\n",
    "w_ans = torch.tensor([2., 1.], dtype=torch.float32)\n",
    "# Y - приавильные ответы: \n",
    "Y = X @ w_ans\n",
    "\n",
    "torch.set_printoptions(precision=5) # точность вывода на печать значений тензоров\n",
    "print(f'w true value = {w_ans}, Y = {Y}')\n",
    "\n",
    "# model (модель, в нашем случае: линейная регрессия)\n",
    "\n",
    "# изначальное значение весов w\n",
    "#!!! requires_grad=True\n",
    "w = torch.tensor([0.0, 0.0], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# прямое распространение:\n",
    "def forward(X):\n",
    "    return X @ w # Size([4])\n",
    "\n",
    "# loss = MSE (функция потерь, в нашем слаучае: средняя квадратичная ошибка)\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred - y)**2).mean() # Size([])\n",
    "\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.0013\n",
    "n_iters = 1000 + 1\n",
    "\n",
    "# основной цикл:\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    #!!! backward pass        \n",
    "    # calculate gradients = backward pass\n",
    "    l.backward()\n",
    "    \n",
    "    # update weights\n",
    "    #w.data = w.data - learning_rate * w.grad\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        dw = w.grad\n",
    "        print(f'ep {epoch}: w = {w}, loss = {l:.8f} grad = {dw}')                        \n",
    "#         print(f'ep {epoch}: w = {w}, y_p = {y_pred}, loss = {l:.8f}\\ngrad = {dw}')                \n",
    "#         print(f'ep {epoch:3}: y_p = {y_pred}, loss = {l:.8f} grd = {dw}')\n",
    "                \n",
    "        \n",
    "    # zero the gradients after updating\n",
    "    w.grad.zero_()    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "Использование встроенного оптимизатора `optimizer = torch.optim.SGD([w], lr=learning_rate)` и функции потерь `loss = nn.MSELoss()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w true value = tensor([2., 1.]), Y = tensor([42., 34., 26., 18.])\n",
      "ep 0: w = tensor([0.16900, 2.21000], requires_grad=True), loss = 980.00000000 grad = tensor([ -130., -1700.])\n",
      "ep 5: w = tensor([0.13813, 0.24422], requires_grad=True), loss = 646.58898926 grad = tensor([  77.23860, 1378.76135])\n",
      "ep 10: w = tensor([0.33966, 1.82453], requires_grad=True), loss = 427.49307251 grad = tensor([  -89.12965, -1114.91821])\n",
      "ep 15: w = tensor([0.34364, 0.53338], requires_grad=True), loss = 283.42614746 grad = tensor([ 47.01924, 904.69263])\n",
      "ep 20: w = tensor([0.49880, 1.56850], requires_grad=True), loss = 188.61228943 grad = tensor([ -61.92347, -731.13928])\n",
      "ep 25: w = tensor([0.52316, 0.72007], requires_grad=True), loss = 126.13926697 grad = tensor([ 27.57067, 593.68494])\n",
      "ep 30: w = tensor([0.64554, 1.39771], requires_grad=True), loss = 84.91012573 grad = tensor([ -43.72146, -479.40924])\n",
      "ep 35: w = tensor([0.68103, 0.83984], requires_grad=True), loss = 57.64197159 grad = tensor([ 15.14910, 389.64639])\n",
      "ep 40: w = tensor([0.77979, 1.28313], requires_grad=True), loss = 39.55477905 grad = tensor([ -31.46260, -314.29950])\n",
      "ep 45: w = tensor([0.82057, 0.91600], requires_grad=True), loss = 27.51074600 grad = tensor([  7.30249, 255.77905])\n",
      "ep 50: w = tensor([0.90194, 1.20568], requires_grad=True), loss = 19.44943237 grad = tensor([ -23.13506, -206.00888])\n",
      "ep 55: w = tensor([0.94440, 0.96380], requires_grad=True), loss = 14.01713943 grad = tensor([  2.42605, 167.94620])\n",
      "ep 60: w = tensor([1.01265, 1.15282], requires_grad=True), loss = 10.32425213 grad = tensor([ -17.41577, -134.98840])\n",
      "ep 65: w = tensor([1.05460, 0.99321], requires_grad=True), loss = 7.78575325 grad = tensor([ -0.52992, 110.31209])\n",
      "ep 70: w = tensor([1.11272, 1.11631], requires_grad=True), loss = 6.01642036 grad = tensor([-13.43388, -88.41566])\n",
      "ep 75: w = tensor([1.15289, 1.01076], requires_grad=True), loss = 4.76234579 grad = tensor([-2.25095, 72.49083])\n",
      "ep 80: w = tensor([1.20299, 1.09071], requires_grad=True), loss = 3.85587597 grad = tensor([-10.61534, -57.87885])\n",
      "ep 85: w = tensor([1.24068, 1.02071], requires_grad=True), loss = 3.18603969 grad = tensor([-3.18430, 47.66795])\n",
      "ep 90: w = tensor([1.28429, 1.07244], requires_grad=True), loss = 2.67915201 grad = tensor([ -8.58116, -37.85973])\n",
      "ep 95: w = tensor([1.31920, 1.02583], requires_grad=True), loss = 2.28610182 grad = tensor([-3.62107, 31.37278])\n",
      "ep 100: w = tensor([1.35745, 1.05912], requires_grad=True), loss = 1.97393394 grad = tensor([ -7.08053, -24.73854])\n",
      "ep 105: w = tensor([1.38948, 1.02793], requires_grad=True), loss = 1.72041774 grad = tensor([-3.75016, 20.67220])\n",
      "ep 110: w = tensor([1.42322, 1.04919], requires_grad=True), loss = 1.51038170 grad = tensor([ -5.94694, -16.14056])\n",
      "ep 115: w = tensor([1.45243, 1.02818], requires_grad=True), loss = 1.33336782 grad = tensor([-3.69356, 13.64340])\n",
      "ep 120: w = tensor([1.48233, 1.04161], requires_grad=True), loss = 1.18204451 grad = tensor([ -5.06955, -10.51023])\n",
      "ep 125: w = tensor([1.50884, 1.02733], requires_grad=True), loss = 1.05119181 grad = tensor([-3.52963,  9.02459])\n",
      "ep 130: w = tensor([1.53541, 1.03567], requires_grad=True), loss = 0.93700927 grad = tensor([-4.37389, -6.82430])\n",
      "ep 135: w = tensor([1.55940, 1.02587], requires_grad=True), loss = 0.83668095 grad = tensor([-3.30841,  5.98640])\n",
      "ep 140: w = tensor([1.58308, 1.03092], requires_grad=True), loss = 0.74805164 grad = tensor([-3.80985, -4.41404])\n",
      "ep 145: w = tensor([1.60474, 1.02409], requires_grad=True), loss = 0.66944206 grad = tensor([-3.06120,  3.98732])\n",
      "ep 150: w = tensor([1.62588, 1.02703], requires_grad=True), loss = 0.59950674 grad = tensor([-3.34317, -2.83932])\n",
      "ep 155: w = tensor([1.64540, 1.02219], requires_grad=True), loss = 0.53715217 grad = tensor([-2.80748,  2.66960])\n",
      "ep 160: w = tensor([1.66429, 1.02378], requires_grad=True), loss = 0.48146015 grad = tensor([-2.95031, -1.81211])\n",
      "ep 165: w = tensor([1.68186, 1.02029], requires_grad=True), loss = 0.43166173 grad = tensor([-2.55889,  1.79972])\n",
      "ep 170: w = tensor([1.69877, 1.02103], requires_grad=True), loss = 0.38709140 grad = tensor([-2.61480, -1.14371])\n",
      "ep 175: w = tensor([1.71457, 1.01845], requires_grad=True), loss = 0.34717295 grad = tensor([-2.32213,  1.22391])\n",
      "ep 180: w = tensor([1.72971, 1.01867], requires_grad=True), loss = 0.31140596 grad = tensor([-2.32487, -0.70953])\n",
      "ep 185: w = tensor([1.74391, 1.01672], requires_grad=True), loss = 0.27934441 grad = tensor([-2.10070,  0.84159])\n",
      "ep 190: w = tensor([1.75748, 1.01662], requires_grad=True), loss = 0.25059801 grad = tensor([-2.07201, -0.42871])\n",
      "ep 195: w = tensor([1.77024, 1.01511], requires_grad=True), loss = 0.22481927 grad = tensor([-1.89610,  0.58674])\n",
      "ep 200: w = tensor([1.78240, 1.01482], requires_grad=True), loss = 0.20169993 grad = tensor([-1.84993, -0.24814])\n",
      "ep 205: w = tensor([1.79385, 1.01363], requires_grad=True), loss = 0.18096074 grad = tensor([-1.70864,  0.41626])\n",
      "ep 210: w = tensor([1.80476, 1.01324], requires_grad=True), loss = 0.16235729 grad = tensor([-1.65388, -0.13396])\n",
      "ep 215: w = tensor([1.81504, 1.01227], requires_grad=True), loss = 0.14566770 grad = tensor([-1.53788,  0.30142])\n",
      "ep 220: w = tensor([1.82482, 1.01185], requires_grad=True), loss = 0.13069558 grad = tensor([-1.48000, -0.06169])\n",
      "ep 225: w = tensor([1.83405, 1.01104], requires_grad=True), loss = 0.11726224 grad = tensor([-1.38304,  0.22278])\n",
      "ep 230: w = tensor([1.84282, 1.01060], requires_grad=True), loss = 0.10521053 grad = tensor([-1.32529, -0.01637])\n",
      "ep 235: w = tensor([1.85111, 1.00993], requires_grad=True), loss = 0.09439759 grad = tensor([-1.24302,  0.16842])\n",
      "ep 240: w = tensor([1.85897, 1.00950], requires_grad=True), loss = 0.08469677 grad = tensor([-1.18740,  0.01068])\n",
      "ep 245: w = tensor([1.86641, 1.00892], requires_grad=True), loss = 0.07599217 grad = tensor([-1.11665,  0.13068])\n",
      "ep 250: w = tensor([1.87346, 1.00851], requires_grad=True), loss = 0.06818256 grad = tensor([-1.06427,  0.02612])\n",
      "ep 255: w = tensor([1.88014, 1.00801], requires_grad=True), loss = 0.06117591 grad = tensor([-1.00278,  0.10420])\n",
      "ep 260: w = tensor([1.88647, 1.00763], requires_grad=True), loss = 0.05488885 grad = tensor([-0.95419,  0.03394])\n",
      "ep 265: w = tensor([1.89246, 1.00719], requires_grad=True), loss = 0.04924808 grad = tensor([-0.90031,  0.08475])\n",
      "ep 270: w = tensor([1.89813, 1.00684], requires_grad=True), loss = 0.04418735 grad = tensor([-0.85564,  0.03786])\n",
      "ep 275: w = tensor([1.90351, 1.00646], requires_grad=True), loss = 0.03964623 grad = tensor([-0.80819,  0.07013])\n",
      "ep 280: w = tensor([1.90860, 1.00614], requires_grad=True), loss = 0.03557184 grad = tensor([-0.76740,  0.03853])\n",
      "ep 285: w = tensor([1.91342, 1.00580], requires_grad=True), loss = 0.03191639 grad = tensor([-0.72539,  0.05912])\n",
      "ep 290: w = tensor([1.91799, 1.00550], requires_grad=True), loss = 0.02863660 grad = tensor([-0.68833,  0.03774])\n",
      "ep 295: w = tensor([1.92232, 1.00520], requires_grad=True), loss = 0.02569385 grad = tensor([-0.65102,  0.05041])\n",
      "ep 300: w = tensor([1.92642, 1.00494], requires_grad=True), loss = 0.02305340 grad = tensor([-0.61746,  0.03589])\n",
      "ep 305: w = tensor([1.93030, 1.00467], requires_grad=True), loss = 0.02068412 grad = tensor([-0.58424,  0.04342])\n",
      "ep 310: w = tensor([1.93398, 1.00443], requires_grad=True), loss = 0.01855873 grad = tensor([-0.55391,  0.03366])\n",
      "ep 315: w = tensor([1.93747, 1.00419], requires_grad=True), loss = 0.01665138 grad = tensor([-0.52427,  0.03789])\n",
      "ep 320: w = tensor([1.94077, 1.00397], requires_grad=True), loss = 0.01494033 grad = tensor([-0.49691,  0.03132])\n",
      "ep 325: w = tensor([1.94389, 1.00376], requires_grad=True), loss = 0.01340491 grad = tensor([-0.47046,  0.03308])\n",
      "ep 330: w = tensor([1.94685, 1.00356], requires_grad=True), loss = 0.01202754 grad = tensor([-0.44581,  0.02867])\n",
      "ep 335: w = tensor([1.94966, 1.00337], requires_grad=True), loss = 0.01079139 grad = tensor([-0.42217,  0.02872])\n",
      "ep 340: w = tensor([1.95231, 1.00320], requires_grad=True), loss = 0.00968240 grad = tensor([-0.39996,  0.02622])\n",
      "ep 345: w = tensor([1.95483, 1.00303], requires_grad=True), loss = 0.00868748 grad = tensor([-0.37878,  0.02594])\n",
      "ep 350: w = tensor([1.95722, 1.00287], requires_grad=True), loss = 0.00779471 grad = tensor([-0.35886,  0.02347])\n",
      "ep 355: w = tensor([1.95947, 1.00272], requires_grad=True), loss = 0.00699359 grad = tensor([-0.33986,  0.02316])\n",
      "ep 360: w = tensor([1.96161, 1.00257], requires_grad=True), loss = 0.00627505 grad = tensor([-0.32198,  0.02117])\n",
      "ep 365: w = tensor([1.96364, 1.00244], requires_grad=True), loss = 0.00563018 grad = tensor([-0.30494,  0.02073])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 370: w = tensor([1.96556, 1.00231], requires_grad=True), loss = 0.00505164 grad = tensor([-0.28889,  0.01907])\n",
      "ep 375: w = tensor([1.96737, 1.00219], requires_grad=True), loss = 0.00453248 grad = tensor([-0.27360,  0.01865])\n",
      "ep 380: w = tensor([1.96910, 1.00207], requires_grad=True), loss = 0.00406663 grad = tensor([-0.25919,  0.01716])\n",
      "ep 385: w = tensor([1.97073, 1.00196], requires_grad=True), loss = 0.00364873 grad = tensor([-0.24547,  0.01686])\n",
      "ep 390: w = tensor([1.97227, 1.00186], requires_grad=True), loss = 0.00327375 grad = tensor([-0.23257,  0.01525])\n",
      "ep 395: w = tensor([1.97374, 1.00176], requires_grad=True), loss = 0.00293732 grad = tensor([-0.22025,  0.01501])\n",
      "ep 400: w = tensor([1.97512, 1.00167], requires_grad=True), loss = 0.00263548 grad = tensor([-0.20865,  0.01399])\n",
      "ep 405: w = tensor([1.97643, 1.00158], requires_grad=True), loss = 0.00236459 grad = tensor([-0.19764,  0.01316])\n",
      "ep 410: w = tensor([1.97768, 1.00150], requires_grad=True), loss = 0.00212172 grad = tensor([-0.18721,  0.01259])\n",
      "ep 415: w = tensor([1.97886, 1.00142], requires_grad=True), loss = 0.00190360 grad = tensor([-0.17732,  0.01196])\n",
      "ep 420: w = tensor([1.97997, 1.00134], requires_grad=True), loss = 0.00170798 grad = tensor([-0.16796,  0.01138])\n",
      "ep 425: w = tensor([1.98103, 1.00127], requires_grad=True), loss = 0.00153252 grad = tensor([-0.15909,  0.01087])\n",
      "ep 430: w = tensor([1.98203, 1.00120], requires_grad=True), loss = 0.00137501 grad = tensor([-0.15071,  0.01001])\n",
      "ep 435: w = tensor([1.98298, 1.00114], requires_grad=True), loss = 0.00123367 grad = tensor([-0.14276,  0.00950])\n",
      "ep 440: w = tensor([1.98388, 1.00108], requires_grad=True), loss = 0.00110684 grad = tensor([-0.13520,  0.00934])\n",
      "ep 445: w = tensor([1.98473, 1.00102], requires_grad=True), loss = 0.00099315 grad = tensor([-0.12813,  0.00793])\n",
      "ep 450: w = tensor([1.98553, 1.00097], requires_grad=True), loss = 0.00089105 grad = tensor([-0.12130,  0.00848])\n",
      "ep 455: w = tensor([1.98630, 1.00092], requires_grad=True), loss = 0.00079952 grad = tensor([-0.11493,  0.00759])\n",
      "ep 460: w = tensor([1.98702, 1.00087], requires_grad=True), loss = 0.00071731 grad = tensor([-0.10887,  0.00698])\n",
      "ep 465: w = tensor([1.98771, 1.00082], requires_grad=True), loss = 0.00064363 grad = tensor([-0.10306,  0.00759])\n",
      "ep 470: w = tensor([1.98835, 1.00078], requires_grad=True), loss = 0.00057748 grad = tensor([-0.09772,  0.00578])\n",
      "ep 475: w = tensor([1.98897, 1.00074], requires_grad=True), loss = 0.00051811 grad = tensor([-0.09247,  0.00677])\n",
      "ep 480: w = tensor([1.98955, 1.00070], requires_grad=True), loss = 0.00046487 grad = tensor([-0.08766,  0.00547])\n",
      "ep 485: w = tensor([1.99010, 1.00066], requires_grad=True), loss = 0.00041709 grad = tensor([-0.08298,  0.00595])\n",
      "ep 490: w = tensor([1.99063, 1.00063], requires_grad=True), loss = 0.00037425 grad = tensor([-0.07866,  0.00471])\n",
      "ep 495: w = tensor([1.99112, 1.00060], requires_grad=True), loss = 0.00033581 grad = tensor([-0.07447,  0.00512])\n",
      "ep 500: w = tensor([1.99159, 1.00056], requires_grad=True), loss = 0.00030129 grad = tensor([-0.07053,  0.00497])\n",
      "ep 505: w = tensor([1.99203, 1.00053], requires_grad=True), loss = 0.00027033 grad = tensor([-0.06684,  0.00425])\n",
      "ep 510: w = tensor([1.99245, 1.00051], requires_grad=True), loss = 0.00024254 grad = tensor([-0.06329,  0.00439])\n",
      "ep 515: w = tensor([1.99285, 1.00048], requires_grad=True), loss = 0.00021764 grad = tensor([-0.05998,  0.00373])\n",
      "ep 520: w = tensor([1.99323, 1.00045], requires_grad=True), loss = 0.00019526 grad = tensor([-0.05680,  0.00376])\n",
      "ep 525: w = tensor([1.99359, 1.00043], requires_grad=True), loss = 0.00017518 grad = tensor([-0.05380,  0.00354])\n",
      "ep 530: w = tensor([1.99392, 1.00041], requires_grad=True), loss = 0.00015719 grad = tensor([-0.05095,  0.00349])\n",
      "ep 535: w = tensor([1.99425, 1.00039], requires_grad=True), loss = 0.00014103 grad = tensor([-0.04827,  0.00313])\n",
      "ep 540: w = tensor([1.99455, 1.00037], requires_grad=True), loss = 0.00012652 grad = tensor([-0.04570,  0.00323])\n",
      "ep 545: w = tensor([1.99484, 1.00035], requires_grad=True), loss = 0.00011353 grad = tensor([-0.04331,  0.00290])\n",
      "ep 550: w = tensor([1.99511, 1.00033], requires_grad=True), loss = 0.00010187 grad = tensor([-0.04103,  0.00267])\n",
      "ep 555: w = tensor([1.99537, 1.00031], requires_grad=True), loss = 0.00009140 grad = tensor([-0.03886,  0.00257])\n",
      "ep 560: w = tensor([1.99561, 1.00029], requires_grad=True), loss = 0.00008202 grad = tensor([-0.03678,  0.00282])\n",
      "ep 565: w = tensor([1.99584, 1.00028], requires_grad=True), loss = 0.00007359 grad = tensor([-0.03490,  0.00187])\n",
      "ep 570: w = tensor([1.99606, 1.00026], requires_grad=True), loss = 0.00006602 grad = tensor([-0.03301,  0.00237])\n",
      "ep 575: w = tensor([1.99627, 1.00025], requires_grad=True), loss = 0.00005925 grad = tensor([-0.03131,  0.00177])\n",
      "ep 580: w = tensor([1.99647, 1.00024], requires_grad=True), loss = 0.00005316 grad = tensor([-0.02962,  0.00223])\n",
      "ep 585: w = tensor([1.99665, 1.00022], requires_grad=True), loss = 0.00004769 grad = tensor([-0.02807,  0.00181])\n",
      "ep 590: w = tensor([1.99683, 1.00021], requires_grad=True), loss = 0.00004279 grad = tensor([-0.02660,  0.00154])\n",
      "ep 595: w = tensor([1.99700, 1.00020], requires_grad=True), loss = 0.00003839 grad = tensor([-0.02516,  0.00196])\n",
      "ep 600: w = tensor([1.99716, 1.00019], requires_grad=True), loss = 0.00003444 grad = tensor([-0.02386,  0.00145])\n",
      "ep 605: w = tensor([1.99731, 1.00018], requires_grad=True), loss = 0.00003091 grad = tensor([-0.02259,  0.00156])\n",
      "ep 610: w = tensor([1.99745, 1.00017], requires_grad=True), loss = 0.00002773 grad = tensor([-0.02138,  0.00179])\n",
      "ep 615: w = tensor([1.99758, 1.00016], requires_grad=True), loss = 0.00002489 grad = tensor([-0.02030,  0.00092])\n",
      "ep 620: w = tensor([1.99771, 1.00015], requires_grad=True), loss = 0.00002233 grad = tensor([-0.01918,  0.00168])\n",
      "ep 625: w = tensor([1.99783, 1.00015], requires_grad=True), loss = 0.00002004 grad = tensor([-0.01821,  0.00098])\n",
      "ep 630: w = tensor([1.99795, 1.00014], requires_grad=True), loss = 0.00001798 grad = tensor([-0.01723,  0.00118])\n",
      "ep 635: w = tensor([1.99805, 1.00013], requires_grad=True), loss = 0.00001613 grad = tensor([-0.01633,  0.00100])\n",
      "ep 640: w = tensor([1.99816, 1.00012], requires_grad=True), loss = 0.00001448 grad = tensor([-0.01547,  0.00095])\n",
      "ep 645: w = tensor([1.99825, 1.00012], requires_grad=True), loss = 0.00001298 grad = tensor([-0.01461,  0.00145])\n",
      "ep 650: w = tensor([1.99835, 1.00011], requires_grad=True), loss = 0.00001165 grad = tensor([-0.01390,  0.00051])\n",
      "ep 655: w = tensor([1.99843, 1.00010], requires_grad=True), loss = 0.00001045 grad = tensor([-0.01312,  0.00114])\n",
      "ep 660: w = tensor([1.99852, 1.00010], requires_grad=True), loss = 0.00000938 grad = tensor([-0.01247,  0.00051])\n",
      "ep 665: w = tensor([1.99859, 1.00009], requires_grad=True), loss = 0.00000841 grad = tensor([-0.01178,  0.00090])\n",
      "ep 670: w = tensor([1.99867, 1.00009], requires_grad=True), loss = 0.00000755 grad = tensor([-0.01119,  0.00039])\n",
      "ep 675: w = tensor([1.99874, 1.00008], requires_grad=True), loss = 0.00000678 grad = tensor([-0.01054,  0.00137])\n",
      "ep 680: w = tensor([1.99881, 1.00008], requires_grad=True), loss = 0.00000608 grad = tensor([-1.00603e-02,  8.58307e-05])\n",
      "ep 685: w = tensor([1.99887, 1.00008], requires_grad=True), loss = 0.00000546 grad = tensor([-0.00946,  0.00114])\n",
      "ep 690: w = tensor([1.99893, 1.00007], requires_grad=True), loss = 0.00000489 grad = tensor([-9.02748e-03,  5.72205e-05])\n",
      "ep 695: w = tensor([1.99898, 1.00007], requires_grad=True), loss = 0.00000439 grad = tensor([-0.00851,  0.00069])\n",
      "ep 700: w = tensor([1.99904, 1.00006], requires_grad=True), loss = 0.00000394 grad = tensor([-0.00805,  0.00076])\n",
      "ep 705: w = tensor([1.99909, 1.00006], requires_grad=True), loss = 0.00000353 grad = tensor([-0.00765,  0.00038])\n",
      "ep 710: w = tensor([1.99914, 1.00006], requires_grad=True), loss = 0.00000317 grad = tensor([-0.00726,  0.00015])\n",
      "ep 715: w = tensor([1.99918, 1.00005], requires_grad=True), loss = 0.00000284 grad = tensor([-0.00684,  0.00063])\n",
      "ep 720: w = tensor([1.99923, 1.00005], requires_grad=True), loss = 0.00000255 grad = tensor([-6.51836e-03,  9.53674e-05])\n",
      "ep 725: w = tensor([1.99927, 1.00005], requires_grad=True), loss = 0.00000229 grad = tensor([-0.00612,  0.00083])\n",
      "ep 730: w = tensor([1.99931, 1.00005], requires_grad=True), loss = 0.00000206 grad = tensor([-0.00586,  0.00000])\n",
      "ep 735: w = tensor([1.99934, 1.00004], requires_grad=True), loss = 0.00000184 grad = tensor([-0.00549,  0.00076])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 740: w = tensor([1.99938, 1.00004], requires_grad=True), loss = 0.00000166 grad = tensor([-5.25570e-03, -3.81470e-05])\n",
      "ep 745: w = tensor([1.99941, 1.00004], requires_grad=True), loss = 0.00000149 grad = tensor([-0.00494,  0.00058])\n",
      "ep 750: w = tensor([1.99944, 1.00004], requires_grad=True), loss = 0.00000133 grad = tensor([-0.00467,  0.00057])\n",
      "ep 755: w = tensor([1.99947, 1.00004], requires_grad=True), loss = 0.00000119 grad = tensor([-0.00445,  0.00010])\n",
      "ep 760: w = tensor([1.99950, 1.00003], requires_grad=True), loss = 0.00000107 grad = tensor([-0.00419,  0.00046])\n",
      "ep 765: w = tensor([1.99952, 1.00003], requires_grad=True), loss = 0.00000096 grad = tensor([-0.00398,  0.00027])\n",
      "ep 770: w = tensor([1.99955, 1.00003], requires_grad=True), loss = 0.00000086 grad = tensor([-0.00379,  0.00010])\n",
      "ep 775: w = tensor([1.99957, 1.00003], requires_grad=True), loss = 0.00000077 grad = tensor([-0.00357,  0.00038])\n",
      "ep 780: w = tensor([1.99960, 1.00003], requires_grad=True), loss = 0.00000069 grad = tensor([-3.39699e-03,  6.67572e-05])\n",
      "ep 785: w = tensor([1.99962, 1.00003], requires_grad=True), loss = 0.00000062 grad = tensor([-0.00320,  0.00031])\n",
      "ep 790: w = tensor([1.99964, 1.00002], requires_grad=True), loss = 0.00000056 grad = tensor([-0.00303,  0.00041])\n",
      "ep 795: w = tensor([1.99966, 1.00002], requires_grad=True), loss = 0.00000050 grad = tensor([-0.00287,  0.00035])\n",
      "ep 800: w = tensor([1.99967, 1.00002], requires_grad=True), loss = 0.00000045 grad = tensor([-2.73609e-03,  3.81470e-05])\n",
      "ep 805: w = tensor([1.99969, 1.00002], requires_grad=True), loss = 0.00000040 grad = tensor([-0.00257,  0.00031])\n",
      "ep 810: w = tensor([1.99971, 1.00002], requires_grad=True), loss = 0.00000036 grad = tensor([-0.00246,  0.00000])\n",
      "ep 815: w = tensor([1.99972, 1.00002], requires_grad=True), loss = 0.00000033 grad = tensor([-0.00232,  0.00015])\n",
      "ep 820: w = tensor([1.99974, 1.00002], requires_grad=True), loss = 0.00000029 grad = tensor([-0.00219,  0.00019])\n",
      "ep 825: w = tensor([1.99975, 1.00002], requires_grad=True), loss = 0.00000026 grad = tensor([-0.00208,  0.00011])\n",
      "ep 830: w = tensor([1.99977, 1.00002], requires_grad=True), loss = 0.00000024 grad = tensor([-1.97983e-03,  9.53674e-06])\n",
      "ep 835: w = tensor([1.99978, 1.00001], requires_grad=True), loss = 0.00000021 grad = tensor([-0.00186,  0.00025])\n",
      "ep 840: w = tensor([1.99979, 1.00001], requires_grad=True), loss = 0.00000019 grad = tensor([-0.00177,  0.00000])\n",
      "ep 845: w = tensor([1.99980, 1.00001], requires_grad=True), loss = 0.00000017 grad = tensor([-0.00167,  0.00015])\n",
      "ep 850: w = tensor([1.99981, 1.00001], requires_grad=True), loss = 0.00000015 grad = tensor([-0.00158,  0.00020])\n",
      "ep 855: w = tensor([1.99982, 1.00001], requires_grad=True), loss = 0.00000014 grad = tensor([-0.00151,  0.00000])\n",
      "ep 860: w = tensor([1.99983, 1.00001], requires_grad=True), loss = 0.00000012 grad = tensor([-0.00145, -0.00031])\n",
      "ep 865: w = tensor([1.99984, 1.00001], requires_grad=True), loss = 0.00000011 grad = tensor([-0.00134,  0.00018])\n",
      "ep 870: w = tensor([1.99985, 1.00001], requires_grad=True), loss = 0.00000010 grad = tensor([-0.00127,  0.00023])\n",
      "ep 875: w = tensor([1.99986, 1.00001], requires_grad=True), loss = 0.00000009 grad = tensor([-0.00124, -0.00038])\n",
      "ep 880: w = tensor([1.99986, 1.00001], requires_grad=True), loss = 0.00000008 grad = tensor([-0.00111,  0.00054])\n",
      "ep 885: w = tensor([1.99987, 1.00001], requires_grad=True), loss = 0.00000007 grad = tensor([-0.00111, -0.00032])\n",
      "ep 890: w = tensor([1.99988, 1.00001], requires_grad=True), loss = 0.00000006 grad = tensor([-0.00095,  0.00114])\n",
      "ep 895: w = tensor([1.99988, 1.00001], requires_grad=True), loss = 0.00000006 grad = tensor([-0.00102, -0.00066])\n",
      "ep 900: w = tensor([1.99989, 1.00001], requires_grad=True), loss = 0.00000005 grad = tensor([-0.00089,  0.00051])\n",
      "ep 905: w = tensor([1.99990, 1.00001], requires_grad=True), loss = 0.00000005 grad = tensor([-0.00092, -0.00057])\n",
      "ep 910: w = tensor([1.99990, 1.00001], requires_grad=True), loss = 0.00000004 grad = tensor([-0.00081,  0.00035])\n",
      "ep 915: w = tensor([1.99991, 1.00001], requires_grad=True), loss = 0.00000004 grad = tensor([-0.00077,  0.00025])\n",
      "ep 920: w = tensor([1.99991, 1.00001], requires_grad=True), loss = 0.00000003 grad = tensor([-7.50542e-04, -2.86102e-05])\n",
      "ep 925: w = tensor([1.99992, 1.00001], requires_grad=True), loss = 0.00000003 grad = tensor([-0.00070,  0.00015])\n",
      "ep 930: w = tensor([1.99992, 1.00001], requires_grad=True), loss = 0.00000003 grad = tensor([-0.00069, -0.00031])\n",
      "ep 935: w = tensor([1.99992, 1.00000], requires_grad=True), loss = 0.00000002 grad = tensor([-0.00062,  0.00025])\n",
      "ep 940: w = tensor([1.99993, 1.00001], requires_grad=True), loss = 0.00000002 grad = tensor([-0.00062, -0.00035])\n",
      "ep 945: w = tensor([1.99993, 1.00000], requires_grad=True), loss = 0.00000002 grad = tensor([-0.00055,  0.00019])\n",
      "ep 950: w = tensor([1.99994, 1.00000], requires_grad=True), loss = 0.00000002 grad = tensor([-0.00055, -0.00019])\n",
      "ep 955: w = tensor([1.99994, 1.00000], requires_grad=True), loss = 0.00000002 grad = tensor([-0.00050,  0.00019])\n",
      "ep 960: w = tensor([1.99994, 1.00000], requires_grad=True), loss = 0.00000001 grad = tensor([-4.83513e-04, -2.86102e-05])\n",
      "ep 965: w = tensor([1.99995, 1.00000], requires_grad=True), loss = 0.00000001 grad = tensor([-4.52995e-04,  9.53674e-05])\n",
      "ep 970: w = tensor([1.99995, 1.00000], requires_grad=True), loss = 0.00000001 grad = tensor([-0.00046, -0.00038])\n",
      "ep 975: w = tensor([1.99995, 1.00000], requires_grad=True), loss = 0.00000001 grad = tensor([-0.00037,  0.00063])\n",
      "ep 980: w = tensor([1.99995, 1.00000], requires_grad=True), loss = 0.00000001 grad = tensor([-0.00040, -0.00019])\n",
      "ep 985: w = tensor([1.99996, 1.00000], requires_grad=True), loss = 0.00000001 grad = tensor([-0.00034,  0.00038])\n",
      "ep 990: w = tensor([1.99996, 1.00000], requires_grad=True), loss = 0.00000001 grad = tensor([-3.47137e-04, -9.53674e-06])\n",
      "ep 995: w = tensor([1.99996, 1.00000], requires_grad=True), loss = 0.00000001 grad = tensor([-0.00034, -0.00019])\n",
      "ep 1000: w = tensor([1.99996, 1.00000], requires_grad=True), loss = 0.00000001 grad = tensor([-0.00029,  0.00038])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Модель линейной регрессии (с несколькими параметрами)\n",
    "# f = X * w \n",
    "\n",
    "#--------------------\n",
    "# 0) Training samples\n",
    "\n",
    "# Данные для обучения: \n",
    "# принзаки X: рассматривается 4 наблюдения (ось 0) и 2 признака (ось 1):\n",
    "X = torch.tensor([[1., 40.],\n",
    "                  [2., 30.],\n",
    "                  [3., 20.],\n",
    "                  [4., 10.]], dtype=torch.float32) # Size([4, 2])\n",
    "\n",
    "# истинное значение весов (используется только для получения обучающих правильных ответов):\n",
    "w_ans = torch.tensor([2., 1.], dtype=torch.float32)\n",
    "# Y - приавильные ответы: \n",
    "Y = X @ w_ans\n",
    "\n",
    "torch.set_printoptions(precision=5) # точность вывода на печать значений тензоров\n",
    "print(f'w true value = {w_ans}, Y = {Y}')\n",
    "\n",
    "#--------------------\n",
    "# 1) Design Model: Weights to optimize and forward function\n",
    "\n",
    "# изначальное значение весов w\n",
    "w = torch.tensor([0.0, 0.0], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "\n",
    "# model (модель, в нашем случае: линейная регрессия)\n",
    "# прямое распространение:\n",
    "def forward(X):\n",
    "    return X @ w # Size([4])\n",
    "\n",
    "#--------------------\n",
    "# 2) Define loss and optimizer\n",
    "\n",
    "# callable function\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "# loss = MSE (функция потерь, в нашем слаучае: средняя квадратичная ошибка)\n",
    "# def loss(y, y_pred):\n",
    "#     return ((y_pred - y)**2).mean() # Size([])\n",
    "\n",
    "learning_rate = 0.0013\n",
    "optimizer = torch.optim.SGD([w], lr=learning_rate)\n",
    "\n",
    "\n",
    "#--------------------\n",
    "# 3) Training loop\n",
    "# основной цикл:\n",
    "n_iters = 1000 + 1\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "         \n",
    "    # calculate gradients = backward pass\n",
    "    l.backward()\n",
    "    \n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        dw = optimizer.param_groups[0]['params'][0].grad\n",
    "        print(f'ep {epoch}: w = {w}, loss = {l:.8f} grad = {w.grad}')                        \n",
    "#         print(f'ep {epoch}: w = {w}, y_p = {y_pred}, loss = {l:.8f}\\ngrad = {dw}')                \n",
    "#         print(f'ep {epoch:3}: y_p = {y_pred}, loss = {l:.8f} grd = {dw}')    \n",
    "\n",
    "    # zero the gradients after updating\n",
    "    optimizer.zero_grad()    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "Использование модели:\n",
    "\n",
    "`torch.nn.Linear(in_features, out_features, bias=True)`\n",
    "Applies a linear transformation to the incoming data: $y = xA^T + b$\n",
    "\n",
    "Parameters:\n",
    "* `in_features` – size of each input sample\n",
    "* `out_features` – size of each output sample\n",
    "* `bias` – If set to False, the layer will not learn an additive bias. Default: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2\n",
      "Prediction before training: f(5) = tensor([[-18.99730],\n",
      "        [-14.79363],\n",
      "        [-10.58997],\n",
      "        [ -6.38630]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# X_test = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32) # torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "X = torch.tensor([[1., 40.],\n",
    "                  [2., 30.],\n",
    "                  [3., 20.],\n",
    "                  [4., 10.]], dtype=torch.float32) # Size([4, 2])\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "print(n_samples, n_features)\n",
    "\n",
    "# n_samples, n_features = X_test.shape\n",
    "# input_size = n_features\n",
    "# output_size = n_features\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # define diferent layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim, bias=False)\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "    \n",
    "model = LinearRegression(n_features, 1)\n",
    "\n",
    "\n",
    "print(f'Prediction before training: f(5) = {model(X)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape = torch.Size([4, 2])\n",
      "Y.shape = torch.Size([4, 1])\n",
      "w true value = tensor([2., 1.]), Y = tensor([[42.],\n",
      "        [34.],\n",
      "        [26.],\n",
      "        [18.]])\n",
      "Prediction before training: f(tensor([[ 1., 40.],\n",
      "        [ 2., 30.],\n",
      "        [ 3., 20.],\n",
      "        [ 4., 10.]])) = tensor([[10.77437],\n",
      "        [ 8.85886],\n",
      "        [ 6.94336],\n",
      "        [ 5.02785]], grad_fn=<MmBackward0>)\n",
      " w=tensor([[0.62247, 0.25380]])\n",
      "ep 0: w = tensor([[0.74634, 1.88797]]), loss = 534.63720703 grad = tensor([[  -95.28321, -1257.05676]])\n",
      "ep 5: w = tensor([[0.71822, 0.43480]]), loss = 352.47018433 grad = tensor([[  57.90713, 1019.41907]])\n",
      "ep 10: w = tensor([[0.86223, 1.60364]]), loss = 232.79104614 grad = tensor([[ -65.14955, -824.43488]])\n",
      "ep 15: w = tensor([[0.86043, 0.64926]]), loss = 154.12103271 grad = tensor([[ 35.48081, 668.89294]])\n",
      "ep 20: w = tensor([[0.97066, 1.41495]]), loss = 102.36933899 grad = tensor([[ -45.11044, -540.65851]])\n",
      "ep 25: w = tensor([[0.98441, 0.78789]]), loss = 68.29065704 grad = tensor([[ 21.02690, 438.93506]])\n",
      "ep 30: w = tensor([[1.07087, 1.28922]]), loss = 45.81860352 grad = tensor([[ -31.72119, -354.52164]])\n",
      "ep 35: w = tensor([[1.09329, 0.87698]]), loss = 30.97233963 grad = tensor([[ 11.77637, 288.07050]])\n",
      "ep 40: w = tensor([[1.16270, 1.20500]]), loss = 21.13931274 grad = tensor([[ -22.71922, -232.43347]])\n",
      "ep 45: w = tensor([[1.18943, 0.93377]]), loss = 14.60447407 grad = tensor([[  5.91544, 189.09174]])\n",
      "ep 50: w = tensor([[1.24635, 1.14818]]), loss = 10.24193382 grad = tensor([[ -16.61769, -152.35809]])\n",
      "ep 55: w = tensor([[1.27467, 0.96953]]), loss = 7.31217051 grad = tensor([[  2.25669, 124.15018]])\n",
      "ep 60: w = tensor([[1.32222, 1.10950]]), loss = 5.32932186 grad = tensor([[-12.43903, -99.84192]])\n",
      "ep 65: w = tensor([[1.35048, 0.99166]]), loss = 3.97385526 grad = tensor([[2.35138e-02, 8.15389e+01]])\n",
      "ep 70: w = tensor([[1.39084, 1.08286]]), loss = 3.03554773 grad = tensor([[ -9.53981, -65.40211]])\n",
      "ep 75: w = tensor([[1.41806, 1.00498]]), loss = 2.37598014 grad = tensor([[-1.29177, 53.57552]])\n",
      "ep 80: w = tensor([[1.45277, 1.06425]]), loss = 1.90377557 grad = tensor([[ -7.49612, -42.81937]])\n",
      "ep 85: w = tensor([[1.47841, 1.01264]]), loss = 1.55855381 grad = tensor([[-2.02024, 35.22311]])\n",
      "ep 90: w = tensor([[1.50856, 1.05103]]), loss = 1.30026865 grad = tensor([[ -6.02829, -28.01457]])\n",
      "ep 95: w = tensor([[1.53237, 1.01670]]), loss = 1.10227966 grad = tensor([[-2.37755, 23.17651]])\n",
      "ep 100: w = tensor([[1.55877, 1.04144]]), loss = 0.94677621 grad = tensor([[ -4.95123, -18.31004]])\n",
      "ep 105: w = tensor([[1.58066, 1.01850]]), loss = 0.82178593 grad = tensor([[-2.50380, 15.26677]])\n",
      "ep 110: w = tensor([[1.60392, 1.03433]]), loss = 0.71916324 grad = tensor([[ -4.14227, -11.95122]])\n",
      "ep 115: w = tensor([[1.62391, 1.01891]]), loss = 0.63333857 grad = tensor([[-2.48962, 10.07118]])\n",
      "ep 120: w = tensor([[1.64450, 1.02894]]), loss = 0.56043446 grad = tensor([[-3.51968, -7.78564]])\n",
      "ep 125: w = tensor([[1.66266, 1.01848]]), loss = 0.49771130 grad = tensor([[-2.39327,  6.65709]])\n",
      "ep 130: w = tensor([[1.68095, 1.02474]]), loss = 0.44320148 grad = tensor([[-3.02883, -5.05905]])\n",
      "ep 135: w = tensor([[1.69740, 1.01757]]), loss = 0.39545012 grad = tensor([[-2.25191,  4.41298]])\n",
      "ep 140: w = tensor([[1.71368, 1.02139]]), loss = 0.35336262 grad = tensor([[-2.63285, -3.27530]])\n",
      "ep 145: w = tensor([[1.72854, 1.01642]]), loss = 0.31610078 grad = tensor([[-2.08910,  2.93598]])\n",
      "ep 150: w = tensor([[1.74307, 1.01866]]), loss = 0.28299296 grad = tensor([[-2.30671, -2.10972]])\n",
      "ep 155: w = tensor([[1.75646, 1.01516]]), loss = 0.25350267 grad = tensor([[-1.91946,  1.96238]])\n",
      "ep 160: w = tensor([[1.76945, 1.01640]]), loss = 0.22718281 grad = tensor([[-2.03318, -1.34869]])\n",
      "ep 165: w = tensor([[1.78151, 1.01388]]), loss = 0.20366088 grad = tensor([[-1.75173,  1.32021]])\n",
      "ep 170: w = tensor([[1.79313, 1.01449]]), loss = 0.18261614 grad = tensor([[-1.80032, -0.85326]])\n",
      "ep 175: w = tensor([[1.80397, 1.01264]]), loss = 0.16377395 grad = tensor([[-1.59109,  0.89544]])\n",
      "ep 180: w = tensor([[1.81438, 1.01285]]), loss = 0.14689445 grad = tensor([[-1.59964, -0.53192]])\n",
      "ep 185: w = tensor([[1.82413, 1.01146]]), loss = 0.13176687 grad = tensor([[-1.44026,  0.61442]])\n",
      "ep 190: w = tensor([[1.83345, 1.01143]]), loss = 0.11820388 grad = tensor([[-1.42496, -0.32362]])\n",
      "ep 195: w = tensor([[1.84220, 1.01036]]), loss = 0.10604337 grad = tensor([[-1.30061,  0.42665]])\n",
      "ep 200: w = tensor([[1.85056, 1.01019]]), loss = 0.09513646 grad = tensor([[-1.27176, -0.18952]])\n",
      "ep 205: w = tensor([[1.85842, 1.00935]]), loss = 0.08535381 grad = tensor([[-1.17242,  0.30120]])\n",
      "ep 210: w = tensor([[1.86591, 1.00910]]), loss = 0.07657827 grad = tensor([[-1.13664, -0.10402]])\n",
      "ep 215: w = tensor([[1.87298, 1.00842]]), loss = 0.06870645 grad = tensor([[-1.05552,  0.21681]])\n",
      "ep 220: w = tensor([[1.87969, 1.00814]]), loss = 0.06164394 grad = tensor([[-1.01694, -0.05017]])\n",
      "ep 225: w = tensor([[1.88603, 1.00758]]), loss = 0.05530818 grad = tensor([[-0.94940,  0.15945]])\n",
      "ep 230: w = tensor([[1.89205, 1.00729]]), loss = 0.04962354 grad = tensor([[-0.91052, -0.01635]])\n",
      "ep 235: w = tensor([[1.89774, 1.00682]]), loss = 0.04452337 grad = tensor([[-0.85343,  0.11932]])\n",
      "ep 240: w = tensor([[1.90314, 1.00652]]), loss = 0.03994750 grad = tensor([[-0.81566,  0.00444]])\n",
      "ep 245: w = tensor([[1.90825, 1.00612]]), loss = 0.03584224 grad = tensor([[-0.76673,  0.09201]])\n",
      "ep 250: w = tensor([[1.91310, 1.00585]]), loss = 0.03215866 grad = tensor([[-0.73103,  0.01609]])\n",
      "ep 255: w = tensor([[1.91768, 1.00550]]), loss = 0.02885388 grad = tensor([[-0.68858,  0.07302]])\n",
      "ep 260: w = tensor([[1.92203, 1.00524]]), loss = 0.02588875 grad = tensor([[-0.65538,  0.02227]])\n",
      "ep 265: w = tensor([[1.92614, 1.00494]]), loss = 0.02322838 grad = tensor([[-0.61825,  0.05913]])\n",
      "ep 270: w = tensor([[1.93004, 1.00470]]), loss = 0.02084134 grad = tensor([[-0.58770,  0.02495]])\n",
      "ep 275: w = tensor([[1.93373, 1.00444]]), loss = 0.01869953 grad = tensor([[-0.55500,  0.04884]])\n",
      "ep 280: w = tensor([[1.93723, 1.00421]]), loss = 0.01677767 grad = tensor([[-0.52707,  0.02590]])\n",
      "ep 285: w = tensor([[1.94054, 1.00398]]), loss = 0.01505375 grad = tensor([[-0.49814,  0.04121]])\n",
      "ep 290: w = tensor([[1.94368, 1.00378]]), loss = 0.01350673 grad = tensor([[-0.47278,  0.02517]])\n",
      "ep 295: w = tensor([[1.94665, 1.00357]]), loss = 0.01211873 grad = tensor([[-0.44703,  0.03572]])\n",
      "ep 300: w = tensor([[1.94947, 1.00339]]), loss = 0.01087339 grad = tensor([[-0.42413,  0.02357]])\n",
      "ep 305: w = tensor([[1.95213, 1.00321]]), loss = 0.00975596 grad = tensor([[-0.40117,  0.03091]])\n",
      "ep 310: w = tensor([[1.95466, 1.00304]]), loss = 0.00875344 grad = tensor([[-0.38048,  0.02213]])\n",
      "ep 315: w = tensor([[1.95705, 1.00288]]), loss = 0.00785386 grad = tensor([[-0.35999,  0.02696]])\n",
      "ep 320: w = tensor([[1.95932, 1.00273]]), loss = 0.00704673 grad = tensor([[-0.34132,  0.02072]])\n",
      "ep 325: w = tensor([[1.96147, 1.00258]]), loss = 0.00632255 grad = tensor([[-0.32305,  0.02346]])\n",
      "ep 330: w = tensor([[1.96350, 1.00245]]), loss = 0.00567289 grad = tensor([[-0.30622,  0.01888]])\n",
      "ep 335: w = tensor([[1.96543, 1.00232]]), loss = 0.00508993 grad = tensor([[-0.28986,  0.02099]])\n",
      "ep 340: w = tensor([[1.96725, 1.00220]]), loss = 0.00456686 grad = tensor([[-0.27475,  0.01703]])\n",
      "ep 345: w = tensor([[1.96898, 1.00208]]), loss = 0.00409749 grad = tensor([[-0.26009,  0.01849]])\n",
      "ep 350: w = tensor([[1.97062, 1.00197]]), loss = 0.00367642 grad = tensor([[-0.24649,  0.01563]])\n",
      "ep 355: w = tensor([[1.97217, 1.00187]]), loss = 0.00329867 grad = tensor([[-0.23340,  0.01602]])\n",
      "ep 360: w = tensor([[1.97364, 1.00177]]), loss = 0.00295964 grad = tensor([[-0.22113,  0.01444]])\n",
      "ep 365: w = tensor([[1.97503, 1.00167]]), loss = 0.00265555 grad = tensor([[-0.20940,  0.01465]])\n",
      "ep 370: w = tensor([[1.97635, 1.00159]]), loss = 0.00238265 grad = tensor([[-0.19842,  0.01278]])\n",
      "ep 375: w = tensor([[1.97759, 1.00150]]), loss = 0.00213771 grad = tensor([[-0.18788,  0.01306]])\n",
      "ep 380: w = tensor([[1.97878, 1.00142]]), loss = 0.00191810 grad = tensor([[-0.17801,  0.01170]])\n",
      "ep 385: w = tensor([[1.97990, 1.00135]]), loss = 0.00172101 grad = tensor([[-0.16861,  0.01125]])\n",
      "ep 390: w = tensor([[1.98096, 1.00128]]), loss = 0.00154416 grad = tensor([[-0.15971,  0.01064]])\n",
      "ep 395: w = tensor([[1.98196, 1.00121]]), loss = 0.00138540 grad = tensor([[-0.15129,  0.00992]])\n",
      "ep 400: w = tensor([[1.98291, 1.00115]]), loss = 0.00124301 grad = tensor([[-0.14328,  0.00978]])\n",
      "ep 405: w = tensor([[1.98382, 1.00108]]), loss = 0.00111530 grad = tensor([[-0.13573,  0.00916]])\n",
      "ep 410: w = tensor([[1.98467, 1.00103]]), loss = 0.00100073 grad = tensor([[-0.12858,  0.00843]])\n",
      "ep 415: w = tensor([[1.98548, 1.00097]]), loss = 0.00089787 grad = tensor([[-0.12177,  0.00838]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 420: w = tensor([[1.98625, 1.00092]]), loss = 0.00080559 grad = tensor([[-0.11536,  0.00770]])\n",
      "ep 425: w = tensor([[1.98697, 1.00087]]), loss = 0.00072281 grad = tensor([[-0.10926,  0.00740]])\n",
      "ep 430: w = tensor([[1.98766, 1.00083]]), loss = 0.00064850 grad = tensor([[-0.10350,  0.00690]])\n",
      "ep 435: w = tensor([[1.98831, 1.00078]]), loss = 0.00058183 grad = tensor([[-0.09804,  0.00648]])\n",
      "ep 440: w = tensor([[1.98893, 1.00074]]), loss = 0.00052207 grad = tensor([[-0.09287,  0.00618]])\n",
      "ep 445: w = tensor([[1.98951, 1.00070]]), loss = 0.00046841 grad = tensor([[-0.08797,  0.00585]])\n",
      "ep 450: w = tensor([[1.99007, 1.00067]]), loss = 0.00042026 grad = tensor([[-0.08331,  0.00565]])\n",
      "ep 455: w = tensor([[1.99059, 1.00063]]), loss = 0.00037706 grad = tensor([[-0.07893,  0.00514]])\n",
      "ep 460: w = tensor([[1.99109, 1.00060]]), loss = 0.00033833 grad = tensor([[-0.07474,  0.00531]])\n",
      "ep 465: w = tensor([[1.99156, 1.00057]]), loss = 0.00030357 grad = tensor([[-0.07083,  0.00449]])\n",
      "ep 470: w = tensor([[1.99200, 1.00054]]), loss = 0.00027236 grad = tensor([[-0.06706,  0.00464]])\n",
      "ep 475: w = tensor([[1.99242, 1.00051]]), loss = 0.00024436 grad = tensor([[-0.06357,  0.00376]])\n",
      "ep 480: w = tensor([[1.99282, 1.00048]]), loss = 0.00021926 grad = tensor([[-0.06015,  0.00444]])\n",
      "ep 485: w = tensor([[1.99320, 1.00046]]), loss = 0.00019674 grad = tensor([[-0.05703,  0.00351]])\n",
      "ep 490: w = tensor([[1.99356, 1.00043]]), loss = 0.00017652 grad = tensor([[-0.05396,  0.00420]])\n",
      "ep 495: w = tensor([[1.99390, 1.00041]]), loss = 0.00015839 grad = tensor([[-0.05119,  0.00289]])\n",
      "ep 500: w = tensor([[1.99422, 1.00039]]), loss = 0.00014211 grad = tensor([[-0.04840,  0.00395]])\n",
      "ep 505: w = tensor([[1.99453, 1.00037]]), loss = 0.00012750 grad = tensor([[-0.04592,  0.00261]])\n",
      "ep 510: w = tensor([[1.99482, 1.00035]]), loss = 0.00011438 grad = tensor([[-0.04343,  0.00353]])\n",
      "ep 515: w = tensor([[1.99509, 1.00033]]), loss = 0.00010264 grad = tensor([[-0.04121,  0.00223]])\n",
      "ep 520: w = tensor([[1.99535, 1.00031]]), loss = 0.00009211 grad = tensor([[-0.03897,  0.00311]])\n",
      "ep 525: w = tensor([[1.99559, 1.00030]]), loss = 0.00008263 grad = tensor([[-0.03698,  0.00191]])\n",
      "ep 530: w = tensor([[1.99583, 1.00028]]), loss = 0.00007414 grad = tensor([[-0.03497,  0.00271]])\n",
      "ep 535: w = tensor([[1.99605, 1.00027]]), loss = 0.00006652 grad = tensor([[-0.03317,  0.00191]])\n",
      "ep 540: w = tensor([[1.99626, 1.00025]]), loss = 0.00005969 grad = tensor([[-0.03139,  0.00222]])\n",
      "ep 545: w = tensor([[1.99645, 1.00024]]), loss = 0.00005355 grad = tensor([[-0.02974,  0.00195]])\n",
      "ep 550: w = tensor([[1.99664, 1.00022]]), loss = 0.00004805 grad = tensor([[-0.02815,  0.00216]])\n",
      "ep 555: w = tensor([[1.99682, 1.00021]]), loss = 0.00004311 grad = tensor([[-0.02672,  0.00136]])\n",
      "ep 560: w = tensor([[1.99699, 1.00020]]), loss = 0.00003868 grad = tensor([[-0.02526,  0.00202]])\n",
      "ep 565: w = tensor([[1.99714, 1.00019]]), loss = 0.00003471 grad = tensor([[-0.02398,  0.00114]])\n",
      "ep 570: w = tensor([[1.99730, 1.00018]]), loss = 0.00003114 grad = tensor([[-0.02264,  0.00210]])\n",
      "ep 575: w = tensor([[1.99744, 1.00017]]), loss = 0.00002794 grad = tensor([[-0.02153,  0.00079]])\n",
      "ep 580: w = tensor([[1.99757, 1.00016]]), loss = 0.00002507 grad = tensor([[-0.02031,  0.00200]])\n",
      "ep 585: w = tensor([[1.99770, 1.00015]]), loss = 0.00002250 grad = tensor([[-0.01931,  0.00082]])\n",
      "ep 590: w = tensor([[1.99782, 1.00015]]), loss = 0.00002018 grad = tensor([[-0.01824,  0.00152]])\n",
      "ep 595: w = tensor([[1.99794, 1.00014]]), loss = 0.00001811 grad = tensor([[-0.01731,  0.00101]])\n",
      "ep 600: w = tensor([[1.99805, 1.00013]]), loss = 0.00001624 grad = tensor([[-0.01637,  0.00120]])\n",
      "ep 605: w = tensor([[1.99815, 1.00012]]), loss = 0.00001457 grad = tensor([[-0.01551,  0.00114]])\n",
      "ep 610: w = tensor([[1.99825, 1.00012]]), loss = 0.00001308 grad = tensor([[-0.01472,  0.00057]])\n",
      "ep 615: w = tensor([[1.99834, 1.00011]]), loss = 0.00001173 grad = tensor([[-0.01390,  0.00130]])\n",
      "ep 620: w = tensor([[1.99843, 1.00011]]), loss = 0.00001053 grad = tensor([[-0.01320,  0.00069]])\n",
      "ep 625: w = tensor([[1.99851, 1.00010]]), loss = 0.00000944 grad = tensor([[-0.01245,  0.00139]])\n",
      "ep 630: w = tensor([[1.99859, 1.00009]]), loss = 0.00000847 grad = tensor([[-0.01184,  0.00057]])\n",
      "ep 635: w = tensor([[1.99866, 1.00009]]), loss = 0.00000760 grad = tensor([[-0.01120,  0.00080]])\n",
      "ep 640: w = tensor([[1.99873, 1.00008]]), loss = 0.00000683 grad = tensor([[-0.01062,  0.00065]])\n",
      "ep 645: w = tensor([[1.99880, 1.00008]]), loss = 0.00000612 grad = tensor([[-0.01006,  0.00053]])\n",
      "ep 650: w = tensor([[1.99886, 1.00008]]), loss = 0.00000549 grad = tensor([[-0.00953,  0.00051]])\n",
      "ep 655: w = tensor([[1.99892, 1.00007]]), loss = 0.00000493 grad = tensor([[-0.00902,  0.00071]])\n",
      "ep 660: w = tensor([[1.99898, 1.00007]]), loss = 0.00000442 grad = tensor([[-0.00854,  0.00057]])\n",
      "ep 665: w = tensor([[1.99903, 1.00006]]), loss = 0.00000397 grad = tensor([[-0.00810,  0.00053]])\n",
      "ep 670: w = tensor([[1.99909, 1.00006]]), loss = 0.00000356 grad = tensor([[-0.00768,  0.00042]])\n",
      "ep 675: w = tensor([[1.99913, 1.00006]]), loss = 0.00000319 grad = tensor([[-7.29942e-03, -5.72205e-05]])\n",
      "ep 680: w = tensor([[1.99918, 1.00005]]), loss = 0.00000287 grad = tensor([[-0.00683,  0.00120]])\n",
      "ep 685: w = tensor([[1.99922, 1.00005]]), loss = 0.00000257 grad = tensor([[-6.54888e-03, -2.86102e-05]])\n",
      "ep 690: w = tensor([[1.99926, 1.00005]]), loss = 0.00000231 grad = tensor([[-0.00616,  0.00056]])\n",
      "ep 695: w = tensor([[1.99930, 1.00005]]), loss = 0.00000207 grad = tensor([[-5.87940e-03, -9.53674e-05]])\n",
      "ep 700: w = tensor([[1.99934, 1.00004]]), loss = 0.00000186 grad = tensor([[-0.00550,  0.00102]])\n",
      "ep 705: w = tensor([[1.99937, 1.00004]]), loss = 0.00000167 grad = tensor([[-0.00525,  0.00023]])\n",
      "ep 710: w = tensor([[1.99941, 1.00004]]), loss = 0.00000150 grad = tensor([[-0.00497,  0.00032]])\n",
      "ep 715: w = tensor([[1.99944, 1.00004]]), loss = 0.00000134 grad = tensor([[-0.00470,  0.00042]])\n",
      "ep 720: w = tensor([[1.99947, 1.00004]]), loss = 0.00000120 grad = tensor([[-0.00446,  0.00027]])\n",
      "ep 725: w = tensor([[1.99950, 1.00003]]), loss = 0.00000108 grad = tensor([[-0.00421,  0.00046]])\n",
      "ep 730: w = tensor([[1.99952, 1.00003]]), loss = 0.00000097 grad = tensor([[-0.00401,  0.00013]])\n",
      "ep 735: w = tensor([[1.99955, 1.00003]]), loss = 0.00000087 grad = tensor([[-0.00376,  0.00069]])\n",
      "ep 740: w = tensor([[1.99957, 1.00003]]), loss = 0.00000078 grad = tensor([[-0.00359,  0.00025]])\n",
      "ep 745: w = tensor([[1.99959, 1.00003]]), loss = 0.00000070 grad = tensor([[-0.00339,  0.00039]])\n",
      "ep 750: w = tensor([[1.99962, 1.00003]]), loss = 0.00000063 grad = tensor([[-0.00321,  0.00035]])\n",
      "ep 755: w = tensor([[1.99964, 1.00002]]), loss = 0.00000056 grad = tensor([[-0.00309, -0.00031]])\n",
      "ep 760: w = tensor([[1.99966, 1.00002]]), loss = 0.00000051 grad = tensor([[-0.00286,  0.00065]])\n",
      "ep 765: w = tensor([[1.99967, 1.00002]]), loss = 0.00000045 grad = tensor([[-2.75326e-03, -2.86102e-05]])\n",
      "ep 770: w = tensor([[1.99969, 1.00002]]), loss = 0.00000041 grad = tensor([[-0.00259,  0.00020]])\n",
      "ep 775: w = tensor([[1.99971, 1.00002]]), loss = 0.00000036 grad = tensor([[-0.00245,  0.00025]])\n",
      "ep 780: w = tensor([[1.99972, 1.00002]]), loss = 0.00000033 grad = tensor([[-2.33555e-03,  3.81470e-05]])\n",
      "ep 785: w = tensor([[1.99974, 1.00002]]), loss = 0.00000029 grad = tensor([[-0.00221,  0.00000]])\n",
      "ep 790: w = tensor([[1.99975, 1.00002]]), loss = 0.00000026 grad = tensor([[-0.00208,  0.00027]])\n",
      "ep 795: w = tensor([[1.99976, 1.00002]]), loss = 0.00000024 grad = tensor([[-0.00198,  0.00000]])\n",
      "ep 800: w = tensor([[1.99978, 1.00001]]), loss = 0.00000021 grad = tensor([[-1.87492e-03,  5.72205e-05]])\n",
      "ep 805: w = tensor([[1.99979, 1.00001]]), loss = 0.00000019 grad = tensor([[-0.00176,  0.00029]])\n",
      "ep 810: w = tensor([[1.99980, 1.00001]]), loss = 0.00000017 grad = tensor([[-0.00168,  0.00012]])\n",
      "ep 815: w = tensor([[1.99981, 1.00001]]), loss = 0.00000015 grad = tensor([[-0.00157,  0.00034]])\n",
      "ep 820: w = tensor([[1.99982, 1.00001]]), loss = 0.00000014 grad = tensor([[-0.00153, -0.00019]])\n",
      "ep 825: w = tensor([[1.99983, 1.00001]]), loss = 0.00000012 grad = tensor([[-0.00139,  0.00071]])\n",
      "ep 830: w = tensor([[1.99984, 1.00001]]), loss = 0.00000011 grad = tensor([[-0.00140, -0.00073]])\n",
      "ep 835: w = tensor([[1.99985, 1.00001]]), loss = 0.00000010 grad = tensor([[-0.00121,  0.00120]])\n",
      "ep 840: w = tensor([[1.99986, 1.00001]]), loss = 0.00000009 grad = tensor([[-0.00126, -0.00063]])\n",
      "ep 845: w = tensor([[1.99986, 1.00001]]), loss = 0.00000008 grad = tensor([[-0.00111,  0.00061]])\n",
      "ep 850: w = tensor([[1.99987, 1.00001]]), loss = 0.00000007 grad = tensor([[-0.00113, -0.00051]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 855: w = tensor([[1.99988, 1.00001]]), loss = 0.00000006 grad = tensor([[-0.00100,  0.00056]])\n",
      "ep 860: w = tensor([[1.99988, 1.00001]]), loss = 0.00000006 grad = tensor([[-0.00099, -0.00019]])\n",
      "ep 865: w = tensor([[1.99989, 1.00001]]), loss = 0.00000005 grad = tensor([[-0.00091,  0.00034]])\n",
      "ep 870: w = tensor([[1.99990, 1.00001]]), loss = 0.00000005 grad = tensor([[-0.00092, -0.00057]])\n",
      "ep 875: w = tensor([[1.99990, 1.00001]]), loss = 0.00000004 grad = tensor([[-0.00081,  0.00034]])\n",
      "ep 880: w = tensor([[1.99991, 1.00001]]), loss = 0.00000004 grad = tensor([[-0.00077,  0.00023]])\n",
      "ep 885: w = tensor([[1.99991, 1.00001]]), loss = 0.00000003 grad = tensor([[-0.00074,  0.00015]])\n",
      "ep 890: w = tensor([[1.99992, 1.00001]]), loss = 0.00000003 grad = tensor([[-7.16209e-04, -8.58307e-05]])\n",
      "ep 895: w = tensor([[1.99992, 1.00001]]), loss = 0.00000003 grad = tensor([[-0.00065,  0.00025]])\n",
      "ep 900: w = tensor([[1.99992, 1.00001]]), loss = 0.00000002 grad = tensor([[-6.31332e-04,  6.67572e-05]])\n",
      "ep 905: w = tensor([[1.99993, 1.00000]]), loss = 0.00000002 grad = tensor([[-0.00059,  0.00011]])\n",
      "ep 910: w = tensor([[1.99993, 1.00000]]), loss = 0.00000002 grad = tensor([[-0.00056,  0.00016]])\n",
      "ep 915: w = tensor([[1.99994, 1.00000]]), loss = 0.00000002 grad = tensor([[-0.00055, -0.00019]])\n",
      "ep 920: w = tensor([[1.99994, 1.00000]]), loss = 0.00000002 grad = tensor([[-0.00049,  0.00032]])\n",
      "ep 925: w = tensor([[1.99994, 1.00000]]), loss = 0.00000001 grad = tensor([[-0.00051, -0.00037]])\n",
      "ep 930: w = tensor([[1.99995, 1.00000]]), loss = 0.00000001 grad = tensor([[-4.55856e-04,  7.62939e-05]])\n",
      "ep 935: w = tensor([[1.99995, 1.00000]]), loss = 0.00000001 grad = tensor([[-0.00045, -0.00023]])\n",
      "ep 940: w = tensor([[1.99995, 1.00000]]), loss = 0.00000001 grad = tensor([[-4.06265e-04,  5.72205e-05]])\n",
      "ep 945: w = tensor([[1.99995, 1.00000]]), loss = 0.00000001 grad = tensor([[-0.00041, -0.00022]])\n",
      "ep 950: w = tensor([[1.99996, 1.00000]]), loss = 0.00000001 grad = tensor([[-0.00034,  0.00038]])\n",
      "ep 955: w = tensor([[1.99996, 1.00000]]), loss = 0.00000001 grad = tensor([[-0.00039, -0.00063]])\n",
      "ep 960: w = tensor([[1.99996, 1.00000]]), loss = 0.00000001 grad = tensor([[-0.00029,  0.00067]])\n",
      "ep 965: w = tensor([[1.99996, 1.00000]]), loss = 0.00000001 grad = tensor([[-0.00034, -0.00038]])\n",
      "ep 970: w = tensor([[1.99996, 1.00000]]), loss = 0.00000001 grad = tensor([[-0.00026,  0.00058]])\n",
      "ep 975: w = tensor([[1.99997, 1.00000]]), loss = 0.00000000 grad = tensor([[-0.00032, -0.00054]])\n",
      "ep 980: w = tensor([[1.99997, 1.00000]]), loss = 0.00000000 grad = tensor([[-0.00023,  0.00057]])\n",
      "ep 985: w = tensor([[1.99997, 1.00000]]), loss = 0.00000000 grad = tensor([[-0.00028, -0.00034]])\n",
      "ep 990: w = tensor([[1.99997, 1.00000]]), loss = 0.00000000 grad = tensor([[-0.00023,  0.00011]])\n",
      "ep 995: w = tensor([[1.99997, 1.00000]]), loss = 0.00000000 grad = tensor([[-0.00024, -0.00015]])\n",
      "ep 1000: w = tensor([[1.99997, 1.00000]]), loss = 0.00000000 grad = tensor([[-2.11716e-04,  6.67572e-05]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Модель линейной регрессии (с несколькими параметрами)\n",
    "# f = X * w \n",
    "\n",
    "#--------------------\n",
    "# 0) Training samples\n",
    "\n",
    "# Данные для обучения: \n",
    "# принзаки X: рассматривается 4 наблюдения (ось 0) и 2 признака (ось 1):\n",
    "X = torch.tensor([[1., 40.],\n",
    "                  [2., 30.],\n",
    "                  [3., 20.],\n",
    "                  [4., 10.]], dtype=torch.float32) # Size([4, 2])\n",
    "\n",
    "print(f'X.shape = {X.shape}')\n",
    "X_samples, X_features = X.shape\n",
    "\n",
    "# истинное значение весов (используется только для получения обучающих правильных ответов):\n",
    "w_ans = torch.tensor([2., 1.], dtype=torch.float32)\n",
    "# Y - приавильные ответы: \n",
    "Y = X @ w_ans\n",
    "Y = Y.reshape((-1, 1))\n",
    "print(f'Y.shape = {Y.shape}')\n",
    "Y_features = 1\n",
    "\n",
    "torch.set_printoptions(precision=5) # точность вывода на печать значений тензоров\n",
    "print(f'w true value = {w_ans}, Y = {Y}')\n",
    "\n",
    "#--------------------\n",
    "# 1) Design Model, the model has to implement the forward pass!\n",
    "# Here we can use a built-in model from PyTorch\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "# we can call this model with samples X\n",
    "# model = nn.Linear(input_size, output_size)\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # define diferent layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim, bias=False)\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "    \n",
    "model = LinearRegression(X_features, Y_features)\n",
    "\n",
    "\n",
    "print(f'Prediction before training: f({X}) = {model(X)}\\n w={model.lin.weight.data}')\n",
    "\n",
    "\n",
    "# # model (модель, в нашем случае: линейная регрессия)\n",
    "# # прямое распространение:\n",
    "# def forward(X):\n",
    "#     return X @ w # Size([4])\n",
    "\n",
    "#--------------------\n",
    "# 2) Define loss and optimizer\n",
    "\n",
    "# callable function\n",
    "criterion  = nn.MSELoss()\n",
    "\n",
    "learning_rate = 0.0013\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#--------------------\n",
    "# 3) Training loop\n",
    "# основной цикл:\n",
    "n_iters = 1000 + 1\n",
    "# n_iters = 40 + 1\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # Forward pass and loss\n",
    "    y_predicted = model(X)\n",
    "    loss = criterion(y_predicted, Y)\n",
    "\n",
    "    \n",
    "    # Backward pass and update\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        w = model.lin.weight.data\n",
    "        dw = model.lin.weight.grad\n",
    "        print(f'ep {epoch}: w = {w}, loss = {loss:.8f} grad = {dw}')                        \n",
    "#         print(f'ep {epoch}: w = {w}, y_p = {y_pred}, loss = {l:.8f}\\ngrad = {dw}')                \n",
    "#         print(f'ep {epoch:3}: y_p = {y_pred}, loss = {l:.8f} grd = {dw}')       \n",
    "    \n",
    "    # zero grad before new step\n",
    "    optimizer.zero_grad()    \n",
    "    \n",
    "#     if epoch % 5 == 0:\n",
    "#         print(f'epoch {epoch}: w = {w}, y_pred = {y_pred}, loss = {l:.8f}\\ngradient = {dw}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42., 34., 26., 18.])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[26.74803],\n",
       "        [22.73413],\n",
       "        [18.72023],\n",
       "        [14.70633]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.21181, 1.08752]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lin.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[45.71261, 37.04921, 28.38582, 19.72242]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lin.weight.data @ X.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x3 and 2x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11628\\3043280319.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x3 and 2x1)"
     ]
    }
   ],
   "source": [
    "model.lin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lin.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.99989, 0.60001]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lin.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.99989, 0.60001]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.parameter.Parameter"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(list(model.parameters())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': [Parameter containing:\n",
       "   tensor([[5.99989, 0.60001]], requires_grad=True)],\n",
       "  'lr': 0.0013,\n",
       "  'momentum': 0,\n",
       "  'dampening': 0,\n",
       "  'weight_decay': 0,\n",
       "  'nesterov': False,\n",
       "  'maximize': False,\n",
       "  'foreach': None,\n",
       "  'differentiable': False}]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.param_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[5.99989, 0.60001]], requires_grad=True)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.param_groups[0]['params'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.param_groups[0]['params'][0].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_parameter() missing 1 required positional argument: 'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11628\\3518948.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_parameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: get_parameter() missing 1 required positional argument: 'target'"
     ]
    }
   ],
   "source": [
    "model.get_parameter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.99988, 0.60001]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.99988, 0.60001]])\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "#     p.requires_grad: bool\n",
    "    print(p.data) #: Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2, out_features=1, bias=False)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__func__',\n",
       " '__ge__',\n",
       " '__get__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__self__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model.lin.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_forward_hooks',\n",
       " '_forward_pre_hooks',\n",
       " '_get_backward_hooks',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_version',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'children',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'ipu',\n",
       " 'lin',\n",
       " 'load_state_dict',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'set_extra_state',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Спасибо за внимание!\n",
    "\n",
    "---\n",
    "### Технический раздел:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> next <em class=\"qs\"></em> qs line \n",
    "<br/> next <em class=\"an\"></em> an line \n",
    "<br/> next <em class=\"nt\"></em> an line \n",
    "<br/> next <em class=\"df\"></em> df line \n",
    "<br/> next <em class=\"ex\"></em> ex line \n",
    "<br/> next <em class=\"pl\"></em> pl line \n",
    "<br/> next <em class=\"mn\"></em> mn line \n",
    "<br/> next <em class=\"plmn\"></em> plmn line \n",
    "<br/> next <em class=\"hn\"></em> hn line "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Работа с графом потока вычислений нужна  для того, чтобы решить __задачу обучения многослойной ИНС__. А эта задача требует после получения резуьтатов и оценки ошибки __выполнения обратного прохода__ дающего градиент ошибки для весов (параметров) модели и последующей процедуры оптимизации весов. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
