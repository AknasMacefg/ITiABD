{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "58b7de24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import time\n",
    "from urllib.parse import unquote, urljoin, urlparse\n",
    "import html\n",
    "import sys\n",
    "import io\n",
    "\n",
    "# Ensure UTF-8 encoding for output (Jupyter-compatible)\n",
    "try:\n",
    "    # Check if we're in Jupyter/IPython - only modify if buffer exists\n",
    "    if hasattr(sys.stdout, 'buffer') and sys.stdout.encoding != 'utf-8':\n",
    "        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')\n",
    "    if hasattr(sys.stderr, 'buffer') and sys.stderr.encoding != 'utf-8':\n",
    "        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')\n",
    "except (AttributeError, ValueError):\n",
    "    # In Jupyter, stdout/stderr are already UTF-8 compatible, no action needed\n",
    "    pass\n",
    "\n",
    "# Configuration\n",
    "BASE_URL = \"https://ru.wikipedia.org\"\n",
    "CATEGORY_URL = \"https://ru.wikipedia.org/wiki/Категория:Родившиеся_18_ноября\"\n",
    "EXISTING_XML = \"pyy1118.xml\"\n",
    "OUTPUT_XML = \"qyy1118.xml\"\n",
    "\n",
    "# Russian occupation names for matching - expanded list\n",
    "OCCUPATIONS = [\n",
    "    # Writers and journalists\n",
    "    \"писатель\", \"поэт\", \"прозаик\", \"драматург\", \"журналист\", \"публицист\", \"эссеист\", \"критик\", \"литературный критик\",\n",
    "    \"библиограф\", \"библиофил\", \"лексикограф\", \"литературовед\", \"историк литературы\",\n",
    "    # Artists and designers\n",
    "    \"художник\", \"скульптор\", \"архитектор\", \"дизайнер\", \"иллюстратор\", \"график\", \"живописец\", \"портретист\",\n",
    "    \"карикатурист\", \"гравёр\", \"художник-гравёр\", \"художник-график\", \"художник-постановщик\", \"художник-мультипликатор\",\n",
    "    \"деятель изобразительного искусства\", \"керамист\", \"керамистка\", \"гобеленистка\", \"рисовальщик\", \"рисовальщица\",\n",
    "    # Film and theater\n",
    "    \"актёр\", \"актриса\", \"режиссёр\", \"кинорежиссёр\", \"сценарист\", \"оператор\", \"кинематографист\", \"кинооператор\",\n",
    "    \"театральный деятель\", \"танцор\", \"хореограф\", \"балетмейстер\", \"артистка балета\", \"театральный актёр\",\n",
    "    \"театральный педагог\", \"театральный критик\", \"театровед\", \"кинокритик\", \"киновед\", \"искусствовед\",\n",
    "    # Music\n",
    "    \"музыкант\", \"композитор\", \"певец\", \"дирижёр\", \"пианист\", \"пианистка\", \"басистка\",\n",
    "    # Scientists and educators\n",
    "    \"учёный\", \"исследователь\", \"профессор\", \"преподаватель\", \"учитель\", \"лектор\", \"преподаватель университета\",\n",
    "    \"физик\", \"химик\", \"математик\", \"биолог\", \"ботаник\", \"геолог\", \"историк\", \"географ\", \"археолог\", \"лингвист\",\n",
    "    \"антрополог\", \"этнограф\", \"палеонтолог\", \"палеоботаник\", \"вирусолог\", \"микробиолог\", \"биохимик\", \"физиолог\",\n",
    "    \"анатом\", \"энтомолог\", \"зоолог\", \"орнитолог\", \"ихтиолог\", \"лепидоптеролог\", \"лихенолог\", \"селекционер\",\n",
    "    \"филолог\", \"классицист\", \"классицистка\", \"романист\", \"нумизматка\", \"византолог\",\n",
    "    # Medical\n",
    "    \"врач\", \"медик\", \"хирург\", \"психиатр\", \"невролог\", \"патолог\", \"фармаколог\", \"военный врач\",\n",
    "    # Engineering and technology\n",
    "    \"инженер\", \"конструктор\", \"изобретатель\", \"авиационный инженер\", \"горный инженер\", \"военный инженер\",\n",
    "    \"инженер-строитель\", \"инженер-механик\", \"инженер-гидротехник\", \"инженер-архитектор\",\n",
    "    # Politics and law\n",
    "    \"политик\", \"государственный деятель\", \"дипломат\", \"юрист\", \"адвокат\", \"практикующий юрист\", \"судья\",\n",
    "    \"прокурор\", \"депутат\", \"посол\", \"министр\", \"президент\", \"премьер-министр\",\n",
    "    # Military\n",
    "    \"военный деятель\", \"военнослужащий\", \"офицер\", \"полководец\", \"лётчик\", \"солдат\", \"капитан\", \"генерал\",\n",
    "    \"адмирал\", \"командир\", \"разведчик\", \"мемуарист\",\n",
    "    # Sports\n",
    "    \"спортсмен\", \"футболист\", \"тренер\", \"тренер-преподаватель\", \"бейсболист\", \"хоккеист\", \"биатлонист\",\n",
    "    \"бобслеист\", \"скелетонист\", \"автогонщик\", \"раллийный автогонщик\",\n",
    "    # Business\n",
    "    \"предприниматель\", \"бизнесмен\", \"торговец\", \"банкир\", \"менеджер\", \"промышленник\", \"магнат\",\n",
    "    # Religious\n",
    "    \"священник\", \"религиозный деятель\", \"монах\", \"раввин\", \"настоятель\", \"миссионер\", \"теолог\", \"богослов\",\n",
    "    # Philosophy and humanities\n",
    "    \"философ\", \"философ науки\", \"историк науки\", \"культуролог\", \"социолог\", \"экономист\", \"политолог\",\n",
    "    # Other professions\n",
    "    \"библиотекарь\", \"переводчик\", \"редактор\", \"издатель\", \"фотограф\", \"кинопродюсер\", \"телепродюсер\",\n",
    "    \"телеведущий\", \"радиоведущий\", \"подкастер\", \"обозреватель\", \"корреспондент\", \"репортёр\",\n",
    "    \"модель\", \"фотомодель\", \"порноактриса\", \"эротическая модель\", \"участница конкурса красоты\",\n",
    "    \"блогер\", \"видеоблогер\", \"стример\", \"тиктокер\", \"ютубер\",\n",
    "    \"модельер\", \"кутюрье\", \"переплётчица\",\n",
    "    # General\n",
    "    \"долгожитель\", \"аристократ\", \"аристократка\", \"фрейлина\", \"аэронавт\", \"капитан судна\",\n",
    "    \"капитан речного флота\", \"шеф-повар\", \"агроном\", \"доярка\", \"свинарка\", \"шахтёр\", \"рабочий\", \"крестьянин\",\n",
    "    \"партизан\", \"революционерка\", \"активист\", \"активистка\", \"гуманистка\", \"правозащитница\",\n",
    "    \"коллекционер\", \"коллекционерка\", \"коллекционер искусства\"\n",
    "]\n",
    "\n",
    "OCCUPATION_CATEGORIES = {\n",
    "    \"Писатель\": [\"писатель\", \"поэт\", \"прозаик\", \"драматург\", \"журналист\", \"публицист\", \"переводчик\", \"редактор\", \"издатель\"],\n",
    "    \"Художник\": [\"художник\", \"скульптор\", \"архитектор\", \"дизайнер\", \"иллюстратор\", \"фотограф\"],\n",
    "    \"Кинематографист\": [\"актёр\", \"актриса\", \"режиссёр\", \"кинорежиссёр\", \"сценарист\", \"оператор\", \"кинематографист\"],\n",
    "    \"Театральный деятель\": [\"театральный деятель\", \"актёр\", \"актриса\", \"режиссёр\", \"танцор\", \"хореограф\"],\n",
    "    \"Учёный\": [\"учёный\", \"исследователь\", \"профессор\", \"преподаватель\", \"учитель\", \"физик\", \"химик\", \"математик\", \"биолог\", \"ботаник\", \"геолог\", \"историк\", \"географ\", \"археолог\", \"лингвист\"],\n",
    "    \"Врач\": [\"врач\", \"медик\", \"хирург\", \"психиатр\", \"невролог\"],\n",
    "    \"Военный деятель\": [\"военный деятель\", \"военнослужащий\", \"офицер\", \"полководец\", \"лётчик\"],\n",
    "    \"Государственный деятель\": [\"государственный деятель\", \"политик\", \"дипломат\", \"юрист\", \"адвокат\"],\n",
    "    \"Политик\": [\"политик\", \"государственный деятель\"],\n",
    "    \"Предприниматель\": [\"предприниматель\", \"бизнесмен\", \"торговец\"],\n",
    "    \"Религиозный деятель\": [\"религиозный деятель\", \"священник\", \"монах\"],\n",
    "    \"Философ\": [\"философ\"],\n",
    "    \"Персона\": [\"долгожитель\", \"аристократ\", \"аристократка\", \"фрейлина\"],\n",
    "    \"Порноактриса\": [\"порноактриса\"],\n",
    "    \"Фотомодель\": [\"фотомодель\", \"модель\"],\n",
    "    \"Модельер\": [\"модельер\"],\n",
    "    \"Блогер\": [\"блогер\", \"видеоблогер\", \"стример\", \"тиктокер\"],\n",
    "    \"Ректор\": [\"ректор\"],\n",
    "    \"Священник\": [\"священник\"],\n",
    "    \"Убийца\": [\"убийца\"],\n",
    "    \"Футболист\": [\"футболист\"],\n",
    "    \"Биатлонист\": [\"биатлонист\"],\n",
    "    \"Карточка фотографа\": [\"фотограф\"]\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "75243cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading pyy1118.xml: not well-formed (invalid token): line 1230, column 23\n"
     ]
    }
   ],
   "source": [
    "def normalize_name(name):\n",
    "    \"\"\"Normalize a name to a standard format for comparison\"\"\"\n",
    "    if not name:\n",
    "        return ''\n",
    "    # Decode URL encoding if present\n",
    "    normalized = unquote(name)\n",
    "    # Ensure underscores are used (not spaces) to match XML format\n",
    "    normalized = normalized.replace(' ', '_')\n",
    "    return normalized\n",
    "\n",
    "def load_existing_entries(xml_file):\n",
    "    \"\"\"Load existing entries from XML file with their data for comparison\"\"\"\n",
    "    existing_data = {}  # Dictionary: normalized_name -> {'year': year, 'occupation': occupation}\n",
    "    existing_names = set()  # Set of all name variations for quick lookup\n",
    "    \n",
    "    try:\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        for psn in root.findall('psn'):\n",
    "            name = psn.get('h', '')\n",
    "            if name:\n",
    "                # Normalize the name\n",
    "                normalized = normalize_name(name)\n",
    "                \n",
    "                # Extract year and occupation\n",
    "                year = psn.get('y', '')\n",
    "                occupation_attr = psn.get('p', '')\n",
    "                \n",
    "                # Extract occupation (remove category if present, format: \"Category;occupation\")\n",
    "                occupation = ''\n",
    "                if occupation_attr:\n",
    "                    if ';' in occupation_attr:\n",
    "                        occupation = occupation_attr.split(';', 1)[1]  # Get part after semicolon\n",
    "                    else:\n",
    "                        occupation = occupation_attr\n",
    "                \n",
    "                # Store data by normalized name\n",
    "                existing_data[normalized] = {\n",
    "                    'year': year if year and year != 'yyyy' else None,\n",
    "                    'occupation': occupation\n",
    "                }\n",
    "                \n",
    "                # Also add name variations for quick lookup\n",
    "                existing_names.add(normalized)\n",
    "                existing_names.add(name)\n",
    "                try:\n",
    "                    from urllib.parse import quote\n",
    "                    existing_names.add(quote(normalized, safe=''))\n",
    "                    existing_names.add(quote(name, safe=''))\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        print(f\"Loaded {len(existing_data)} existing entries from {xml_file}\")\n",
    "        print(f\"Sample entries: {list(existing_data.keys())[:3] if existing_data else 'None'}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {xml_file} not found, starting fresh\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {xml_file}: {e}\")\n",
    "    \n",
    "    return existing_data, existing_names\n",
    "\n",
    "existing_data, existing_names = load_existing_entries(EXISTING_XML)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "10b82b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def get_page_content(url):\n",
    "    \"\"\"Fetch Wikipedia page content\"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        return BeautifulSoup(response.content, 'html.parser')\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_year_of_birth(soup):\n",
    "    \"\"\"Extract year of birth from Wikipedia page\"\"\"\n",
    "    # Try to find in infobox (most reliable)\n",
    "    infobox = soup.find('table', class_='infobox')\n",
    "    if infobox:\n",
    "        # Look for \"Дата рождения\" or \"Родился/Родилась\"\n",
    "        for row in infobox.find_all('tr'):\n",
    "            th = row.find('th')\n",
    "            if th:\n",
    "                th_text = th.get_text().strip()\n",
    "                if 'Дата рождения' in th_text or 'Родился' in th_text or 'Родилась' in th_text:\n",
    "                    td = row.find('td')\n",
    "                    if td:\n",
    "                        text = td.get_text()\n",
    "                        # Extract year (4 digits, between 1000-2099)\n",
    "                        years = re.findall(r'\\b(1[0-9]{3}|20[0-2][0-9])\\b', text)\n",
    "                        if years:\n",
    "                            # Return the first valid year found\n",
    "                            return years[0]\n",
    "    \n",
    "    # Try to find in first paragraph\n",
    "    first_para = soup.find('div', class_='mw-parser-output')\n",
    "    if first_para:\n",
    "        paragraphs = first_para.find_all('p', recursive=False)\n",
    "        for p in paragraphs[:3]:  # Check first 3 paragraphs\n",
    "            text = p.get_text()\n",
    "            # Look for patterns like \"родился 14 ноября 1900 года\" or \"родился в 1900 году\"\n",
    "            patterns = [\n",
    "                r'(?:родился|родилась)\\s+(?:14\\s+ноября\\s+)?(\\d{4})\\s*(?:года|г\\.|г)',\n",
    "                r'(?:родился|родилась)\\s+в\\s+(\\d{4})\\s*(?:году|г\\.|г)',\n",
    "                r'(\\d{4})\\s*(?:года|г\\.|г)\\s*—\\s*(?:родился|родилась)',\n",
    "                r'14\\s+ноября\\s+(\\d{4})',  # Direct pattern for \"14 ноября YYYY\"\n",
    "            ]\n",
    "            for pattern in patterns:\n",
    "                year_match = re.search(pattern, text, re.IGNORECASE)\n",
    "                if year_match:\n",
    "                    year = year_match.group(1)\n",
    "                    if 1000 <= int(year) <= 2099:\n",
    "                        return year\n",
    "            \n",
    "            # Or just find 4-digit year near \"14 ноября\" or \"ноября\"\n",
    "            if '14 ноября' in text or 'ноября' in text:\n",
    "                years = re.findall(r'\\b(1[0-9]{3}|20[0-2][0-9])\\b', text)\n",
    "                if years:\n",
    "                    return years[0]\n",
    "    \n",
    "    return None\n",
    "\n",
    "def extract_occupation(soup):\n",
    "    \"\"\"Extract occupation from Wikipedia page\"\"\"\n",
    "    occupations_found = []\n",
    "    category_found = None\n",
    "    \n",
    "    # Try to find in infobox first (most reliable)\n",
    "    infobox = soup.find('table', class_='infobox')\n",
    "    if infobox:\n",
    "        # Look for \"Род деятельности\" or \"Профессия\"\n",
    "        for row in infobox.find_all('tr'):\n",
    "            th = row.find('th')\n",
    "            if th:\n",
    "                th_text = th.get_text().strip()\n",
    "                if 'Род деятельности' in th_text or 'Профессия' in th_text or 'Род занятий' in th_text:\n",
    "                    td = row.find('td')\n",
    "                    if td:\n",
    "                        text = td.get_text().lower()\n",
    "                        # Extract occupations\n",
    "                        for occ in OCCUPATIONS:\n",
    "                            if occ.lower() in text and occ not in occupations_found:\n",
    "                                occupations_found.append(occ)\n",
    "    \n",
    "    # Try to find in categories (at bottom of page)\n",
    "    categories = soup.find('div', id='catlinks')\n",
    "    if categories:\n",
    "        cat_text = categories.get_text().lower()\n",
    "        # Check each category\n",
    "        for cat_name, occs in OCCUPATION_CATEGORIES.items():\n",
    "            # Check if any occupation from this category is mentioned\n",
    "            for occ in occs:\n",
    "                if occ.lower() in cat_text:\n",
    "                    category_found = cat_name\n",
    "                    if occ not in occupations_found:\n",
    "                        occupations_found.append(occ)\n",
    "                    break\n",
    "    \n",
    "    # Try to find in first paragraph if nothing found yet\n",
    "    if not occupations_found:\n",
    "        first_para = soup.find('div', class_='mw-parser-output')\n",
    "        if first_para:\n",
    "            paragraphs = first_para.find_all('p', recursive=False)\n",
    "            for p in paragraphs[:3]:\n",
    "                text = p.get_text().lower()\n",
    "                for occ in OCCUPATIONS:\n",
    "                    if occ.lower() in text and occ not in occupations_found:\n",
    "                        occupations_found.append(occ)\n",
    "    \n",
    "    # Format occupation string (no category, just occupations)\n",
    "    # Remove \"персона\" from occupations if present\n",
    "    occupations_found = [occ for occ in occupations_found if occ != 'персона']\n",
    "    \n",
    "    if occupations_found:\n",
    "        occ_str = ', '.join(occupations_found[:5])  # Limit to 5 occupations\n",
    "        return occ_str\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"Helper functions defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "efb71457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category parser function defined\n"
     ]
    }
   ],
   "source": [
    "def parse_category_page(url):\n",
    "    \"\"\"Parse category page and extract person links\"\"\"\n",
    "    soup = get_page_content(url)\n",
    "    if not soup:\n",
    "        return [], None\n",
    "    \n",
    "    person_links = []\n",
    "    \n",
    "    # Find all links in the category page\n",
    "    # Try different possible containers\n",
    "    content = soup.find('div', id='mw-pages')\n",
    "    if not content:\n",
    "        content = soup.find('div', class_='mw-category')\n",
    "    if not content:\n",
    "        content = soup.find('div', class_='mw-category-group')\n",
    "    \n",
    "    # List of special page prefixes to skip\n",
    "    skip_prefixes = ['Категория:', 'Файл:', 'Шаблон:', 'Обсуждение:', 'Участник:', 'Портал:', \n",
    "                     'Википедия:', 'Служебная:', 'Медиа:', 'Справка:', 'Обсуждение_шаблона:',\n",
    "                     'Обсуждение_участника:', 'Обсуждение_файла:', 'Обсуждение_категории:']\n",
    "    \n",
    "    if content:\n",
    "        # Find all links in the category listing\n",
    "        for link in content.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if href.startswith('/wiki/'):\n",
    "                title_part = href.split('/wiki/')[1].split('#')[0]\n",
    "                # Decode to check if it's a special page\n",
    "                title_decoded = unquote(title_part)\n",
    "                \n",
    "                # Skip special pages\n",
    "                should_skip = False\n",
    "                for prefix in skip_prefixes:\n",
    "                    if title_part.startswith(prefix) or title_decoded.startswith(prefix):\n",
    "                        should_skip = True\n",
    "                        break\n",
    "                \n",
    "                if not should_skip:\n",
    "                    full_url = urljoin(BASE_URL, href)\n",
    "                    # Use the URL-encoded title for consistency with XML format\n",
    "                    person_links.append((title_part, full_url))\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    seen = set()\n",
    "    unique_links = []\n",
    "    for title, url in person_links:\n",
    "        if title not in seen:\n",
    "            seen.add(title)\n",
    "            unique_links.append((title, url))\n",
    "    \n",
    "    # Find \"Следующая страница\" link - improved search\n",
    "    next_link = None\n",
    "    \n",
    "    # Method 1: Look for link with exact text \"Следующая страница\"\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        link_text = link.get_text().strip()\n",
    "        if link_text == 'Следующая страница':\n",
    "            next_link = urljoin(BASE_URL, link['href'])\n",
    "            break\n",
    "    \n",
    "    # Method 2: Look in navigation divs (more comprehensive)\n",
    "    if not next_link:\n",
    "        # Try different navigation containers\n",
    "        nav_selectors = [\n",
    "            ('div', {'class': 'mw-category-navigation'}),\n",
    "            ('div', {'id': 'mw-pages'}),\n",
    "            ('div', {'class': 'mw-category'}),\n",
    "        ]\n",
    "        \n",
    "        for tag, attrs in nav_selectors:\n",
    "            nav_divs = soup.find_all(tag, attrs)\n",
    "            for nav_div in nav_divs:\n",
    "                for link in nav_div.find_all('a', href=True):\n",
    "                    link_text = link.get_text().strip()\n",
    "                    # Check for \"Следующая страница\" or variations\n",
    "                    if 'Следующая' in link_text or 'следующая' in link_text or 'Следующая страница' in link_text:\n",
    "                        href = link.get('href')\n",
    "                        if href and href.startswith('/wiki/'):\n",
    "                            next_link = urljoin(BASE_URL, href)\n",
    "                            break\n",
    "                if next_link:\n",
    "                    break\n",
    "            if next_link:\n",
    "                break\n",
    "    \n",
    "    # Method 3: Look for links with \"from\" parameter (pagination)\n",
    "    if not next_link:\n",
    "        # Wikipedia category pages use \"from\" parameter for pagination\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link.get('href', '')\n",
    "            if 'from=' in href and 'Категория:Родившиеся_14_ноября' in href:\n",
    "                next_link = urljoin(BASE_URL, href)\n",
    "                break\n",
    "    \n",
    "    # Method 4: Look for \"next\" class or similar\n",
    "    if not next_link:\n",
    "        next_links = soup.find_all('a', class_=re.compile('next', re.I))\n",
    "        for link in next_links:\n",
    "            if link.get('href'):\n",
    "                next_link = urljoin(BASE_URL, link['href'])\n",
    "                break\n",
    "    \n",
    "    return unique_links, next_link\n",
    "\n",
    "print(\"Category parser function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6b77e1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person parser function defined\n"
     ]
    }
   ],
   "source": [
    "def parse_person_page(url, name):\n",
    "    \"\"\"Parse individual person page and extract data\"\"\"\n",
    "    soup = get_page_content(url)\n",
    "    if not soup:\n",
    "        return None\n",
    "    \n",
    "    # Extract year of birth\n",
    "    year = extract_year_of_birth(soup)\n",
    "    \n",
    "    # Extract occupation (no category, just occupation)\n",
    "    occupation = extract_occupation(soup)\n",
    "    \n",
    "    # Normalize name to match XML format (decoded, with underscores)\n",
    "    name_normalized = normalize_name(name)\n",
    "    \n",
    "    return {\n",
    "        'year': year,\n",
    "        'name': name_normalized,  # Normalized format to match XML\n",
    "        'occupation': occupation or ''  # Just occupation, no category\n",
    "    }\n",
    "\n",
    "print(\"Person parser function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b968cd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XML writer function defined\n"
     ]
    }
   ],
   "source": [
    "def write_xml(persons, output_file):\n",
    "    \"\"\"Write persons data to XML file\"\"\"\n",
    "    root = ET.Element('persons')\n",
    "    root.set('day', '14-11')\n",
    "    root.set('comm', f'Parsed {len(persons)} persons')\n",
    "    \n",
    "    # Sort by year, then by name - handle None values properly\n",
    "    def sort_key(person):\n",
    "        year = person.get('year')\n",
    "        name = person.get('name', '')\n",
    "        # Convert None to '9999' for sorting, ensure year is string\n",
    "        year_str = str(year) if year is not None else '9999'\n",
    "        return (year_str, name)\n",
    "    \n",
    "    sorted_persons = sorted(persons, key=sort_key)\n",
    "    \n",
    "    for person in sorted_persons:\n",
    "        psn = ET.SubElement(root, 'psn')\n",
    "        if person.get('year'):\n",
    "            psn.set('y', str(person['year']))\n",
    "        else:\n",
    "            psn.set('y', 'yyyy')  # Placeholder if year not found\n",
    "        \n",
    "        psn.set('h', person['name'])\n",
    "        \n",
    "        # Add occupation if available (no category, just occupation)\n",
    "        if person.get('occupation'):\n",
    "            psn.set('p', person['occupation'])\n",
    "    \n",
    "    # Write to file with proper UTF-8 encoding\n",
    "    tree = ET.ElementTree(root)\n",
    "    ET.indent(tree, space='  ')\n",
    "    # Write with UTF-8 encoding explicitly\n",
    "    with open(output_file, 'wb') as f:\n",
    "        f.write('<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n'.encode('utf-8'))\n",
    "        tree.write(f, encoding='utf-8', xml_declaration=False)\n",
    "    print(f\"Written {len(persons)} persons to {output_file}\")\n",
    "\n",
    "print(\"XML writer function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e698af96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to parse category: https://ru.wikipedia.org/wiki/Категория:Родившиеся_18_ноября\n",
      "Existing entries loaded: 0\n",
      "\n",
      "============================================================\n",
      "--- Parsing page 1 ---\n",
      "URL: https://ru.wikipedia.org/wiki/Категория:Родившиеся_18_ноября\n",
      "============================================================\n",
      "Found 200 person links on this page\n",
      "[1/200] Processing: Ааней, Андрея\n",
      "  + New: Year: 1993, Occupation: \n",
      "[2/200] Processing: Абашидзе, Аслан Хусейнович\n",
      "  + New: Year: 1958, Occupation: учёный, юрист\n",
      "[3/200] Processing: Аббуд, Карима\n",
      "  + New: Year: 1893, Occupation: фотограф\n",
      "[4/200] Processing: Абдешев, Хайрулла\n",
      "  + New: Year: 1925, Occupation: командир, крестьянин\n",
      "[5/200] Processing: Абель, Джейк\n",
      "  + New: Year: 1987, Occupation: актёр\n",
      "[6/200] Processing: Абер, Лоис\n",
      "  + New: Year: 1983, Occupation: биатлонист\n",
      "[7/200] Processing: Абзалов, Мелис Арипович\n",
      "  + New: Year: 1938, Occupation: актёр, режиссёр\n",
      "[8/200] Processing: Абова, Тамара Евгеньевна\n",
      "  + New: Year: 1927, Occupation: юрист\n",
      "[9/200] Processing: Абу Бакр-мирза (сын Мухаммеда-Джуки)\n",
      "  + New: Year: 1427, Occupation: политик\n",
      "[10/200] Processing: Аванесова, Галина Алексеевна\n",
      "  + New: Year: 1943, Occupation: учёный, философ\n",
      "[11/200] Processing: Август Кристиан (герцог Ангальт-Кётена)\n",
      "  + New: Year: 1769, Occupation: аристократ\n",
      "[12/200] Processing: Август I (князь Брауншвейг-Люнебурга)\n",
      "  + New: Year: None, Occupation: политик\n",
      "[13/200] Processing: Авдонин, Александр Николаевич (поэт)\n",
      "  + New: Year: 1923, Occupation: поэт, редактор\n",
      "[14/200] Processing: Аверьянов, Николай Яковлевич\n",
      "  + New: Year: 1894, Occupation: учитель\n",
      "[15/200] Processing: Авсеенко, Владимир Лаврович\n",
      "  + New: Year: 1917, Occupation: инженер, военный инженер\n",
      "[16/200] Processing: Агапов, Владимир Михайлович\n",
      "  + New: Year: 1933, Occupation: футболист\n",
      "[17/200] Processing: Агуто, Антонио\n",
      "  + New: Year: 1966, Occupation: офицер, генерал\n",
      "[18/200] Processing: Адамс, Лиза\n",
      "  + New: Year: 1990, Occupation: \n",
      "[19/200] Processing: Аджей, Сэмми (1973)\n",
      "  + New: Year: 1973, Occupation: футболист\n",
      "[20/200] Processing: Аджумани, Стефан\n",
      "  + New: Year: 1998, Occupation: футболист\n",
      "[21/200] Processing: Адыгозел, Эльвин\n",
      "  + New: Year: 1989, Occupation: режиссёр, кинорежиссёр, сценарист\n",
      "[22/200] Processing: Азизов, Энглен Атакузиевич\n",
      "  + New: Year: 1936, Occupation: физик\n",
      "[23/200] Processing: Аксельрод, Николай Ильич\n",
      "  + New: Year: 1945, Occupation: поэт, прозаик, драматург, писатель\n",
      "[24/200] Processing: Аксенёнок, Георгий Александрович\n",
      "  + New: Year: 1910, Occupation: профессор, философ, корреспондент\n",
      "[25/200] Processing: Аладжалов, Константин Иванович\n",
      "  + New: Year: 1900, Occupation: художник\n",
      "[26/200] Processing: Алвес да Силва, Аилтон Сезар Жуниор\n",
      "  + New: Year: 1994, Occupation: футболист\n",
      "[27/200] Processing: Александрова, Юлия Васильевна\n",
      "  + New: Year: 1934, Occupation: поэт, учёный\n",
      "[28/200] Processing: Алеш, Миколаш\n",
      "  + New: Year: 1852, Occupation: художник\n",
      "[29/200] Processing: Алиев, Билал Шахширин оглы\n",
      "  + New: Year: 1959, Occupation: инженер\n",
      "[30/200] Processing: Алиев, Кямран Байрам оглы\n",
      "  + New: Year: 1965, Occupation: юрист, прокурор, президент, генерал\n",
      "[31/200] Processing: Алкман, Эльза\n",
      "  + New: Year: None, Occupation: \n",
      "[32/200] Processing: Алтуфьев, Павел Владимирович\n",
      "  + New: Year: 1890, Occupation: офицер\n",
      "[33/200] Processing: Аль-Дау, Мохаммед\n",
      "  + New: Year: 1993, Occupation: футболист\n",
      "[34/200] Processing: Аль-Задах, Захир Шах\n",
      "  + New: Year: 1910, Occupation: хоккеист\n",
      "[35/200] Processing: Альмирон, Серхио Омар\n",
      "  + New: Year: 1958, Occupation: футболист\n",
      "[36/200] Processing: Альпидовский, Михаил Гаврилович\n",
      "  + New: Year: 1923, Occupation: государственный деятель\n",
      "[37/200] Processing: Аматов, Бегиш Аматович\n",
      "  + New: Year: None, Occupation: ректор\n",
      "[38/200] Processing: Амирджанов, Артём Аркадьевич\n",
      "  + New: Year: 1945, Occupation: футболист\n",
      "[39/200] Processing: Амирэджиби, Чабуа Ираклиевич\n",
      "  + New: Year: 1921, Occupation: прозаик, редактор\n",
      "[40/200] Processing: Амусья, Мирон Янкелевич\n",
      "  + New: Year: 1934, Occupation: учёный\n",
      "[41/200] Processing: Ананьев, Геннадий Андреевич\n",
      "  + New: Year: 1928, Occupation: писатель, прозаик, журналист, историк\n",
      "[42/200] Processing: Анашкина, Юлия Игоревна\n",
      "  + New: Year: 1980, Occupation: спортсмен, тренер\n",
      "[43/200] Processing: Андрухов, Виктор Павлович\n",
      "  + New: Year: None, Occupation: \n",
      "[44/200] Processing: Анкилов, Николай Пантелеевич\n",
      "  + New: Year: 1923, Occupation: прозаик, драматург, писатель\n",
      "[45/200] Processing: Анна Саксонская (1567—1613)\n",
      "  + New: Year: 1567, Occupation: поэт\n",
      "[46/200] Processing: Антанавичюс, Домантас\n",
      "  + New: Year: 1998, Occupation: футболист\n",
      "[47/200] Processing: Антоневич, Михаил Моисеевич\n",
      "  + New: Year: 1912, Occupation: химик, футболист\n",
      "[48/200] Processing: Антонелли, Эннио\n",
      "  + New: Year: 1936, Occupation: министр, генерал, священник, теолог, философ\n",
      "[49/200] Processing: Антонеску, Михай\n",
      "  + New: Year: 1904, Occupation: политик\n",
      "[50/200] Processing: Аракелов, Вартан Нерсесович\n",
      "  + New: Year: 1910, Occupation: художник\n",
      "[51/200] Processing: Арановский, Соломон Моисеевич\n",
      "  + New: Year: None, Occupation: врач\n",
      "[52/200] Processing: Аргутинский-Долгоруков, Георгий Давыдович\n",
      "  + New: Year: 1873, Occupation: военнослужащий\n",
      "[53/200] Processing: Арендт, Андрей Андреевич\n",
      "  + New: Year: 1890, Occupation: хирург\n",
      "[54/200] Processing: Армани, Анджелина\n",
      "  + New: Year: 1987, Occupation: актриса, порноактриса, актёр\n",
      "[55/200] Processing: Армато, Доминик\n",
      "  + New: Year: 1976, Occupation: актёр\n",
      "[56/200] Processing: Арндт, Никиас\n",
      "  + New: Year: 1991, Occupation: \n",
      "[57/200] Processing: Арсений (Лазаров)\n",
      "  + New: Year: 1986, Occupation: богослов\n",
      "[58/200] Processing: Арсон, Жан-Клод Элеонор Ле Мишо д’\n",
      "  + New: Year: 1733, Occupation: инженер, генерал\n",
      "[59/200] Processing: Артёмов, Сергей Викторович\n",
      "  + New: Year: 1980, Occupation: футболист\n"
     ]
    }
   ],
   "source": [
    "# Main parsing loop\n",
    "def main():\n",
    "    all_persons = []\n",
    "    current_url = CATEGORY_URL\n",
    "    page_count = 0\n",
    "    total_processed = 0\n",
    "    total_skipped_identical = 0\n",
    "    total_updated = 0\n",
    "    total_new = 0\n",
    "    total_failed = 0\n",
    "    \n",
    "    print(f\"Starting to parse category: {CATEGORY_URL}\")\n",
    "    print(f\"Existing entries loaded: {len(existing_data)}\")\n",
    "    \n",
    "    while current_url:\n",
    "        page_count += 1\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"--- Parsing page {page_count} ---\")\n",
    "        print(f\"URL: {current_url}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Parse category page\n",
    "        person_links, next_link = parse_category_page(current_url)\n",
    "        print(f\"Found {len(person_links)} person links on this page\")\n",
    "        \n",
    "        if not person_links:\n",
    "            print(\"No person links found, trying to continue...\")\n",
    "        \n",
    "        # Process each person\n",
    "        for idx, (name, url) in enumerate(person_links, 1):\n",
    "            # Normalize the name to standard format for comparison\n",
    "            name_normalized = normalize_name(name)\n",
    "            name_display = unquote(name).replace('_', ' ')  # For display with spaces\n",
    "            \n",
    "            print(f\"[{idx}/{len(person_links)}] Processing: {name_display}\")\n",
    "            person_data = parse_person_page(url, name)\n",
    "            \n",
    "            if not person_data:\n",
    "                total_failed += 1\n",
    "                print(f\"  ✗ Failed to extract data\")\n",
    "                time.sleep(0.5)\n",
    "                continue\n",
    "            \n",
    "            # Check if person exists in existing data\n",
    "            if name_normalized in existing_data:\n",
    "                existing = existing_data[name_normalized]\n",
    "                new_year = person_data.get('year')\n",
    "                new_occupation = person_data.get('occupation', '')\n",
    "                existing_year = existing.get('year')\n",
    "                existing_occupation = existing.get('occupation', '')\n",
    "                \n",
    "                # If new year is None, use existing year (don't overwrite with None)\n",
    "                if new_year is None:\n",
    "                    person_data['year'] = existing_year\n",
    "                    new_year = existing_year\n",
    "                \n",
    "                # Normalize years for comparison (handle None and 'yyyy')\n",
    "                new_year_str = str(new_year) if new_year else None\n",
    "                existing_year_str = str(existing_year) if existing_year else None\n",
    "                \n",
    "                # Check if data is different\n",
    "                year_different = new_year_str != existing_year_str\n",
    "                \n",
    "                # Only change occupation if original file doesn't have it or it's empty/dash\n",
    "                # Check if existing occupation is empty, None, or just '-'\n",
    "                existing_occ_empty = not existing_occupation or existing_occupation.strip() == '' or existing_occupation.strip() == '-'\n",
    "                \n",
    "                # Only update occupation if existing is empty/dash\n",
    "                if existing_occ_empty and new_occupation:\n",
    "                    occupation_different = True\n",
    "                    # Use new occupation\n",
    "                    person_data['occupation'] = new_occupation\n",
    "                else:\n",
    "                    # Keep existing occupation (don't change it)\n",
    "                    occupation_different = False\n",
    "                    person_data['occupation'] = existing_occupation\n",
    "                \n",
    "                # Update if year is different OR occupation was updated\n",
    "                if year_different or occupation_different:\n",
    "                    # Data is different, add to output (will overwrite)\n",
    "                    all_persons.append(person_data)\n",
    "                    total_updated += 1\n",
    "                    changes = []\n",
    "                    if year_different:\n",
    "                        changes.append(f\"year: {existing_year_str} → {new_year_str}\")\n",
    "                    if occupation_different:\n",
    "                        changes.append(f\"occupation: '{existing_occupation}' → '{new_occupation}'\")\n",
    "                    print(f\"  ↻ Updated: {', '.join(changes)}\")\n",
    "                else:\n",
    "                    # Data is identical, skip\n",
    "                    total_skipped_identical += 1\n",
    "                    print(f\"  ✓ Identical data, skipping\")\n",
    "            else:\n",
    "                # New person, add to output\n",
    "                all_persons.append(person_data)\n",
    "                total_new += 1\n",
    "                year = person_data.get('year', 'N/A')\n",
    "                occupation = person_data.get('occupation', 'N/A')\n",
    "                print(f\"  + New: Year: {year}, Occupation: {occupation}\")\n",
    "            \n",
    "            total_processed += 1\n",
    "            \n",
    "            # Be polite to Wikipedia servers\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        # Move to next page\n",
    "        current_url = next_link\n",
    "        if next_link:\n",
    "            print(f\"\\n→ Moving to next page: {next_link}\")\n",
    "        else:\n",
    "            print(f\"\\n→ No more pages found\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"=== Parsing complete ===\")\n",
    "    print(f\"Total pages parsed: {page_count}\")\n",
    "    print(f\"Total persons processed: {total_processed}\")\n",
    "    print(f\"Total new persons: {total_new}\")\n",
    "    print(f\"Total updated (different data): {total_updated}\")\n",
    "    print(f\"Total skipped (identical data): {total_skipped_identical}\")\n",
    "    print(f\"Total failed to extract: {total_failed}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Write to XML\n",
    "    if all_persons:\n",
    "        write_xml(all_persons, OUTPUT_XML)\n",
    "        print(f\"\\n✓ Successfully wrote {len(all_persons)} persons to {OUTPUT_XML}\")\n",
    "    else:\n",
    "        print(\"\\n⚠ No persons to write (all were identical to existing data)\")\n",
    "\n",
    "# Run the parser\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
