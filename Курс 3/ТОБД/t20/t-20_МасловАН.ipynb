{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bd3995d",
   "metadata": {},
   "source": [
    "# ИД23-1 Маслов АН\n",
    "## Парсер Википедии (разработан с помощью Cursor)\n",
    "### Парсинг проходил по датам 14-19 ноября (qyy1114.xml - qyy1119.xml), парсились только новые люди и люди у которых не было года или професии в файлах pyy1114.xml - pyy1119.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58b7de24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Конфигурация загружена.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import time\n",
    "from urllib.parse import unquote, urljoin, urlparse\n",
    "import html\n",
    "import sys\n",
    "import io\n",
    "\n",
    "# Проверка на UTF-8 в Jupyter/IPython\n",
    "try:\n",
    "    if hasattr(sys.stdout, 'buffer') and sys.stdout.encoding != 'utf-8':\n",
    "        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')\n",
    "    if hasattr(sys.stderr, 'buffer') and sys.stderr.encoding != 'utf-8':\n",
    "        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')\n",
    "except (AttributeError, ValueError):\n",
    "    pass\n",
    "\n",
    "# Конфигурация\n",
    "BASE_URL = \"https://ru.wikipedia.org\"\n",
    "CATEGORY_URL = \"https://ru.wikipedia.org/wiki/Категория:Родившиеся_8_ноября\"\n",
    "EXISTING_XML = \"pyy1108.xml\"\n",
    "OUTPUT_XML = \"qyy1108.xml\"\n",
    "\n",
    "# Перечень профессий на русском языке\n",
    "OCCUPATIONS = [\n",
    "    \"писатель\", \"поэт\", \"прозаик\", \"драматург\", \"журналист\", \"публицист\", \"эссеист\", \"критик\", \"литературный критик\",\n",
    "    \"библиограф\", \"библиофил\", \"лексикограф\", \"литературовед\", \"историк литературы\",\n",
    "    \"художник\", \"скульптор\", \"архитектор\", \"дизайнер\", \"иллюстратор\", \"график\", \"живописец\", \"портретист\",\n",
    "    \"карикатурист\", \"гравёр\", \"художник-гравёр\", \"художник-график\", \"художник-постановщик\", \"художник-мультипликатор\",\n",
    "    \"деятель изобразительного искусства\", \"керамист\", \"керамистка\", \"гобеленистка\", \"рисовальщик\", \"рисовальщица\",\n",
    "    \"актёр\", \"актриса\", \"режиссёр\", \"кинорежиссёр\", \"сценарист\", \"оператор\", \"кинематографист\", \"кинооператор\",\n",
    "    \"театральный деятель\", \"танцор\", \"хореограф\", \"балетмейстер\", \"артистка балета\", \"театральный актёр\",\n",
    "    \"театральный педагог\", \"театральный критик\", \"театровед\", \"кинокритик\", \"киновед\", \"искусствовед\",\n",
    "    \"музыкант\", \"композитор\", \"певец\", \"дирижёр\", \"пианист\", \"пианистка\", \"басистка\",\n",
    "    \"учёный\", \"исследователь\", \"профессор\", \"преподаватель\", \"учитель\", \"лектор\", \"преподаватель университета\",\n",
    "    \"физик\", \"химик\", \"математик\", \"биолог\", \"ботаник\", \"геолог\", \"историк\", \"географ\", \"археолог\", \"лингвист\",\n",
    "    \"антрополог\", \"этнограф\", \"палеонтолог\", \"палеоботаник\", \"вирусолог\", \"микробиолог\", \"биохимик\", \"физиолог\",\n",
    "    \"анатом\", \"энтомолог\", \"зоолог\", \"орнитолог\", \"ихтиолог\", \"лепидоптеролог\", \"лихенолог\", \"селекционер\",\n",
    "    \"филолог\", \"классицист\", \"классицистка\", \"романист\", \"нумизматка\", \"византолог\",\n",
    "    \"врач\", \"медик\", \"хирург\", \"психиатр\", \"невролог\", \"патолог\", \"фармаколог\", \"военный врач\",\n",
    "    \"инженер\", \"конструктор\", \"изобретатель\", \"авиационный инженер\", \"горный инженер\", \"военный инженер\",\n",
    "    \"инженер-строитель\", \"инженер-механик\", \"инженер-гидротехник\", \"инженер-архитектор\",\n",
    "    \"политик\", \"государственный деятель\", \"дипломат\", \"юрист\", \"адвокат\", \"практикующий юрист\", \"судья\",\n",
    "    \"прокурор\", \"депутат\", \"посол\", \"министр\", \"президент\", \"премьер-министр\",\n",
    "    \"военный деятель\", \"военнослужащий\", \"офицер\", \"полководец\", \"лётчик\", \"солдат\", \"капитан\", \"генерал\",\n",
    "    \"адмирал\", \"командир\", \"разведчик\", \"мемуарист\",\n",
    "    \"спортсмен\", \"футболист\", \"тренер\", \"тренер-преподаватель\", \"бейсболист\", \"хоккеист\", \"биатлонист\",\n",
    "    \"бобслеист\", \"скелетонист\", \"автогонщик\", \"раллийный автогонщик\",\n",
    "    \"предприниматель\", \"бизнесмен\", \"торговец\", \"банкир\", \"менеджер\", \"промышленник\", \"магнат\",\n",
    "    \"священник\", \"религиозный деятель\", \"монах\", \"раввин\", \"настоятель\", \"миссионер\", \"теолог\", \"богослов\",\n",
    "    \"философ\", \"философ науки\", \"историк науки\", \"культуролог\", \"социолог\", \"экономист\", \"политолог\",\n",
    "    \"библиотекарь\", \"переводчик\", \"редактор\", \"издатель\", \"фотограф\", \"кинопродюсер\", \"телепродюсер\",\n",
    "    \"телеведущий\", \"радиоведущий\", \"подкастер\", \"обозреватель\", \"корреспондент\", \"репортёр\",\n",
    "    \"модель\", \"фотомодель\", \"порноактриса\", \"эротическая модель\", \"участница конкурса красоты\",\n",
    "    \"блогер\", \"видеоблогер\", \"стример\", \"тиктокер\", \"ютубер\",\n",
    "    \"модельер\", \"кутюрье\", \"переплётчица\",\n",
    "    \"долгожитель\", \"аристократ\", \"аристократка\", \"фрейлина\", \"аэронавт\", \"капитан судна\",\n",
    "    \"капитан речного флота\", \"шеф-повар\", \"агроном\", \"доярка\", \"свинарка\", \"шахтёр\", \"рабочий\", \"крестьянин\",\n",
    "    \"партизан\", \"революционерка\", \"активист\", \"активистка\", \"гуманистка\", \"правозащитница\",\n",
    "    \"коллекционер\", \"коллекционерка\", \"коллекционер искусства\"\n",
    "]\n",
    "\n",
    "# Категории профессий и соответствующие им ключевые слова на русском языке\n",
    "OCCUPATION_CATEGORIES = {\n",
    "    \"Писатель\": [\"писатель\", \"поэт\", \"прозаик\", \"драматург\", \"журналист\", \"публицист\", \"переводчик\", \"редактор\", \"издатель\"],\n",
    "    \"Художник\": [\"художник\", \"скульптор\", \"архитектор\", \"дизайнер\", \"иллюстратор\", \"фотограф\"],\n",
    "    \"Кинематографист\": [\"актёр\", \"актриса\", \"режиссёр\", \"кинорежиссёр\", \"сценарист\", \"оператор\", \"кинематографист\"],\n",
    "    \"Театральный деятель\": [\"театральный деятель\", \"актёр\", \"актриса\", \"режиссёр\", \"танцор\", \"хореограф\"],\n",
    "    \"Учёный\": [\"учёный\", \"исследователь\", \"профессор\", \"преподаватель\", \"учитель\", \"физик\", \"химик\", \"математик\", \"биолог\", \"ботаник\", \"геолог\", \"историк\", \"географ\", \"археолог\", \"лингвист\"],\n",
    "    \"Врач\": [\"врач\", \"медик\", \"хирург\", \"психиатр\", \"невролог\"],\n",
    "    \"Военный деятель\": [\"военный деятель\", \"военнослужащий\", \"офицер\", \"полководец\", \"лётчик\"],\n",
    "    \"Государственный деятель\": [\"государственный деятель\", \"политик\", \"дипломат\", \"юрист\", \"адвокат\"],\n",
    "    \"Политик\": [\"политик\", \"государственный деятель\"],\n",
    "    \"Предприниматель\": [\"предприниматель\", \"бизнесмен\", \"торговец\"],\n",
    "    \"Религиозный деятель\": [\"религиозный деятель\", \"священник\", \"монах\"],\n",
    "    \"Философ\": [\"философ\"],\n",
    "    \"Персона\": [\"долгожитель\", \"аристократ\", \"аристократка\", \"фрейлина\"],\n",
    "    \"Порноактриса\": [\"порноактриса\"],\n",
    "    \"Фотомодель\": [\"фотомодель\", \"модель\"],\n",
    "    \"Модельер\": [\"модельер\"],\n",
    "    \"Блогер\": [\"блогер\", \"видеоблогер\", \"стример\", \"тиктокер\"],\n",
    "    \"Ректор\": [\"ректор\"],\n",
    "    \"Священник\": [\"священник\"],\n",
    "    \"Убийца\": [\"убийца\"],\n",
    "    \"Футболист\": [\"футболист\"],\n",
    "    \"Биатлонист\": [\"биатлонист\"],\n",
    "    \"Карточка фотографа\": [\"фотограф\"]\n",
    "}\n",
    "\n",
    "print(\"Конфигурация загружена.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75243cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружено 1314 существующих записей из pyy1119.xml\n",
      "Пример записей: ['Абаджян,_Гаррий_Артушевич', 'Абдеррахим,_Муни', 'Абдулкагиров,_Магомед_Камингаджиевич']\n"
     ]
    }
   ],
   "source": [
    "def normalize_name(name):\n",
    "    \"\"\"Normalize a name to a standard format for comparison\"\"\"\n",
    "    if not name:\n",
    "        return ''\n",
    "    # Декодировать URL-encoded строки\n",
    "    normalized = unquote(name)\n",
    "    # Заменить пробелы на подчеркивания для соответствия формату XML\n",
    "    normalized = normalized.replace(' ', '_')\n",
    "    return normalized\n",
    "\n",
    "def load_existing_entries(xml_file):\n",
    "    \"\"\"Загрузить существующие записи из XML-файла с их данными для сравнения\"\"\"\n",
    "    existing_data = {}  # Словарь: normalized_name -> {'year': year, 'occupation': occupation}\n",
    "    existing_names = set()  # Множество всех вариантов имен для быстрого поиска\n",
    "    \n",
    "    try:\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        for psn in root.findall('psn'):\n",
    "            name = psn.get('h', '')\n",
    "            if name:\n",
    "                # Нормализовать имя\n",
    "                normalized = normalize_name(name)\n",
    "                \n",
    "                # Извлечь год и профессию\n",
    "                year = psn.get('y', '')\n",
    "                occupation_attr = psn.get('p', '')\n",
    "                \n",
    "                # Извлечь профессию (удалить категорию, если присутствует, формат: \"Категория;профессия\")\n",
    "                occupation = ''\n",
    "                if occupation_attr:\n",
    "                    if ';' in occupation_attr:\n",
    "                        occupation = occupation_attr.split(';', 1)[1]  # Get part after semicolon\n",
    "                    else:\n",
    "                        occupation = occupation_attr\n",
    "                \n",
    "                # Сохранить данные в словарь\n",
    "                existing_data[normalized] = {\n",
    "                    'year': year if year and year != 'yyyy' else None,\n",
    "                    'occupation': occupation\n",
    "                }\n",
    "                \n",
    "                # Добавить все варианты имен в множество для быстрого поиска\n",
    "                existing_names.add(normalized)\n",
    "                existing_names.add(name)\n",
    "                try:\n",
    "                    from urllib.parse import quote\n",
    "                    existing_names.add(quote(normalized, safe=''))\n",
    "                    existing_names.add(quote(name, safe=''))\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        print(f\"Загружено {len(existing_data)} существующих записей из {xml_file}\")\n",
    "        print(f\"Пример записей: {list(existing_data.keys())[:3] if existing_data else 'None'}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Файл {xml_file} не найден, начинаем с чистого листа\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка загрузки {xml_file}: {e}\")\n",
    "    \n",
    "    return existing_data, existing_names\n",
    "\n",
    "existing_data, existing_names = load_existing_entries(EXISTING_XML)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10b82b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def get_page_content(url):\n",
    "    \"\"\"Ищет содержимое страницы Википедии\"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        return BeautifulSoup(response.content, 'html.parser')\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_year_of_birth(soup):\n",
    "    \"\"\"Вытаскивает год рождения из страницы Википедии\"\"\"\n",
    "    # Пытаемся найти в инфобоксе (самый надежный)\n",
    "    infobox = soup.find('table', class_='infobox')\n",
    "    if infobox:\n",
    "        # Ищет \"Дата рождения\" или \"Родился/Родилась\"\n",
    "        for row in infobox.find_all('tr'):\n",
    "            th = row.find('th')\n",
    "            if th:\n",
    "                th_text = th.get_text().strip()\n",
    "                if 'Дата рождения' in th_text or 'Родился' in th_text or 'Родилась' in th_text:\n",
    "                    td = row.find('td')\n",
    "                    if td:\n",
    "                        text = td.get_text()\n",
    "                        # Ищем шаблоны даты с 4-значным годом (между 1000-2099)\n",
    "                        years = re.findall(r'\\b(1[0-9]{3}|20[0-2][0-9])\\b', text)\n",
    "                        if years:\n",
    "                            # Возвращаем первый найденный год\n",
    "                            return years[0]\n",
    "    \n",
    "    # Пытаемся найти в первом абзаце\n",
    "    first_para = soup.find('div', class_='mw-parser-output')\n",
    "    if first_para:\n",
    "        paragraphs = first_para.find_all('p', recursive=False)\n",
    "        for p in paragraphs[:3]:  # Проверяем первые 3 абзаца\n",
    "            text = p.get_text()\n",
    "            # Ищем шаблоны вроде \"родился 14 ноября 1900 года\" или \"родился в 1900 году\"\n",
    "            patterns = [\n",
    "                r'(?:родился|родилась)\\s+(?:14\\s+ноября\\s+)?(\\d{4})\\s*(?:года|г\\.|г)',\n",
    "                r'(?:родился|родилась)\\s+в\\s+(\\d{4})\\s*(?:году|г\\.|г)',\n",
    "                r'(\\d{4})\\s*(?:года|г\\.|г)\\s*—\\s*(?:родился|родилась)',\n",
    "                r'14\\s+ноября\\s+(\\d{4})',  # Прямой шаблон для \"XX ноября YYYY\"\n",
    "            ]\n",
    "            for pattern in patterns:\n",
    "                year_match = re.search(pattern, text, re.IGNORECASE)\n",
    "                if year_match:\n",
    "                    year = year_match.group(1)\n",
    "                    if 1000 <= int(year) <= 2099:\n",
    "                        return year\n",
    "            \n",
    "            # Или просто найти 4-значный год рядом с \"XX ноября\" или \"ноября\"\n",
    "            if '14 ноября' in text or 'ноября' in text:\n",
    "                years = re.findall(r'\\b(1[0-9]{3}|20[0-2][0-9])\\b', text)\n",
    "                if years:\n",
    "                    return years[0]\n",
    "    \n",
    "    return None\n",
    "\n",
    "def extract_occupation(soup):\n",
    "    \"\"\"Extract occupation from Wikipedia page\"\"\"\n",
    "    occupations_found = []\n",
    "    category_found = None\n",
    "    \n",
    "    # Пытаемся найти в инфобоксе (самый надежный)\n",
    "    infobox = soup.find('table', class_='infobox')\n",
    "    if infobox:\n",
    "        # Ищет \"Род деятельности\" или \"Профессия\"\n",
    "        for row in infobox.find_all('tr'):\n",
    "            th = row.find('th')\n",
    "            if th:\n",
    "                th_text = th.get_text().strip()\n",
    "                if 'Род деятельности' in th_text or 'Профессия' in th_text or 'Род занятий' in th_text:\n",
    "                    td = row.find('td')\n",
    "                    if td:\n",
    "                        text = td.get_text().lower()\n",
    "                        # Извлекаем профессии\n",
    "                        for occ in OCCUPATIONS:\n",
    "                            if occ.lower() in text and occ not in occupations_found:\n",
    "                                occupations_found.append(occ)\n",
    "    \n",
    "    # Пытаемся найти в категориях (внизу страницы)\n",
    "    categories = soup.find('div', id='catlinks')\n",
    "    if categories:\n",
    "        cat_text = categories.get_text().lower()\n",
    "        # Проверяем каждую категорию\n",
    "        for cat_name, occs in OCCUPATION_CATEGORIES.items():\n",
    "            # Проверяем каждое ключевое слово в категории\n",
    "            for occ in occs:\n",
    "                if occ.lower() in cat_text:\n",
    "                    if not category_found:  # Берем первую найденную категорию\n",
    "                        category_found = cat_name\n",
    "                    if occ not in occupations_found:\n",
    "                        occupations_found.append(occ)\n",
    "                    break\n",
    "    \n",
    "    # Пытаемся найти в первом абзаце, если не нашли ранее\n",
    "    if not occupations_found:\n",
    "        first_para = soup.find('div', class_='mw-parser-output')\n",
    "        if first_para:\n",
    "            paragraphs = first_para.find_all('p', recursive=False)\n",
    "            for p in paragraphs[:3]:\n",
    "                text = p.get_text().lower()\n",
    "                for occ in OCCUPATIONS:\n",
    "                    if occ.lower() in text and occ not in occupations_found:\n",
    "                        occupations_found.append(occ)\n",
    "    \n",
    "    # Удаляем \"персона\" из профессий, если присутствует\n",
    "    occupations_found = [occ for occ in occupations_found if occ != 'персона']\n",
    "    \n",
    "    # Если категория не найдена через категории страницы, определяем её по найденным профессиям\n",
    "    if not category_found and occupations_found:\n",
    "        # Ищем категорию по найденным профессиям\n",
    "        for cat_name, occs in OCCUPATION_CATEGORIES.items():\n",
    "            for occ in occupations_found:\n",
    "                if occ in occs:\n",
    "                    category_found = cat_name\n",
    "                    break\n",
    "            if category_found:\n",
    "                break\n",
    "    \n",
    "    if occupations_found:\n",
    "        # Возвращаем словарь с категорией и профессиями\n",
    "        return {\n",
    "            'category': category_found or 'Персона',\n",
    "            'occupations': occupations_found[:5]  # Ограничиваем до 5 профессий\n",
    "        }\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"Helper functions defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efb71457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category parser function defined\n"
     ]
    }
   ],
   "source": [
    "def parse_category_page(url):\n",
    "    \"\"\"Парсит страницу категории и возвращает список ссылок на людей и ссылку на следующую страницу\"\"\"\n",
    "    soup = get_page_content(url)\n",
    "    if not soup:\n",
    "        return [], None\n",
    "    \n",
    "    person_links = []\n",
    "    \n",
    "    # Находит все ссылки на странице категории\n",
    "    # Пытается разные возможные контейнеры\n",
    "    content = soup.find('div', id='mw-pages')\n",
    "    if not content:\n",
    "        content = soup.find('div', class_='mw-category')\n",
    "    if not content:\n",
    "        content = soup.find('div', class_='mw-category-group')\n",
    "    \n",
    "    # Список префиксов специальных страниц для пропуска\n",
    "    skip_prefixes = ['Категория:', 'Файл:', 'Шаблон:', 'Обсуждение:', 'Участник:', 'Портал:', \n",
    "                     'Википедия:', 'Служебная:', 'Медиа:', 'Справка:', 'Обсуждение_шаблона:',\n",
    "                     'Обсуждение_участника:', 'Обсуждение_файла:', 'Обсуждение_категории:']\n",
    "    \n",
    "    if content:\n",
    "        # Находит все ссылки в списке категории\n",
    "        for link in content.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if href.startswith('/wiki/'):\n",
    "                title_part = href.split('/wiki/')[1].split('#')[0]\n",
    "                # Декодируем для проверки, является ли это специальной страницей\n",
    "                title_decoded = unquote(title_part)\n",
    "                \n",
    "                # Пропускаем специальные страницы\n",
    "                should_skip = False\n",
    "                for prefix in skip_prefixes:\n",
    "                    if title_part.startswith(prefix) or title_decoded.startswith(prefix):\n",
    "                        should_skip = True\n",
    "                        break\n",
    "                \n",
    "                if not should_skip:\n",
    "                    full_url = urljoin(BASE_URL, href)\n",
    "                    # Используем URL-кодированный заголовок для согласованности с форматом XML  \n",
    "                    person_links.append((title_part, full_url))\n",
    "    \n",
    "    # Удаляем дубликаты, сохраняя порядок\n",
    "    seen = set()\n",
    "    unique_links = []\n",
    "    for title, url in person_links:\n",
    "        if title not in seen:\n",
    "            seen.add(title)\n",
    "            unique_links.append((title, url))\n",
    "    \n",
    "    # Находит ссылку \"Следующая страница\" - улучшенный поиск\n",
    "    next_link = None\n",
    "    \n",
    "    # Метод 1: Ищет ссылку с точным текстом \"Следующая страница\"\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        link_text = link.get_text().strip()\n",
    "        if link_text == 'Следующая страница':\n",
    "            next_link = urljoin(BASE_URL, link['href'])\n",
    "            break\n",
    "    \n",
    "    # Метод 2: Ищет в навигационных блоках (более комплексный поиск)\n",
    "    if not next_link:\n",
    "        # Пытается разные навигационные контейнеры\n",
    "        nav_selectors = [\n",
    "            ('div', {'class': 'mw-category-navigation'}),\n",
    "            ('div', {'id': 'mw-pages'}),\n",
    "            ('div', {'class': 'mw-category'}),\n",
    "        ]\n",
    "        \n",
    "        for tag, attrs in nav_selectors:\n",
    "            nav_divs = soup.find_all(tag, attrs)\n",
    "            for nav_div in nav_divs:\n",
    "                for link in nav_div.find_all('a', href=True):\n",
    "                    link_text = link.get_text().strip()\n",
    "                    # Проверяет наличие \"Следующая страница\" или вариаций\n",
    "                    if 'Следующая' in link_text or 'следующая' in link_text or 'Следующая страница' in link_text:\n",
    "                        href = link.get('href')\n",
    "                        if href and href.startswith('/wiki/'):\n",
    "                            next_link = urljoin(BASE_URL, href)\n",
    "                            break\n",
    "                if next_link:\n",
    "                    break\n",
    "            if next_link:\n",
    "                break\n",
    "    \n",
    "    # Метод 3: Ищет ссылки с параметром \"from\" (пагинация)  \n",
    "    if not next_link:\n",
    "        # Страницы категорий Wikipedia используют параметр \"from\" для пагинации\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link.get('href', '')\n",
    "            if 'from=' in href and 'Категория:Родившиеся_14_ноября' in href:\n",
    "                next_link = urljoin(BASE_URL, href)\n",
    "                break\n",
    "    \n",
    "    # Метод 4: Ищет класс \"next\" или подобный\n",
    "    if not next_link:\n",
    "        next_links = soup.find_all('a', class_=re.compile('next', re.I))\n",
    "        for link in next_links:\n",
    "            if link.get('href'):\n",
    "                next_link = urljoin(BASE_URL, link['href'])\n",
    "                break\n",
    "    \n",
    "    return unique_links, next_link\n",
    "\n",
    "print(\"Category parser function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b77e1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person parser function defined\n"
     ]
    }
   ],
   "source": [
    "def parse_person_page(url, name):\n",
    "    \"\"\"Parse individual person page and extract data\"\"\"\n",
    "    soup = get_page_content(url)\n",
    "    if not soup:\n",
    "        return None\n",
    "    \n",
    "    # Extract year of birth\n",
    "    year = extract_year_of_birth(soup)\n",
    "    \n",
    "    # Extract occupation (no category, just occupation)\n",
    "    occupation = extract_occupation(soup)\n",
    "    \n",
    "    # Normalize name to match XML format (decoded, with underscores)\n",
    "    name_normalized = normalize_name(name)\n",
    "    \n",
    "    return {\n",
    "        'year': year,\n",
    "        'name': name_normalized,  # Normalized format to match XML\n",
    "        'occupation': occupation or ''  # Just occupation, no category\n",
    "    }\n",
    "\n",
    "print(\"Person parser function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b968cd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XML writer function defined\n"
     ]
    }
   ],
   "source": [
    "def write_xml(persons, output_file):\n",
    "    \"\"\"Записываем информацию о людях в XML-файл\"\"\"\n",
    "    root = ET.Element('persons')\n",
    "    root.set('day', '14-11')\n",
    "    root.set('comm', f'Parsed {len(persons)} persons')\n",
    "    \n",
    "    # Сортировка по году, потом по имени\n",
    "    def sort_key(person):\n",
    "        year = person.get('year')\n",
    "        name = person.get('name', '')\n",
    "        # Конвертируем год в строку для правильной сортировки\n",
    "        year_str = str(year) if year is not None else '9999'\n",
    "        return (year_str, name)\n",
    "    \n",
    "    sorted_persons = sorted(persons, key=sort_key)\n",
    "    \n",
    "    for person in sorted_persons:\n",
    "        psn = ET.SubElement(root, 'psn')\n",
    "        if person.get('year'):\n",
    "            psn.set('y', str(person['year']))\n",
    "        else:\n",
    "            psn.set('y', 'yyyy')  # Плейсхолдер если год не был найден\n",
    "        \n",
    "        psn.set('h', person['name'])\n",
    "        \n",
    "        # добавить профессию, если есть\n",
    "        if person.get('occupation'):\n",
    "            psn.set('p', person['occupation'])\n",
    "    \n",
    "    # Запись в файл с правильной кодировкой UTF-8\n",
    "    tree = ET.ElementTree(root)\n",
    "    ET.indent(tree, space='  ')\n",
    "    # Запись файла с декларацией XML\n",
    "    with open(output_file, 'wb') as f:\n",
    "        f.write('<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n'.encode('utf-8'))\n",
    "        tree.write(f, encoding='utf-8', xml_declaration=False)\n",
    "    print(f\"Written {len(persons)} persons to {output_file}\")\n",
    "\n",
    "print(\"XML writer function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e698af96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начинаем парсинг категории: https://ru.wikipedia.org/wiki/Категория:Родившиеся_19_ноября\n",
      "Загружено существующих записей: 1314\n",
      "\n",
      "============================================================\n",
      "--- Парсинг страницы 1 ---\n",
      "URL: https://ru.wikipedia.org/wiki/Категория:Родившиеся_19_ноября\n",
      "============================================================\n",
      "Error fetching https://ru.wikipedia.org/wiki/Категория:Родившиеся_19_ноября: 403 Client Error: Too many requests. Please respect our robot policy https://w.wiki/4wJS. (dd12474) for url: https://ru.wikipedia.org/wiki/%D0%9A%D0%B0%D1%82%D0%B5%D0%B3%D0%BE%D1%80%D0%B8%D1%8F:%D0%A0%D0%BE%D0%B4%D0%B8%D0%B2%D1%88%D0%B8%D0%B5%D1%81%D1%8F_19_%D0%BD%D0%BE%D1%8F%D0%B1%D1%80%D1%8F\n",
      "Найдено 0 ссылок на людей на этой странице\n",
      "Ссылки на людей не найдены, пытаемся продолжить...\n",
      "\n",
      "→ Следующих страниц не найдено, завершаем парсинг.\n",
      "\n",
      "============================================================\n",
      "=== Парсинг завершен ===\n",
      "Всего страниц запарсено: 1\n",
      "Всего персон обработано: 0\n",
      "Всего новых персон: 0\n",
      "Всего обновлено (отличающиеся данные): 0\n",
      "Всего пропущено (идентичные данные): 0\n",
      "Всего не удалось извлечь: 0\n",
      "============================================================\n",
      "\n",
      "⚠ Нет персон для записи (все были идентичны существующим данным)\n"
     ]
    }
   ],
   "source": [
    "# Основная функция\n",
    "def main():\n",
    "    all_persons = []\n",
    "    current_url = CATEGORY_URL\n",
    "    page_count = 0\n",
    "    total_processed = 0\n",
    "    total_skipped_identical = 0\n",
    "    total_updated = 0\n",
    "    total_new = 0\n",
    "    total_failed = 0\n",
    "    \n",
    "    print(f\"Начинаем парсинг категории: {CATEGORY_URL}\")\n",
    "    print(f\"Загружено существующих записей: {len(existing_data)}\")\n",
    "    \n",
    "    while current_url:\n",
    "        page_count += 1\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"--- Парсинг страницы {page_count} ---\")\n",
    "        print(f\"URL: {current_url}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Парсинг страницы категории\n",
    "        person_links, next_link = parse_category_page(current_url)\n",
    "        print(f\"Найдено {len(person_links)} ссылок на людей на этой странице\")\n",
    "        \n",
    "        if not person_links:\n",
    "            print(\"Ссылки на людей не найдены, пытаемся продолжить...\")\n",
    "        \n",
    "        # Проходим по каждой персоне\n",
    "        for idx, (name, url) in enumerate(person_links, 1):\n",
    "            # Нормализуем имя для проверки\n",
    "            name_normalized = normalize_name(name)\n",
    "            name_display = unquote(name).replace('_', ' ')  # для отображения\n",
    "            \n",
    "            print(f\"[{idx}/{len(person_links)}] Обработка: {name_display}\")\n",
    "            person_data = parse_person_page(url, name)\n",
    "            \n",
    "            if not person_data:\n",
    "                total_failed += 1\n",
    "                print(f\"  ✗ Не удалось извлечь данные\")\n",
    "                time.sleep(0.5)\n",
    "                continue\n",
    "            \n",
    "            # Проверяем, существует ли уже такая персона\n",
    "            if name_normalized in existing_data:\n",
    "                existing = existing_data[name_normalized]\n",
    "                new_year = person_data.get('year')\n",
    "                new_occupation_data = person_data.get('occupation_data')\n",
    "                existing_year = existing.get('year')\n",
    "                existing_occupation_data = existing.get('occupation_data')\n",
    "                \n",
    "                # Если не нашли другой год, используем существующий\n",
    "                if new_year is None:\n",
    "                    person_data['year'] = existing_year\n",
    "                    new_year = existing_year\n",
    "                \n",
    "                # Нормализуем годы для сравнения (обрабатываем None и 'yyyy')\n",
    "                new_year_str = str(new_year) if new_year else None\n",
    "                existing_year_str = str(existing_year) if existing_year else None\n",
    "                \n",
    "                # Проверяем, отличаются ли данные\n",
    "                year_different = new_year_str != existing_year_str\n",
    "                \n",
    "                # Меняем данные, только если у оригинала нет профессии\n",
    "                # Проверяем, пуста ли профессия у существующей записи\n",
    "                existing_occ_empty = not existing_occupation_data or not existing_occupation_data.get('occupations')\n",
    "                \n",
    "                # Меняем профессию только если у существующей записи она пуста\n",
    "                if existing_occ_empty and new_occupation_data and new_occupation_data.get('occupations'):\n",
    "                    occupation_different = True\n",
    "                    # Используем новую профессию\n",
    "                    person_data['occupation_data'] = new_occupation_data\n",
    "                else:\n",
    "                    # Сохраняем существующую профессию (не меняем)\n",
    "                    occupation_different = False\n",
    "                    person_data['occupation_data'] = existing_occupation_data\n",
    "                \n",
    "                # Обновляем, если год отличается ИЛИ профессия была обновлена\n",
    "                if year_different or occupation_different:\n",
    "                    # Данные отличаются, добавляем в вывод (перезапишем)\n",
    "                    all_persons.append(person_data)\n",
    "                    total_updated += 1\n",
    "                    changes = []\n",
    "                    if year_different:\n",
    "                        changes.append(f\"год: {existing_year_str} → {new_year_str}\")\n",
    "                    if occupation_different:\n",
    "                        existing_occ_str = ''\n",
    "                        new_occ_str = ''\n",
    "                        if existing_occupation_data:\n",
    "                            existing_occ_str = f\"{existing_occupation_data.get('category', '')}; {', '.join(existing_occupation_data.get('occupations', []))}\"\n",
    "                        if new_occupation_data:\n",
    "                            new_occ_str = f\"{new_occupation_data.get('category', '')}; {', '.join(new_occupation_data.get('occupations', []))}\"\n",
    "                        changes.append(f\"профессия: '{existing_occ_str}' → '{new_occ_str}'\")\n",
    "                    print(f\"  ↻ Обновление: {', '.join(changes)}\")\n",
    "                else:\n",
    "                    # Данные идентичны, пропускаем\n",
    "                    total_skipped_identical += 1\n",
    "                    print(f\"  ✓ Данные идентичны, пропускаем\")\n",
    "            else:\n",
    "                # Новая персона, добавляем\n",
    "                all_persons.append(person_data)\n",
    "                total_new += 1\n",
    "                year = person_data.get('year', 'N/A')\n",
    "                occupation_data = person_data.get('occupation_data')\n",
    "                if occupation_data:\n",
    "                    occ_str = f\"{occupation_data.get('category', '')}; {', '.join(occupation_data.get('occupations', []))}\"\n",
    "                else:\n",
    "                    occ_str = 'N/A'\n",
    "                print(f\"  + Новая: Год: {year}, Профессия: {occ_str}\")\n",
    "            \n",
    "            total_processed += 1\n",
    "            \n",
    "            # Чтобы не забанило\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        # Переход к следующей странице \n",
    "        current_url = next_link\n",
    "        if next_link:\n",
    "            print(f\"\\n→ Переход к следующей странице: {next_link}\")\n",
    "        else:\n",
    "            print(f\"\\n→ Следующих страниц не найдено, завершаем парсинг.\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"=== Парсинг завершен ===\")\n",
    "    print(f\"Всего страниц запарсено: {page_count}\")\n",
    "    print(f\"Всего персон обработано: {total_processed}\")\n",
    "    print(f\"Всего новых персон: {total_new}\")\n",
    "    print(f\"Всего обновлено (отличающиеся данные): {total_updated}\")\n",
    "    print(f\"Всего пропущено (идентичные данные): {total_skipped_identical}\")\n",
    "    print(f\"Всего не удалось извлечь: {total_failed}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Запись в XML\n",
    "    if all_persons:\n",
    "        write_xml(all_persons, OUTPUT_XML)\n",
    "        print(f\"\\n✓ Успешно записано {len(all_persons)} персон в {OUTPUT_XML}\")\n",
    "    else:\n",
    "        print(\"\\n⚠ Нет персон для записи (все были идентичны существующим данным)\")\n",
    "\n",
    "# Запуск парсера\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
